{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CoE LLM Router Usage Guide\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The SambaNova CoE (Composition of Experts) LLM Router is a flexible system designed to route queries to the most appropriate expert model based on the content of the query. This notebook will guide you through the process of using the CoE LLM router, explaining its customizable nature, different modes of operation, and how to effectively utilize them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the CoE LLM Router\n",
    "\n",
    "The CoE LLM Router uses a customizable approach to classify incoming queries into different categories. Each category corresponds to a specific expert model that is best suited to handle queries in that domain.\n",
    "\n",
    "### Customizable Categories\n",
    "\n",
    "Users can define their own categories based on their specific needs. Here are some example categories that could be used:\n",
    "\n",
    "- Finance\n",
    "- Economics\n",
    "- Mathematics\n",
    "- Code Generation\n",
    "- Legal\n",
    "- Medical\n",
    "- History\n",
    "- Turkish Language\n",
    "- Japanese Language\n",
    "- Literature\n",
    "- Physics\n",
    "- Chemistry\n",
    "- Biology\n",
    "- Psychology\n",
    "- Sociology\n",
    "- Generalist (for queries not fitting into specific categories)\n",
    "\n",
    "Remember, these are just examples. You can define your own categories based on your specific use case and the expert models you have available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Importance of Prompts\n",
    "\n",
    "The effectiveness of the CoE LLM Router heavily depends on the quality and structure of the prompts used. A well-crafted prompt ensures that the router accurately classifies the query and directs it to the appropriate expert model. When designing your prompt, consider including:\n",
    "\n",
    "1. A clear instruction to classify the message into one of your predefined categories.\n",
    "2. Examples of queries for each category to provide context.\n",
    "3. Any specific rules or considerations for classification.\n",
    "4. A request for the model to explain its classification decision.\n",
    "\n",
    "This structured approach helps in maintaining consistency and accuracy in the routing process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modes of Operation\n",
    "\n",
    "The CoE LLM Router can be run in four different modes:\n",
    "\n",
    "1. Expert Mode\n",
    "2. Simple Mode\n",
    "3. E2E (End-to-End) Mode with Vector Database\n",
    "4. Bulk QA Mode\n",
    "\n",
    "Let's explore each of these modes in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Expert Mode\n",
    "\n",
    "In this mode, the router only returns the expert category for a given query without invoking the expert model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "kit_dir = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "repo_dir = os.path.abspath(os.path.join(kit_dir, \"..\"))\n",
    "CONFIG_PATH = os.path.join(kit_dir, \"config.yaml\")\n",
    "\n",
    "sys.path.append(kit_dir)\n",
    "sys.path.append(repo_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CoE_jump_start.src.use_CoE_model import get_expert_only\n",
    "\n",
    "query = \"What is the current inflation rate?\"\n",
    "expert = get_expert_only(query)\n",
    "print(f\"Expert category for query '{query}': {expert}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Simple Mode\n",
    "\n",
    "This mode routes the query to the appropriate expert model and returns both the expert category and the model's response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CoE_jump_start.src.use_CoE_model import run_simple_llm_invoke\n",
    "\n",
    "query = \"Write a Python function to calculate the factorial of a number.\"\n",
    "expert, response = run_simple_llm_invoke(query)\n",
    "print(f\"Expert category: {expert}\")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. E2E Mode with Vector Database\n",
    "\n",
    "This mode uses a vector database for more complex queries that may require context from multiple documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CoE_jump_start.src.use_CoE_model import run_e2e_vector_database\n",
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader\n",
    "\n",
    "# Load your document\n",
    "doc_path = '/path/to/your/document/.pdf'\n",
    "loader = PyPDFLoader(doc_path)\n",
    "documents = loader.load()\n",
    "\n",
    "query = \"Summarize the key economic indicators mentioned in the document.\"\n",
    "expert, response = run_e2e_vector_database(query, documents)\n",
    "print(f\"Expert: {expert}\")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Bulk QA Mode\n",
    "\n",
    "This mode is used for evaluating the router's performance on a large dataset of queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CoE_jump_start.src.use_CoE_model import run_bulk_routing_eval\n",
    "\n",
    "dataset_path = \"path/to/your/dataset.jsonl\"\n",
    "num_examples = 10  # Set to None to run on entire dataset\n",
    "\n",
    "results_df, accuracies, confusion_matrix = run_bulk_routing_eval(dataset_path, num_examples)\n",
    "print(\"Accuracies by category:\", accuracies)\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customizing the CoE LLM Router\n",
    "\n",
    "To customize the CoE LLM Router for your specific use case:\n",
    "\n",
    "1. Define your own categories based on your domain expertise and available expert models.\n",
    "2. Create a mapping between these categories and your expert models.\n",
    "3. Design a prompt that effectively distinguishes between your categories.\n",
    "4. Update the configuration file with your custom categories, expert mappings, and prompt.\n",
    "\n",
    "Remember, the flexibility of the CoE LLM Router allows you to tailor it to your specific needs and continuously refine its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The CoE LLM Router provides a powerful and flexible way to direct queries to specialized expert models, improving the overall quality and relevance of responses. By understanding the different modes of operation, the importance of well-structured prompts, and the ability to customize categories and expert mappings, you can effectively leverage this system for a wide range of applications.\n",
    "\n",
    "Remember to always use appropriate error handling and logging in your production code, and to respect the privacy and security considerations when dealing with sensitive information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
