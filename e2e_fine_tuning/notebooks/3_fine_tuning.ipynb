{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Executing Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "kit_dir =  os.path.abspath(os.path.join(current_dir, '..'))\n",
    "repo_dir = os.path.abspath(os.path.join(kit_dir, '..'))\n",
    "sys.path.append(repo_dir)\n",
    "\n",
    "from utils.fine_tuning.src.snsdk_wrapper import SnsdkWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step by Step / Manual setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 18:57:38,619 [INFO] Using variables from Snapi config to set up Snsdk.\n"
     ]
    }
   ],
   "source": [
    "sambastudio_client = SnsdkWrapper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List trainable models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['meta-llama-3-8b-instruct-128384-vocab',\n",
       " 'E5 Large V2',\n",
       " 'GPT13B 2k SS ITv3',\n",
       " 'GPT_1.5B_GT_Finetuned',\n",
       " 'Multilingual E5 Large',\n",
       " 'GPT_1.5B_Base_Model',\n",
       " 'CLIP-ViT-B-32-laion2B-s34B-b79k',\n",
       " 'CLIP ViT-B-32 Backbone (Deprecated)',\n",
       " 'llava-v1.5-7b',\n",
       " 'Multilingual E5 Large Instruct',\n",
       " 'Hubert_ASR',\n",
       " 'GPT_1.5B_GT_Pretrained',\n",
       " 'TR_Sarashina2-70B_Superglue_Sarashina_8k_SN40L-8_4RDU-ckpt10',\n",
       " 'FakeBox',\n",
       " 'Deepseek-coder-6.7b-instruct',\n",
       " 'RC4_VIEW_TEST',\n",
       " 'SimpleTextClassGenerativeTrained',\n",
       " 'RC4_Colab_Test',\n",
       " 'Deepseek-coder-6.7b-base',\n",
       " 'HermesProInstructV10',\n",
       " 'GPT_13B_Human_Aligned_Instruction_Tuned_V2',\n",
       " 'Llama-2-7b-16k-hf',\n",
       " 'Suzume-Llama-3-8B-Multilingual',\n",
       " 'YANZHEC_TEST_SNAPI_GPT1.5B_GT_Finetuned',\n",
       " 'meta-llama-3-8b-instruct-128256-vocab',\n",
       " 'GPT_1.5B_Dialog_Act_Classification_Finetuned',\n",
       " 'LlamaGuard_7b',\n",
       " 'meta-llama-3-70b-instruct-128256-vocab',\n",
       " 'Thai_LLaMA_70B',\n",
       " 'GPT_13B_Generative_Inference',\n",
       " 'Llama-2-13b-hf',\n",
       " 'meta-llama-3-8b-nan-generator',\n",
       " 'meta-llama-guard-2-8b-128384-vocab',\n",
       " 'Zephyr-7B-Beta',\n",
       " 'Llama-2-7b-chat-hf',\n",
       " 'GPT_13B_Base_Model',\n",
       " 'Llama-2-7b-sambalingo-thai-base-hf',\n",
       " 'GPT_13B_GT_Base_Model_300k_MaxVocabSize',\n",
       " 'meta-llama-guard-2-8b-128256-vocab',\n",
       " 'Llama-2-70b-hf',\n",
       " 'Llama 2 7B base 8-socket',\n",
       " 'mistral-7b-instruct-v0.2',\n",
       " 'Llama-2-7b-sambalingo-thai-chat-hf',\n",
       " 'Llama 2 7B chat 8-socket',\n",
       " 'Llama-2-7b-hf',\n",
       " 'Llama-2-13b-chat-hf',\n",
       " 'llava-v1.5-7b (deprecated)',\n",
       " 'GPT 13B 8k SS SN Pretrained',\n",
       " 'GPT_13B_Dialog_Summarization_Finetuned',\n",
       " 'Llama-2-70-16k-hf',\n",
       " 'GPT13B 8k SS HAv3',\n",
       " 'GPT_13B_Instruction_Tuned_V2',\n",
       " 'GPT13B 8k SS ITv3',\n",
       " 'llama-2-70b-chat-hf',\n",
       " 'Meta-Llama-3-8B-Instruct',\n",
       " 'law-chat',\n",
       " 'meta-llama-3-70b-128256-vocab',\n",
       " 'Llama-2-7b-chat-hf',\n",
       " 'NSQL-Llama-2-7B',\n",
       " 'Test Upload',\n",
       " 'Llama-2-70b-chat-hf',\n",
       " 'GPT13B 2k SS HAv3',\n",
       " 'Llama-2-70b-chat-16k-hf',\n",
       " 'law-chat',\n",
       " 'mistral-7b-v0.1',\n",
       " 'meta-llama-3-70b-instruct-128384-vocab',\n",
       " 'Sarashina2-7b',\n",
       " 'llama-2-7B-chat-hf',\n",
       " 'meta-llama-3-70b-128384-vocab',\n",
       " 'Meta-Llama-3-70B-Instruct',\n",
       " 'Sarashina2-70b',\n",
       " 'Llama-2-13B-chat-hf',\n",
       " 'Llama-2-7b-chat-16k-hf',\n",
       " 'meta-llama-3-8b-128256-vocab',\n",
       " 'meta-llama-3-8b-128384-vocab',\n",
       " 'Sarashina2-7B',\n",
       " 'Sarashina2-70B',\n",
       " 'GPT_1.5B_NER_Finetuned']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[model[\"model_checkpoint_name\"]for model in sambastudio_client.list_models(filter_job_types=[\"train\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'Suzume-Llama-3-8B-Multilingual'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List available datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['openthaigpt_50k_IT0913',\n",
       " 'Generative_Inference_Dataset',\n",
       " 'GPT_13B_Inference_Dataset',\n",
       " 'FiQA',\n",
       " 'Super_Glue_4k_SS',\n",
       " 'E5_Large_V2_Inference_Example',\n",
       " 'ASR_With_Diarization_Dataset',\n",
       " 'Restore_Punctuation_Data',\n",
       " 'ASR_Without_Diarization_Dataset',\n",
       " 'GPT_13B_8k_SS_Toy_Training_Dataset',\n",
       " 'Librispeech',\n",
       " 'GPT_1.5B_Training_Dataset',\n",
       " 'GPT_13B_Training_Dataset',\n",
       " 'Speaker_Diarization',\n",
       " 'test',\n",
       " 'Coding_Generative_Train_4k_SS_Dataset',\n",
       " 'Mistral_Tokenized_Copa',\n",
       " 'thai-dpo-sft-ss4k',\n",
       " 'RBAC_Test_Curl',\n",
       " 'test_upload',\n",
       " 'Coding_Generative_Inference_Dataset',\n",
       " 'E5_Large_V2_Training_MSMarco_Distillation',\n",
       " 'console_upload',\n",
       " 'Super_Glue_8k_SS_128k_vocab',\n",
       " 'yc_snapi_add_localmachine_test_13B_2451_rc3',\n",
       " '0606qa03orgadmin',\n",
       " 'LLaVA-example',\n",
       " 'openwebtext_ss4096_32k_vocab',\n",
       " 'aniket-e5-dataset-upload-trial5',\n",
       " 'GPT_1.5B_Inference_Dataset',\n",
       " 'test_dataset',\n",
       " 'Super_Glue_16k_SS',\n",
       " 'Caltech_256_Clip',\n",
       " 'Superglue_Sarashina_4k',\n",
       " 'Superglue_Sarashina_8k',\n",
       " '1029test',\n",
       " '1113AWS',\n",
       " 'smol_sql_dataset',\n",
       " 'publichealth-qa']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[dataset[\"dataset_name\"] for dataset in sambastudio_client.list_datasets()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'publichealth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Project configs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = {\n",
    "    'project_name': 'byoc fine-tuning project',\n",
    "    'project_description': 'this project will be used to test the BYOC and Fine-tuning e2e pipeline implementation'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 18:57:48,601 [INFO] Project with name 'byoc fine-tuning project' found with id 08b2e9e9-cebe-4f57-9271-a7e6c6f1561d\n",
      "2024-11-22 18:57:48,601 [INFO] Project with name 'byoc fine-tuning project' already exists with id '08b2e9e9-cebe-4f57-9271-a7e6c6f1561d', using it\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'08b2e9e9-cebe-4f57-9271-a7e6c6f1561d'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sambastudio_client.create_project(\n",
    "    project_name = project['project_name'],\n",
    "    project_description = project['project_description']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set train job config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = {\n",
    "    'job_name': 'e2e_fc_taining_job',\n",
    "    'job_description': 'e2e finetuning training job public health for suzume multilingual',\n",
    "    'job_type': 'train',\n",
    "    'model': model,\n",
    "    'model_version': '1',\n",
    "    'parallel_instances': '1',\n",
    "    'dataset_name': dataset_name,\n",
    "    'load_state': False,\n",
    "    'sub_path': '',\n",
    "    'hyperparams': {\n",
    "        \"batch_size\": 8,\n",
    "        \"do_eval\": False,\n",
    "        \"eval_steps\":5,\n",
    "        \"evaluation_strategy\": \"no\",\n",
    "        \"learning_rate\": 0.00001,\n",
    "        \"logging_steps\": 1,\n",
    "        \"lr_schedule\": \"fixed_lr\",\n",
    "        \"max_sequence_length\": 8192,\n",
    "        \"num_iterations\": 10,\n",
    "        \"prompt_loss_weight\": 0.0,\n",
    "        \"save_optimizer_state\": True,\n",
    "        \"save_steps\": 5,\n",
    "        \"skip_checkpoint\": False,\n",
    "        \"subsample_eval\": 0.01,\n",
    "        \"subsample_eval_seed\": 123,\n",
    "        \"use_token_type_ids\": True,\n",
    "        \"vocab_size\": 128256,\n",
    "        \"warmup_steps\": 0,\n",
    "        \"weight_decay\": 0.1,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sambastudio_client.run_training_job(\n",
    "    project_name = project[\"project_name\"],\n",
    "    job_name = job['job_name'],\n",
    "    job_description = job['job_description'],\n",
    "    job_type = job['job_type'],\n",
    "    model = job['model'],\n",
    "    model_version = job['model_version'],\n",
    "    dataset_name = job['dataset_name'],\n",
    "    parallel_instances = job['parallel_instances'],\n",
    "    load_state = job['load_state'],\n",
    "    sub_path = job['sub_path'],\n",
    "    rdu_arch = 'SN40L-8',\n",
    "    hyperparams = job['hyperparams']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 19:08:17,198 [INFO] Project with name 'byoc fine-tuning project' found with id 08b2e9e9-cebe-4f57-9271-a7e6c6f1561d\n",
      "2024-11-22 19:08:17,437 [INFO] Project with name 'byoc fine-tuning project' found with id 08b2e9e9-cebe-4f57-9271-a7e6c6f1561d\n",
      "2024-11-22 19:08:17,652 [INFO] Job with name 'e2e_fc_taining_job2' in project 'byoc fine-tuning project' found with id 'e2b76179-05e2-4e2c-9534-56b22e7c081e'\n",
      "2024-11-22 19:08:17,894 [INFO] Job `e2e_fc_taining_job2` with progress status: PENDING_RDU\n",
      "2024-11-22 19:09:18,148 [INFO] Job `e2e_fc_taining_job2` with progress status: FAILED\n",
      "2024-11-22 19:09:18,149 [ERROR] Job failed. Details: {'job_id': 'e2b76179-05e2-4e2c-9534-56b22e7c081e', 'job_name': 'e2e_fc_taining_job2', 'job_type': 'train', 'status': 'FAILED', 'time_created': '2024-11-23T00:06:50.391725000Z'}\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Job failed. Details: {'job_id': 'e2b76179-05e2-4e2c-9534-56b22e7c081e', 'job_name': 'e2e_fc_taining_job2', 'job_type': 'train', 'status': 'FAILED', 'time_created': '2024-11-23T00:06:50.391725000Z'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msambastudio_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_job_progress\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjob_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43me2e_fc_taining_job2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/ask_public_own/ai-starter-kit-snova/utils/fine_tuning/src/snsdk_wrapper.py:1117\u001b[0m, in \u001b[0;36mSnsdkWrapper.check_job_progress\u001b[0;34m(self, project_name, job_name, verbose, wait)\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[38;5;28;01melif\u001b[39;00m job_progress[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEXIT_WITH_1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFAILED\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m   1116\u001b[0m             logging\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJob failed. Details: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjob_progress\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1117\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJob failed. Details: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjob_progress\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1118\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m60\u001b[39m)\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m job_progress\n",
      "\u001b[0;31mException\u001b[0m: Job failed. Details: {'job_id': 'e2b76179-05e2-4e2c-9534-56b22e7c081e', 'job_name': 'e2e_fc_taining_job2', 'job_type': 'train', 'status': 'FAILED', 'time_created': '2024-11-23T00:06:50.391725000Z'}"
     ]
    }
   ],
   "source": [
    "sambastudio_client.check_job_progress(\n",
    "    project_name=project['project_name'],\n",
    "    job_name=job['job_name'],\n",
    "    wait=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Promote Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = sambastudio_client.list_checkpoints(\n",
    "    project_name=project['project_name'],\n",
    "    job_name=job['job_name'],\n",
    "    sort=True\n",
    ")\n",
    "checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Promoted checkpoint config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = {\n",
    "    'checkpoint_name': checkpoints[0]['checkpoint_name'],\n",
    "    'model_name': 'Suzume-Llama-3-8B-Multilingual-Publichealth',\n",
    "    'model_description': 'finetuned suzume multilingual in public health qa dataset',\n",
    "    'model_type': 'finetuned'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sambastudio_client.promote_checkpoint(\n",
    "    checkpoint_name = model_checkpoint['checkpoint_name'],\n",
    "    project_name=project['project_name'],\n",
    "    job_name=job['job_name'],\n",
    "    model_name=model_checkpoint['model_name'],\n",
    "    model_description=model_checkpoint['model_description'],\n",
    "    model_type=model_checkpoint['model_type']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sambastudio_client.list_models(filter_job_types=[\"deploy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete all saved training checkpoints, after promotion (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for checkpoint in checkpoints:\n",
    "    sambastudio_client.delete_checkpoint(checkpoint[\"checkpoint_name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streamlined Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = os.path.join(repo_dir, 'finetune_config.yaml')\n",
    "sambastudio_client = SnsdkWrapper(config_file)\n",
    "sambastudio_client.create_project()\n",
    "sambastudio_client.run_training_job()\n",
    "sambastudio_client.check_job_progress(wait=True)\n",
    "checkpoints = sambastudio_client.list_checkpoints(sort=True)\n",
    "sambastudio_client.promote_checkpoint(checkpoints[0]['checkpoint_name'])\n",
    "for checkpoint in checkpoints:\n",
    "    sambastudio_client.delete_checkpoint(checkpoint[\"checkpoint_name\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetuning_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
