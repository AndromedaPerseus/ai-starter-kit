{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNSDK Wrapper usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snsdk\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "utils_dir = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "repo_dir = os.path.abspath(os.path.join(utils_dir, \"..\"))\n",
    "\n",
    "sys.path.append(utils_dir)\n",
    "sys.path.append(repo_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## init wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.sambastudio_util import SnsdkWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snsdk = SnsdkWrapper(config_path=\"./config.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:49:22,408 [INFO] Project with name 'example project' found with id 51b1fe13-dcdb-41e3-8a78-514da36937c8\n",
      "2024-07-22 12:49:22,409 [INFO] Project with name 'example project' already exists with id '51b1fe13-dcdb-41e3-8a78-514da36937c8', using it\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'51b1fe13-dcdb-41e3-8a78-514da36937c8'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "snsdk.create_project()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'project_name': 'Benchmarking_Llama7b',\n",
       "  'project_id': '8d752994-bfd1-4eee-98a8-9ca0d50b8b37',\n",
       "  'status': 'Available',\n",
       "  'user_id': 'amit.kushwaha'},\n",
       " {'project_name': 'Shared',\n",
       "  'project_id': 'b9896d2e-5054-4937-978f-67413104d6ba',\n",
       "  'status': 'Available',\n",
       "  'user_id': 'varun.krishna'},\n",
       " {'project_name': 'SNSDK-E2E-Finetuning_Embeddings-Project',\n",
       "  'project_id': 'e2d0c5dc-894e-461a-9730-a814904db9f7',\n",
       "  'status': 'Available',\n",
       "  'user_id': 'rodrigo.maldonado'},\n",
       " {'project_name': 'snsdk_test_project',\n",
       "  'project_id': '78f670e3-6fce-4923-9e7f-390c8dc34be5',\n",
       "  'status': 'Available',\n",
       "  'user_id': 'jorge.piedrahita'},\n",
       " {'project_name': 'sql_finetuning',\n",
       "  'project_id': '302411f2-a7f7-42b3-a056-13f99330d3f9',\n",
       "  'status': 'Available',\n",
       "  'user_id': 'jorge.piedrahita'},\n",
       " {'project_name': 'Test_Finetune_Embeddings',\n",
       "  'project_id': '6f1d50bc-cd1b-4574-b901-70fe929f5aa4',\n",
       "  'status': 'Available',\n",
       "  'user_id': 'francesca.raimondi'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "snsdk.list_projects()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:49:41,470 [INFO] Project with name 'example project' found with id 51b1fe13-dcdb-41e3-8a78-514da36937c8\n",
      "2024-07-22 12:49:42,781 [INFO] Model 'Llama-2-7b-chat-hf' with id '6090d4ac-a7bd-4c46-b417-7f8e42cf7bdb' available for training and deployment found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'models': [{'model_id': '47ee0428-f9f8-6d8c-d02e-1ac778b83eb9', 'model_checkpoint_name': 'CLIP ViT-B-32 Backbone (Deprecated)', 'app_id': '6c14325a-1be7-4e48-b38f-19b33745fc3b', 'user_id': '', 'tenant_id': '', 'checkpoint_type': 'pretrained', 'model_size_gb': 0.57, 'dataset': '', 'dataset_info': '', 'dataset_url': '', 'path': 'common/checkpoints/47ee0428-f9f8-6d8c-d02e-1ac778b83eb9/checkpoint', 'sambanova_provided': True, 'version': 1, 'job_config': '', 'description': 'CLIP is a multi-modal neural network trained on (image, text) pairs.\\nUsing a ViT-B-32 Backbone for visual features and a language model for text features,\\nCLIP projects both modalities to a shared latent space, with the dot product between them used as a similarity score.\\nThis enables the network to predict relevant text snippets given an image, without directly optimizing for the task.\\nIt achieves this through its zero-shot capabilities and has been shown to match the performance of ResNet50 on ImageNet.\\nThis CLIP-ViT checkpoint (adapted from huggingface: `https://huggingface.co/laion/CLIP-ViT-B-32-laion2B-s34B-b79K`) provides the learned knowledge after pretraining on the Laion2B dataset.\\nThe primary usecase is Zero-shot image classification, as well as image and text retrieval.\\n', 'status': 'AvailableToDownload', 'params': None, 'metrics': '', 'steps': 0, 'application_field': 'vision', 'hyperparams': {'train': {}, 'batch_predict': {}, 'deploy': {}}, 'architecture': 'CLIP ViT B', 'pipeline': None, 'model_io': {'train': {'input': {'description': 'The input directory should include the images and a metafile `labels.csv`.\\nThe csv file should contain 4 columns: [image_path, description, subset, metadata].\\n- image_path: the relative path to a given image inside of the dataset directory\\n- description: A text description to encode.\\n- subset: can be one of train or validation. Both train and validation examples should be present.\\n- metadata: [Optional] Additional information relating to the given input data.\\nA fixed set of data transformations will be performed during training, which includes:\\n- Resize(size=[224, 224])\\n- Make RGB\\n- Min-Max Normalization (i.e. [0, 1])\\n- Channel-wise Standardization i.e. (x[ch,:,:]-mean[ch])/(std[ch]), ch=0,1,2; where mean=[0.481, 0.458, 0.408] and std=[0.269, 0.261, 0.276].\\n', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'infer': {'input': {'description': '', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'serve': {'input': {'description': 'The input directory should include the images and a metafile `predictions.csv`.\\nThe csv file should contain 4 columns: [image_path, description, subset, metadata].\\nThe columns with subset and metadata can be ignored.\\n- A given row should only contain one of [image_path or description]. That is, if the row has a value for `image_path` then the corresponding value for `description` should be left empty.\\n- image_path: the relative path to a given image inside of the dataset directory\\n- description: A text description to encode. \\nA fixed set of data augmentation will be performed during inference, which includes:\\n- Resize(size=[224, 224])\\n- Make RGB\\n- Min-Max Normalization (i.e. [0, 1])\\n- Channel-wise Standardization i.e. (x[ch,:,:]-mean[ch])/(std[ch]), ch=0,1,2; ; where mean=[0.481, 0.458, 0.408] and std=[0.269, 0.261, 0.276].\\n', 'example': ''}, 'output': {'description': \"The output of predictions is saved to a json file named `predictions.jsonl`.\\nIt contains a list of per image predictions.\\ne.g.\\n```\\n[\\n    {'input': 'path/relative/to/root_dir', type: img, 'embedding': [...]},\\n    {'input': <description>, type: text, 'embedding': [...]},\\n    {'input': <description>, type: text, 'embedding': [...]},\\n    ...\\n]\\n```\\n\", 'example': ''}}}, 'dataset_metadata': {'info': 'Pretraining Dataset\\nThis model was trained with a 2 Billion sample subset of LAION-5B (https://laion.ai/blog/laion-5b/) consisting of descriptions in the English language.\\n', 'url': ''}, 'preset_ids': [], 'system_prompt_ids': [], 'device': 'rdu', 'evaluation': {'ImageNet2012': {'dataset': {'info': 'ImageNet2012 validation set', 'url': ''}, 'metrics': {'accuracy': 0.66}}}, 'time_created': '2024-05-15 18:05:21.337416 +0000 UTC', 'time_updated': '2024-05-15 18:05:21.337416 +0000 UTC', 'app_name': 'CLIP', 'hidden': False, 'jobTypes': ['train', 'batch_predict', 'deploy'], 'shared_with': [], 'type': '', 'dependencies': [], 'replacement_model_id': '', 'replacement_model_name': '', 'deprecation_date': '', 'linked_coe_models': [], 'model_metadata': None, 'model_parallel_rdus': 0, 'metadata': {'labels': {}, 'creationTimestamp': '', 'updateTimestamp': '', 'resourceVersion': '', 'tenant': '', 'owner': '', 'permissions': ['ModelView', 'ModelExport']}}, {'model_id': 'b68727ff-4e10-4228-8ec9-17b345859c1d', 'model_checkpoint_name': 'ASRBertMergeDatabox', 'app_id': '199e9684-785c-4df0-8dc3-49e808d8eba5', 'user_id': '', 'tenant_id': '', 'checkpoint_type': 'base', 'model_size_gb': 0, 'dataset': '', 'dataset_info': '', 'dataset_url': '', 'path': '', 'sambanova_provided': True, 'version': 1, 'job_config': '', 'description': 'A Databox for merging ASR and Bert Outputs', 'status': 'Available', 'params': None, 'metrics': '', 'steps': 0, 'application_field': '', 'hyperparams': {'train': {}, 'batch_predict': {}, 'deploy': {}}, 'architecture': '', 'pipeline': None, 'model_io': {'train': {'input': {'description': '', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'infer': {'input': {'description': '', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'serve': {'input': {'description': '', 'example': ''}, 'output': {'description': '', 'example': ''}}}, 'dataset_metadata': {'info': '', 'url': ''}, 'preset_ids': [], 'system_prompt_ids': [], 'device': 'cpu', 'evaluation': {}, 'time_created': '2024-05-15 18:05:23.588164 +0000 UTC', 'time_updated': '2024-05-15 18:05:23.588164 +0000 UTC', 'app_name': 'Databox', 'hidden': True, 'jobTypes': [], 'shared_with': [], 'type': '', 'dependencies': [], 'replacement_model_id': '', 'replacement_model_name': '', 'deprecation_date': '', 'linked_coe_models': [], 'model_metadata': None, 'model_parallel_rdus': 0, 'metadata': {'labels': {}, 'creationTimestamp': '', 'updateTimestamp': '', 'resourceVersion': '', 'tenant': '', 'owner': '', 'permissions': ['ModelView', 'ModelExport']}}, {'model_id': '35dfd99d-888b-4331-b78a-ea5c744224ad', 'model_checkpoint_name': 'GPT_1.5B_Dialog_Act_Classification_Finetuned', 'app_id': '0498c73a-5c03-456a-a645-3820728cfcae', 'user_id': '', 'tenant_id': '', 'checkpoint_type': 'finetuned', 'model_size_gb': 6, 'dataset': '', 'dataset_info': '', 'dataset_url': '', 'path': 'common/checkpoints/35dfd99d-888b-4331-b78a-ea5c744224ad/checkpoint', 'sambanova_provided': True, 'version': 1, 'job_config': '', 'description': 'This is a traditionally classification head fine-tuned model on the SWDA (Switchboard) dataset, classifies 42 classes based on the utterance intention', 'status': 'AvailableToDownload', 'params': {'invalidates_checkpoint': {'max_seq_length': 256, 'num_intended_classes': 42}, 'modifiable': {'batch_size': 32, 'learning_rate': 5e-06, 'num_epochs': 2, 'warmup_steps': 0, 'weight_decay': 1}}, 'metrics': '', 'steps': 0, 'application_field': 'language', 'hyperparams': {'train': {}, 'batch_predict': {}, 'deploy': {}}, 'architecture': 'GPT 1.5B', 'pipeline': None, 'model_io': {'train': {'input': {'description': 'A csv/tsv file (needs to be tab delimited), with the following columns: index (optional), text, label.', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'infer': {'input': {'description': '', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'serve': {'input': {'description': '', 'example': ''}, 'output': {'description': 'Each piece of dialog is classified according to the following catagories \"appreciation\": 0, \"downplayer\": 1, \"wh-question\": 2, \"offers,_options_commits\": 3, \"yes_answers\": 4, \"statement-non-opinion\": 5, \"response_acknowledgement\": 6, \"self-talk\": 7, \"hedge\": 8, \"agree/accept\": 9, \"tag-question\": 10, \"other\": 11, \"signal-non-understanding\": 12, \"yes-no-question\": 13, \"repeat-phrase\": 14, \"quotation\": 15, \"dispreferred_answers\": 16, \"conventional-opening\": 17, \"maybe/accept-part\": 18, \"summarize/reformulate\": 19, \"no_answers\": 20, \"negative_non-no_answers\": 21, \"non-verbal\": 22, \"hold_before_answer/agreement\": 23, \"statement-opinion\": 24, \"rhetorical-questions\": 25, \"backchannel_in_question_form\": 26, \"acknowledge_(backchannel)\": 27, \"collaborative_completion\": 28, \"thanking\": 29, \"declarative_yes-no-question\": 30, \"conventional-closing\": 31, \"declarative_wh-question\": 32, \"or-clause\": 33, \"action-directive\": 34, \"apology\": 35, \"abandoned_or_turn-exit/uninterpretable\": 36, \"affirmative_non-yes_answers\": 37, \"reject\": 38, \"other_answers\": 39, \"open-question\": 40, \"3rd-party-talk\": 41', 'example': ''}}}, 'dataset_metadata': {'info': 'The Switchboard Dialog Act Corpus is a dataset that includes information about conversations taken from the Switchboard-1 Telephone Speech Corpus, with labels that indicate the type of speech in each turn.', 'url': 'https://web.stanford.edu/~jurafsky/ws97/manual.august1.html'}, 'preset_ids': [], 'system_prompt_ids': [], 'device': 'rdu', 'evaluation': {'eval_1': {'dataset': {'info': 'Switchboard evaluation set', 'url': ''}, 'metrics': {'test set accuracy': 0.88, 'validation set accuracy': 0.868}}}, 'time_created': '2024-05-15 18:05:35.045487 +0000 UTC', 'time_updated': '2024-05-15 18:05:35.045487 +0000 UTC', 'app_name': 'Dialog Act Classification', 'hidden': False, 'jobTypes': ['train', 'batch_predict', 'deploy'], 'shared_with': [], 'type': '', 'dependencies': [], 'replacement_model_id': '', 'replacement_model_name': '', 'deprecation_date': '', 'linked_coe_models': [], 'model_metadata': None, 'model_parallel_rdus': 0, 'metadata': {'labels': {}, 'creationTimestamp': '', 'updateTimestamp': '', 'resourceVersion': '', 'tenant': '', 'owner': '', 'permissions': ['ModelView', 'ModelExport']}}, {'model_id': 'cb800e99-809b-42d2-a6e2-11b3707ef433', 'model_checkpoint_name': 'GPT_13B_Human_Aligned_Instruction_Tuned_V2', 'app_id': '57f6a3c8-1f04-488a-bb39-3cfc5b4a5d7a', 'user_id': '', 'tenant_id': '', 'checkpoint_type': 'finetuned', 'model_size_gb': 49, 'dataset': '', 'dataset_info': '', 'dataset_url': '', 'path': 'common/checkpoints/cb800e99-809b-42d2-a6e2-11b3707ef433/checkpoint', 'sambanova_provided': True, 'version': 1, 'job_config': '', 'description': 'This model has the best zero shot performance. It is tuned to generate responses that are informative and easy to understand by humans. But this model should not be used in automated industrial pipelines because it tends to give verbose creative responses. When using this model, ensure that you enable sampling with a recommended temperature of 0.7. \\n\\nEvaluation:\\nThis model was evaluated by humans manually scoring the generations of the model. The set of prompts this was evaluated on is an internal SambaNova dataset that reflect the diverse use cases we intend our model to be used for including open ended generation, extractive QA, summarization, text classification, dialogue and more. This model performed very well, with almost 2x as many generations scored as \"completely correct\" compared to the previous instruction tuned delivery.\\n', 'status': 'AvailableToDownload', 'params': {'invalidates_checkpoint': {'max_seq_length': 2048, 'vocab_size': 50260}, 'modifiable': None}, 'metrics': '', 'steps': 0, 'application_field': 'language', 'hyperparams': {'train': {}, 'batch_predict': {}, 'deploy': {}}, 'architecture': 'GPT 13B', 'pipeline': None, 'model_io': {'train': {'input': {'description': '', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'infer': {'input': {'description': '', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'serve': {'input': {'description': '', 'example': ''}, 'output': {'description': 'The output is a concatenation of the prompt (input) string, and the generated completion.', 'example': ''}}}, 'dataset_metadata': {'info': 'This model is fine tuned on the instruction-tuned Version 2 checkpoint, which was trained from our base pretrained checkpoint.\\n\\nView the GPT_13B_Instruction_Tuned_V2 model card for more information on what data that checkpoint was trained on.\\n\\nThe fine-tuning process involved training the model on human alignment datasets for 16 epochs, utilizing a batch size of 128. The training ran for 720 steps which required less than 1 day of training time on a single node.\\n\\nSource Datasets\\n- Open Assistant (https://huggingface.co/datasets/OpenAssistant/oasst1, Apache 2.0)\\n- Dolly (https://huggingface.co/datasets/databricks/databricks-dolly-15k, CC BY-SA 3.0 license)  \\n', 'url': ''}, 'preset_ids': ['399e1997-7b44-4502-affc-747212946153', '777326d4-95e1-4130-9566-faf715382d19', '66203dab-6788-45a5-9154-72acbd760e93', '57203075-890d-4eee-86fb-a03ac93a09d8', '45081088-69ab-4a27-8508-e5e64e5b894b', '54a9f53c-9cbc-4044-a1d7-638b48094983', '530c8eee-1aa2-4671-aa45-098487314014'], 'system_prompt_ids': ['306d6f42-74ad-4d5c-b4eb-ac430aabaeb7'], 'device': 'rdu', 'evaluation': {}, 'time_created': '2024-05-15 18:05:49.134158 +0000 UTC', 'time_updated': '2024-05-15 18:05:49.134158 +0000 UTC', 'app_name': 'Generative Tuning 13B', 'hidden': False, 'jobTypes': ['train', 'batch_predict', 'deploy'], 'shared_with': [], 'type': '', 'dependencies': [], 'replacement_model_id': '', 'replacement_model_name': '', 'deprecation_date': '', 'linked_coe_models': [], 'model_metadata': None, 'model_parallel_rdus': 0, 'metadata': {'labels': {}, 'creationTimestamp': '', 'updateTimestamp': '', 'resourceVersion': '', 'tenant': '', 'owner': '', 'permissions': ['ModelView', 'ModelExport']}}, {'model_id': '0d3a9c87-99e8-4d22-af39-05ab91b84238', 'model_checkpoint_name': 'GPT_13B_Generative_Inference', 'app_id': '57f6a3c8-1f04-488a-bb39-3cfc5b4a5d7a', 'user_id': '', 'tenant_id': '', 'checkpoint_type': 'finetuned', 'model_size_gb': 49, 'dataset': '', 'dataset_info': '', 'dataset_url': '', 'path': 'common/checkpoints/0d3a9c87-99e8-4d22-af39-05ab91b84238/checkpoint', 'sambanova_provided': True, 'version': 1, 'job_config': '', 'description': 'Pre-trained large language models excel in predicting the next word in sentences, but are not aligned for generating the correct responses for many of the common use cases, such as summarization or question answering. However, instruction tuned checkpoints are trained to follow instructions, irrespective of the task it is presented with, without being specifically optimized for any particular task. But because this checkpoint is general, it does not have state of the art performance in any specific domain.\\n\\nThis checkpoint can serve two primary use cases. \\n1. It can be used as a foundational checkpoint for fine-tuning on any task that requires instruction following. By beginning with an instruction-tuned checkpoint instead of a generally pre-trained one, the fine tuned model’s performance will improve.\\n2. The checkpoint has generally good zero-shot performance on a range of tasks. It will yield reasonable responses and adhere to instructions in prompts, making it our best model yet for a playground experience\\n', 'status': 'AvailableToDownload', 'params': {'invalidates_checkpoint': {'max_seq_length': 2048, 'vocab_size': 50260}, 'modifiable': {'batch_size': 4, 'learning_rate': 1e-05, 'num_iterations': 50000, 'warmup_steps': 0, 'weight_decay': 0.1}}, 'metrics': '', 'steps': 0, 'application_field': 'language', 'hyperparams': {'train': {}, 'batch_predict': {}, 'deploy': {}}, 'architecture': 'GPT 13B', 'pipeline': None, 'model_io': {'train': {'input': {'description': '', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'infer': {'input': {'description': '', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'serve': {'input': {'description': '', 'example': ''}, 'output': {'description': 'The output is a concatenation of the prompt (input) string, and the generated completion.', 'example': ''}}}, 'dataset_metadata': {'info': '\\nThis model has been trained from scratch, starting from internally pre-trained 13 billion parameter GPT3 checkpoint. Then training was run using the FLAN V2 collective training recipe and dataset. Training this model took around 2 weeks on 2 racks.\\n\\nPreTraining Datasets\\nThis model was pretrained on C4PILE and saw a total of (possibly duplicated) 300B tokens during its pretraining run. This represents about 1 epoch on the entire dataset very roughly. \\n\\nInstruction Tuning Dataset\\nAfter pretraining, this model was instruction tuned on the FLAN monolith, which is a combination of P3, Chain of Thought, Dialogue, and Natural Instructions datasets. These datasets were formatted in two ways: zeroshot/fewshot training and with/without options. \\n\\nSize\\nInstruction Tuning was done on roughly 5B tokens (accounting for padding). \\n\\nSource Datasets\\n- The source datasets include:\\n- P3 (https://huggingface.co/datasets/bigscience/P3/tree/main)\\n- Natural Instructions (https://huggingface.co/datasets/Muennighoff/natural-instructions)\\n- FLAN Dialogue (https://huggingface.co/datasets/conceptofmind/flan_v2_filtered_dialog)\\n- CoT (https://huggingface.co/datasets/conceptofmind/flan_v2_cot_filtered)\\n\\nData Preparation\\nThe data was formatted as a combination of zeroshot and fewshot settings. In other words, for some of the training data, the prompt had additional demonstration data added into the context for the same completion. This was done for about 50% of the data.\\n\\nFinally, when applicable, the data also had some options added to it. Options are essentially giving the model a set of multiple choice options the model can choose to generate from. The model was required to choose from these options to generate its completion. This was done for about 50% of the data.\\n\\nIn other words,\\n- 25% of the data was fewshot w/ options.\\n- 25% of the data was fewshot w/o options.\\n- 25% of the data was zeroshot w/ options.\\n- 25% of the data was zeroshot w/o options.\\n', 'url': ''}, 'preset_ids': ['777326d4-95e1-4130-9566-faf715382d19', '66203dab-6788-45a5-9154-72acbd760e93', '244265ea-7513-494b-991f-152018aefbdb', 'd2524d91-2ee6-4aeb-b263-7ac08e41054c', '6e464df2-c694-4a0b-bf3c-2a9e1d08024e', 'fe93cf15-d527-4b39-a026-e5348975937b', '76bca2a3-837f-4383-af41-69a707afb86c', '12b7b141-d0ed-479a-8f10-aa38dd28f477', '461a3992-0f1d-447c-829d-414ba456aaed', '850d310f-1e9f-4a1c-b128-94b97147701a', 'c5d36145-0730-4119-bcea-ad4bb973873c', '33c08191-c2e8-462d-ba8e-7cd82d10dddf', '1a0c5df7-3a13-4d61-96f1-29396e0d40cc', 'e53c63a4-63fb-4de9-bedf-963ee170776a', '2a89dfdd-1eab-46b0-a31a-18f9c6dc7f2c', 'cb1c9a8e-9a94-48d9-b3b6-aa9e5494cc05', 'ad6affed-d3f5-429b-8759-af5badbdec2c', '3d730257-3d5d-46d1-ab17-6ffb13770b70', '2e6b8c10-1f48-459c-b45c-d94408ed667c'], 'system_prompt_ids': ['306d6f42-74ad-4d5c-b4eb-ac430aabaeb7'], 'device': 'rdu', 'evaluation': {'eval_1': {'dataset': {'info': 'HellaSwag - Extractive QA (multiple choice) 5-Shot', 'url': ''}, 'metrics': {'Normalized Accuracy': 0.6837}}, 'eval_10': {'dataset': {'info': 'XSum - Summarization 0-Shot', 'url': ''}, 'metrics': {'Rouge L': 24.87}}, 'eval_2': {'dataset': {'info': 'SciQ - Extractive QA (multiple choice) 5-Shot', 'url': ''}, 'metrics': {'Normalized Accuracy': 0.963}}, 'eval_3': {'dataset': {'info': 'Lambada - Sentence Completion 5-shot', 'url': ''}, 'metrics': {'Accuracy': 0.544}}, 'eval_4': {'dataset': {'info': 'CB - Natural Language Inference 5-Shot', 'url': ''}, 'metrics': {'F1 Score': 0.5984}}, 'eval_5': {'dataset': {'info': 'RTE - Natural Language Inference 5-Shot', 'url': ''}, 'metrics': {'Accuracy': 0.7256}}, 'eval_6': {'dataset': {'info': 'Winogrande - Coreference Resolution 5-Shot', 'url': ''}, 'metrics': {'Accuracy': 0.6843}}, 'eval_7': {'dataset': {'info': 'WiC - Word Sense Disambiguation 5-Shot', 'url': ''}, 'metrics': {'Accuracy': 0.5078}}, 'eval_8': {'dataset': {'info': 'MRPC - Paraphrase Identification 5-Shot', 'url': ''}, 'metrics': {'F1 Score': 0.8231}}, 'eval_9': {'dataset': {'info': 'CNNDailyMail - Summarization 0-Shot', 'url': ''}, 'metrics': {'Rouge L': 16.33}}}, 'time_created': '2024-05-15 18:05:49.752286 +0000 UTC', 'time_updated': '2024-05-15 18:05:49.752286 +0000 UTC', 'app_name': 'Generative Tuning 13B', 'hidden': False, 'jobTypes': ['train', 'batch_predict', 'deploy'], 'shared_with': [], 'type': '', 'dependencies': [], 'replacement_model_id': '', 'replacement_model_name': '', 'deprecation_date': '', 'linked_coe_models': [], 'model_metadata': None, 'model_parallel_rdus': 0, 'metadata': {'labels': {}, 'creationTimestamp': '', 'updateTimestamp': '', 'resourceVersion': '', 'tenant': '', 'owner': '', 'permissions': ['ModelView', 'ModelExport']}}, {'model_id': 'c7be342b-208b-4393-b5c2-496aa54eb917', 'model_checkpoint_name': 'GPT13B 2k SS HAv3', 'app_id': '57f6a3c8-1f04-488a-bb39-3cfc5b4a5d7a', 'user_id': '', 'tenant_id': '', 'checkpoint_type': 'finetuned', 'model_size_gb': 49, 'dataset': '', 'dataset_info': '', 'dataset_url': '', 'path': 'common/checkpoints/c7be342b-208b-4393-b5c2-496aa54eb917/checkpoint', 'sambanova_provided': True, 'version': 1, 'job_config': '', 'description': 'Pre-trained large language models excel in predicting the next word in sentences, but are not aligned for generating the correct responses for many of the common use cases, such as summarization or question answering. Human-facing applications in particular, such as for a chatbot, are a pain point. This checkpoint has been trained on human alignment data to optimize it for such applications. This checkpoint can serve two primary use cases:\\n1. It can be directly used for human-facing applications.\\n2. It can be used as a starting checkpoint for further alignment to instill further human-aligned qualities, such as politeness, helpfulness, or harmlessness. Some of its instruction-following capabilities may have been lost in the human alignment process, but it is still usable for instruction following applications.\\n\\nPlease run inference with do_sample=True and a sampling temperature >= 0.7 for best results.\\n', 'status': 'AvailableToDownload', 'params': {'invalidates_checkpoint': {'max_seq_length': 2048, 'vocab_size': 50260}, 'modifiable': None}, 'metrics': '', 'steps': 0, 'application_field': 'language', 'hyperparams': {'train': {}, 'batch_predict': {}, 'deploy': {}}, 'architecture': 'GPT 13B', 'pipeline': None, 'model_io': {'train': {'input': {'description': '', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'infer': {'input': {'description': '', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'serve': {'input': {'description': '', 'example': ''}, 'output': {'description': 'The output is a concatenation of the prompt (input) string, and the generated completion.', 'example': ''}}}, 'dataset_metadata': {'info': '\\nThe starting point for this checkpoint was the GPT 13B 8k SS checkpoint which had been trained on 550B pretraining tokens,\\nand further instruction-tuned on 39.3B tokens of instruction data. We then trained this checkpoint on the following datasets:\\n\\n1. databricks-dolly-15k\\n2. oasst1\\n\\nWe trained on this mixture for 16 epochs.\\n', 'url': ''}, 'preset_ids': ['399e1997-7b44-4502-affc-747212946153', '777326d4-95e1-4130-9566-faf715382d19', '66203dab-6788-45a5-9154-72acbd760e93', '57203075-890d-4eee-86fb-a03ac93a09d8', '45081088-69ab-4a27-8508-e5e64e5b894b', '54a9f53c-9cbc-4044-a1d7-638b48094983', '530c8eee-1aa2-4671-aa45-098487314014'], 'system_prompt_ids': ['306d6f42-74ad-4d5c-b4eb-ac430aabaeb7'], 'device': 'rdu', 'evaluation': {}, 'time_created': '2024-05-15 18:05:46.433901 +0000 UTC', 'time_updated': '2024-05-15 18:05:46.433901 +0000 UTC', 'app_name': 'Generative Tuning 13B', 'hidden': False, 'jobTypes': ['train', 'batch_predict', 'deploy'], 'shared_with': [], 'type': '', 'dependencies': [], 'replacement_model_id': '', 'replacement_model_name': '', 'deprecation_date': '', 'linked_coe_models': [], 'model_metadata': None, 'model_parallel_rdus': 0, 'metadata': {'labels': {}, 'creationTimestamp': '', 'updateTimestamp': '', 'resourceVersion': '', 'tenant': '', 'owner': '', 'permissions': ['ModelView', 'ModelExport']}}, {'model_id': '3cac564a-e822-46fb-a128-1ab34a9d93ed', 'model_checkpoint_name': 'GPT13B 2k SS ITv3', 'app_id': '57f6a3c8-1f04-488a-bb39-3cfc5b4a5d7a', 'user_id': '', 'tenant_id': '', 'checkpoint_type': 'finetuned', 'model_size_gb': 49, 'dataset': '', 'dataset_info': '', 'dataset_url': '', 'path': 'common/checkpoints/3cac564a-e822-46fb-a128-1ab34a9d93ed/checkpoint', 'sambanova_provided': True, 'version': 1, 'job_config': '', 'description': 'Pre-trained large language models excel in predicting the next word in sentences, but are not aligned for generating the correct responses for many of the common use cases, such as summarization or question answering. However, instruction tuned checkpoints are trained to follow instructions, irrespective of the task it is presented with, without being specifically optimized for any particular task. But because this checkpoint is general, it does not have state of the art performance in any specific domain.\\nThis checkpoint can serve two primary use cases.\\n1. It can be used as a foundational checkpoint for fine-tuning on any task that requires instruction following. By beginning with an instruction-tuned checkpoint instead of a generally pre-trained one, the fine tuned model’s performance will improve.\\n2. The checkpoint has generally good zero-shot performance on a range of tasks. It will yield reasonable responses and adhere to instructions in prompts, making it our best model yet for a playground experience.\\n\\nIf using this model for zero-shot or few-shot instruction-following, please run inference with do_sample=False to get short, accurate answers to your instructions.\\nIf using this model for open-ended generation, please run inference with do_sample=True and a sampling temperature >= 0.7 for best results.\\n', 'status': 'AvailableToDownload', 'params': {'invalidates_checkpoint': {'max_seq_length': 2048, 'vocab_size': 50260}, 'modifiable': None}, 'metrics': '', 'steps': 0, 'application_field': 'language', 'hyperparams': {'train': {}, 'batch_predict': {}, 'deploy': {}}, 'architecture': 'GPT 13B', 'pipeline': None, 'model_io': {'train': {'input': {'description': '', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'infer': {'input': {'description': '', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'serve': {'input': {'description': '', 'example': ''}, 'output': {'description': 'The output is a concatenation of the prompt (input) string, and the generated completion.', 'example': ''}}}, 'dataset_metadata': {'info': '\\nThe starting point for this checkpoint was the GPT 13B 2k SS C4Pile checkpoint which had been trained on 300B tokens.\\nTo adapt that checkpoint to a sequence size of 8k, we replicated the absolute positional embeddings (which are of\\nsize 2k) four times.\\n\\nWe then pretrained this checkpoint on the following datasets:\\n1. 160GB of Books Corpus\\n2. 450GB of 8k Sequence Size articles from Common Crawl\\n3. 450GB of randomly sampled Common Crawl\\nThese three datasets produced a total of 250B tokens.\\n\\nHence, this checkpoint has seen a total of 300B + 250B = 550B pretraining tokens.\\n\\nAfter pretraining, this checkpoint has been instruction tuned for 39.3B tokens.  The training dataset consisted of:\\n79% IT V2 data:  The data trained on by the IT V2 checkpoint\\n2% Long SS data:  Collection of Long Sequence datasets including SCROLLS, Arxiv, and Pubmed.  These datasets\\n  generally have articles longer than 2048 tokens.\\n19%  New Instruction Data: New instruction datasets targeting multi-hop QA, fill in the middle, aspect-based\\n  sentiment analysis, and chat-based instruction following verticals.\\n', 'url': ''}, 'preset_ids': ['40f810fa-a1ce-4f99-b20c-b42510ea697d', '32a49b27-3604-4468-8554-d7e245b3cf95', '5354e0c5-f085-476c-a613-c2fc0f7d1997', '74afaf37-fb28-4e4f-81bf-656bb5bff375', 'fd9ee9b5-d345-4ee2-a529-7b5b310be94a', '93d68cd3-ee0e-4360-98e3-fad3f8b685bc'], 'system_prompt_ids': ['306d6f42-74ad-4d5c-b4eb-ac430aabaeb7'], 'device': 'rdu', 'evaluation': {'eval_1': {'dataset': {'info': 'BoolQ - Question Answering (yes / no)', 'url': ''}, 'metrics': {'Exact Match': 0.664}}, 'eval_10': {'dataset': {'info': 'Natural Instructions - General Instruction Following', 'url': ''}, 'metrics': {'Rouge L': 0.407}}, 'eval_11': {'dataset': {'info': 'Public Pool of Prompts (P3) - General Instruction Following', 'url': ''}, 'metrics': {'Rouge L': 0.354}}, 'eval_12': {'dataset': {'info': 'ZeroScrolls - Aggregate Performance on Long Sequence Tasks', 'url': ''}, 'metrics': {'Average': 18.03}}, 'eval_2': {'dataset': {'info': 'NarrativeQA - Question Answering', 'url': ''}, 'metrics': {'F1': 0.719}}, 'eval_3': {'dataset': {'info': 'Natural Questions (closed book) - Coreference Resolution 5-Shot', 'url': ''}, 'metrics': {'Accuracy': 0.233}}, 'eval_4': {'dataset': {'info': 'Natural Questions (open book)', 'url': ''}, 'metrics': {'Accuracy': 0.735}}, 'eval_5': {'dataset': {'info': 'TruthfulQA', 'url': ''}, 'metrics': {'F1 Score': 0.24}}, 'eval_6': {'dataset': {'info': 'CNN / DailyMail', 'url': ''}, 'metrics': {'Rouge L': 0.132}}, 'eval_7': {'dataset': {'info': 'XSum - Summarization', 'url': ''}, 'metrics': {'Rouge L': 0.151}}, 'eval_8': {'dataset': {'info': 'IMDB', 'url': ''}, 'metrics': {'Rouge L': 0.961}}, 'eval_9': {'dataset': {'info': 'RAFT', 'url': ''}, 'metrics': {'Rouge L': 0.605}}}, 'time_created': '2024-05-15 18:05:46.866907 +0000 UTC', 'time_updated': '2024-05-15 18:05:46.866907 +0000 UTC', 'app_name': 'Generative Tuning 13B', 'hidden': False, 'jobTypes': ['train', 'batch_predict', 'deploy'], 'shared_with': [], 'type': '', 'dependencies': [], 'replacement_model_id': '', 'replacement_model_name': '', 'deprecation_date': '', 'linked_coe_models': [], 'model_metadata': None, 'model_parallel_rdus': 0, 'metadata': {'labels': {}, 'creationTimestamp': '', 'updateTimestamp': '', 'resourceVersion': '', 'tenant': '', 'owner': '', 'permissions': ['ModelView', 'ModelExport']}}, {'model_id': '673ea90a-3d25-43ed-9965-55c393e91ddc', 'model_checkpoint_name': 'GPT_1.5B_GT_Finetuned', 'app_id': 'e681c226-86be-40b2-9380-d2de11b19842', 'user_id': '', 'tenant_id': '', 'checkpoint_type': 'finetuned', 'model_size_gb': 0, 'dataset': '', 'dataset_info': '', 'dataset_url': '', 'path': 'common/checkpoints/673ea90a-3d25-43ed-9965-55c393e91ddc/checkpoint', 'sambanova_provided': True, 'version': 1, 'job_config': '', 'description': 'Perform a range of generative tasks like question answering, and other prompt-completion tasks.Ready to deploy for inference, or further fine tune', 'status': 'AvailableToDownload', 'params': None, 'metrics': '', 'steps': 0, 'application_field': 'language', 'hyperparams': {'train': {}, 'batch_predict': {}, 'deploy': {}}, 'architecture': 'GPT 1.5B', 'pipeline': None, 'model_io': {'train': {'input': {'description': '', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'infer': {'input': {'description': '', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'serve': {'input': {'description': '', 'example': ''}, 'output': {'description': '', 'example': ''}}}, 'dataset_metadata': {'info': '', 'url': ''}, 'preset_ids': ['777326d4-95e1-4130-9566-faf715382d19', '66203dab-6788-45a5-9154-72acbd760e93', '244265ea-7513-494b-991f-152018aefbdb', 'd2524d91-2ee6-4aeb-b263-7ac08e41054c', '6e464df2-c694-4a0b-bf3c-2a9e1d08024e', 'fe93cf15-d527-4b39-a026-e5348975937b', '76bca2a3-837f-4383-af41-69a707afb86c', '12b7b141-d0ed-479a-8f10-aa38dd28f477', '461a3992-0f1d-447c-829d-414ba456aaed', '850d310f-1e9f-4a1c-b128-94b97147701a', 'c5d36145-0730-4119-bcea-ad4bb973873c', '33c08191-c2e8-462d-ba8e-7cd82d10dddf', '1a0c5df7-3a13-4d61-96f1-29396e0d40cc', 'e53c63a4-63fb-4de9-bedf-963ee170776a', '2a89dfdd-1eab-46b0-a31a-18f9c6dc7f2c', 'cb1c9a8e-9a94-48d9-b3b6-aa9e5494cc05', 'ad6affed-d3f5-429b-8759-af5badbdec2c', '3d730257-3d5d-46d1-ab17-6ffb13770b70', '2e6b8c10-1f48-459c-b45c-d94408ed667c'], 'system_prompt_ids': [], 'device': 'rdu', 'evaluation': {}, 'time_created': '2024-05-15 18:06:02.820846 +0000 UTC', 'time_updated': '2024-05-15 18:06:02.820846 +0000 UTC', 'app_name': 'Generative Tuning 1.5B', 'hidden': False, 'jobTypes': ['train', 'batch_predict', 'deploy'], 'shared_with': [], 'type': '', 'dependencies': [], 'replacement_model_id': '', 'replacement_model_name': '', 'deprecation_date': '', 'linked_coe_models': [], 'model_metadata': None, 'model_parallel_rdus': 0, 'metadata': {'labels': {}, 'creationTimestamp': '', 'updateTimestamp': '', 'resourceVersion': '', 'tenant': '', 'owner': '', 'permissions': ['ModelView', 'ModelExport']}}, {'model_id': 'f99ce9fe-968a-4d18-b29b-4b3faf75620c', 'model_checkpoint_name': 'meta-llama-guard-2-8b', 'app_id': 'ad39e323-9878-4914-8e29-82c9f2939475', 'user_id': '', 'tenant_id': '', 'checkpoint_type': 'finetuned', 'model_size_gb': 15, 'dataset': '', 'dataset_info': '', 'dataset_url': '', 'path': 'common/checkpoints/f99ce9fe-968a-4d18-b29b-4b3faf75620c/checkpoint', 'sambanova_provided': True, 'version': 1, 'job_config': '', 'description': \"Meta Llama Guard 2 is an 8B parameter Llama 3-based LLM safeguard model. Similar to Llama Guard, it can be used for classifying content in both LLM inputs (prompt classification) and in LLM responses (response classification). It acts as an LLM – it generates text in its output that indicates whether a given prompt or response is safe or unsafe, and if unsafe, it also lists the content categories violated. The model is trained to predict safety labels on the 11 categories shown below, based on the MLCommons taxonomy of hazards.\\n\\nHarm categories\\nS1: Violent Crimes\\t\\nS2: Non-Violent Crimes\\nS3: Sex-Related Crimes\\t\\nS4: Child Sexual Exploitation\\nS5: Specialized Advice\\t\\nS6: Privacy\\nS7: Intellectual Property\\t\\nS8: Indiscriminate Weapons\\nS9: Hate\\t\\nS10: Suicide & Self-Harm\\nS11: Sexual Content\\n\\nPlease refer to the provided links for detailed information on training dataset, output categories, licensing agreements, and terms of use. \\nLlama 3 Community License Agreement: https://llama.meta.com/llama3/license/\\nView on HuggingFace: https://huggingface.co/meta-llama/Meta-Llama-Guard-2-8B\\n\\nTo ensure this model responds correctly, please format the input prompt according to the below prompt template.\\n<br>\\n\\n## Prompt template\\n<|begin_of_text|>[INST] Task: Check if there is unsafe content in '{{ role }}' messages in conversations according our safety policy with the below categories.\\n\\n\\\\<BEGIN UNSAFE CONTENT CATEGORIES\\\\>\\n{{ unsafe_categories }}\\n\\\\<END UNSAFE CONTENT CATEGORIES\\\\>\\n\\n\\\\<BEGIN CONVERSATION\\\\>\\n\\nUser: {{ user_message_1 }}\\n\\nAgent: {{ model_answer_1 }}\\n\\n\\\\<END CONVERSATION\\\\>\\n\\nProvide your safety assessment for ONLY THE LAST {{ role }} in the above conversation:\\n- First line must read 'safe' or 'unsafe'.\\n- If unsafe, a second line must include a comma-separated list of violated categories. [/INST]\\n\", 'status': 'AvailableToDownload', 'params': {'invalidates_checkpoint': {'max_seq_length': 8192, 'model_parameter_count': '8b', 'vocab_size': 128384}, 'modifiable': None}, 'metrics': '', 'steps': 0, 'application_field': 'language', 'hyperparams': {'train': {}, 'batch_predict': {}, 'deploy': {}}, 'architecture': 'Llama v3', 'pipeline': None, 'model_io': {'train': {'input': {'description': 'A folder containing .hdf5 files produced by the SambaNova generative_data_prep package (please see documentation for usage instructions).', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'infer': {'input': {'description': '', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'serve': {'input': {'description': '', 'example': ''}, 'output': {'description': '', 'example': ''}}}, 'dataset_metadata': {'info': 'View on HuggingFace: https://huggingface.co/meta-llama/Meta-Llama-Guard-2-8B\\n', 'url': ''}, 'preset_ids': [], 'system_prompt_ids': [], 'device': 'rdu', 'evaluation': {}, 'time_created': '2024-06-05 22:06:33.865277 +0000 UTC', 'time_updated': '2024-06-05 22:06:33.865277 +0000 UTC', 'app_name': 'Llama 3', 'hidden': False, 'jobTypes': ['train'], 'shared_with': [], 'type': '', 'dependencies': [], 'replacement_model_id': '', 'replacement_model_name': '', 'deprecation_date': '', 'linked_coe_models': [], 'model_metadata': None, 'model_parallel_rdus': 0, 'metadata': {'labels': {}, 'creationTimestamp': '', 'updateTimestamp': '', 'resourceVersion': '', 'tenant': '', 'owner': '', 'permissions': ['ModelView', 'ModelExport']}}, {'model_id': '877fa680-b45f-4fff-a57a-0a3fcd2d0fd7', 'model_checkpoint_name': 'GPT13B 8k SS HAv3', 'app_id': '57f6a3c8-1f04-488a-bb39-3cfc5b4a5d7a', 'user_id': '', 'tenant_id': '', 'checkpoint_type': 'finetuned', 'model_size_gb': 49, 'dataset': '', 'dataset_info': '', 'dataset_url': '', 'path': 'common/checkpoints/877fa680-b45f-4fff-a57a-0a3fcd2d0fd7/checkpoint', 'sambanova_provided': True, 'version': 1, 'job_config': '', 'description': 'Pre-trained large language models excel in predicting the next word in sentences, but are not aligned for generating the correct responses for many of the common use cases, such as summarization or question answering. Human-facing applications in particular, such as for a chatbot, are a pain point. This checkpoint has been trained on human alignment data to optimize it for such applications. This checkpoint can serve two primary use cases:\\n1. It can be directly used for human-facing applications.\\n2. It can be used as a starting checkpoint for further alignment to instill further human-aligned qualities, such as politeness, helpfulness, or harmlessness. Some of its instruction-following capabilities may have been lost in the human alignment process, but it is still usable for instruction following applications.\\n\\nThis checkpoint retains the ability to consume and generate text from long sequences up to 8192 tokens.\\nPlease run inference with do_sample=True and a sampling temperature >= 0.7 for best results.\\n', 'status': 'AvailableToDownload', 'params': {'invalidates_checkpoint': {'max_seq_length': 8192, 'vocab_size': 50260}, 'modifiable': None}, 'metrics': '', 'steps': 0, 'application_field': 'language', 'hyperparams': {'train': {}, 'batch_predict': {}, 'deploy': {}}, 'architecture': 'GPT 13B', 'pipeline': None, 'model_io': {'train': {'input': {'description': '', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'infer': {'input': {'description': '', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'serve': {'input': {'description': '', 'example': ''}, 'output': {'description': 'The output is a concatenation of the prompt (input) string, and the generated completion.', 'example': ''}}}, 'dataset_metadata': {'info': '\\nThe starting point for this checkpoint was the GPT 13B 8k SS checkpoint which had been trained on 550B pretraining tokens,\\nand further instruction-tuned on 39.3B tokens of instruction data. We then trained this checkpoint on the following datasets:\\n\\n1. databricks-dolly-15k\\n2. oasst1\\n\\nWe trained on this mixture for 16 epochs.\\n', 'url': ''}, 'preset_ids': [], 'system_prompt_ids': ['306d6f42-74ad-4d5c-b4eb-ac430aabaeb7'], 'device': 'rdu', 'evaluation': {}, 'time_created': '2024-05-15 18:05:47.595074 +0000 UTC', 'time_updated': '2024-05-15 18:05:47.595074 +0000 UTC', 'app_name': 'Generative Tuning 13B', 'hidden': False, 'jobTypes': ['train', 'batch_predict', 'deploy'], 'shared_with': [], 'type': '', 'dependencies': [], 'replacement_model_id': '', 'replacement_model_name': '', 'deprecation_date': '', 'linked_coe_models': [], 'model_metadata': None, 'model_parallel_rdus': 0, 'metadata': {'labels': {}, 'creationTimestamp': '', 'updateTimestamp': '', 'resourceVersion': '', 'tenant': '', 'owner': '', 'permissions': ['ModelView', 'ModelExport']}}, {'model_id': 'c60a7f67-d9b2-46fd-9c9b-d3f02fb2b6e0', 'model_checkpoint_name': 'GPT13B 8k SS ITv3', 'app_id': '57f6a3c8-1f04-488a-bb39-3cfc5b4a5d7a', 'user_id': '', 'tenant_id': '', 'checkpoint_type': 'finetuned', 'model_size_gb': 49, 'dataset': '', 'dataset_info': '', 'dataset_url': '', 'path': 'common/checkpoints/c60a7f67-d9b2-46fd-9c9b-d3f02fb2b6e0/checkpoint', 'sambanova_provided': True, 'version': 1, 'job_config': '', 'description': 'Pre-trained large language models excel in predicting the next word in sentences, but are not aligned for generating the correct responses for many of the common use cases, such as summarization or question answering. However, instruction tuned checkpoints are trained to follow instructions, irrespective of the task it is presented with, without being specifically optimized for any particular task. But because this checkpoint is general, it does not have state of the art performance in any specific domain.\\nThis checkpoint can serve two primary use cases.\\n1. It can be used as a foundational checkpoint for fine-tuning on any task that requires instruction following. By beginning with an instruction-tuned checkpoint instead of a generally pre-trained one, the fine tuned model’s performance will improve.\\n2. The checkpoint has generally good zero-shot performance on a range of tasks. It will yield reasonable responses and adhere to instructions in prompts, making it our best model yet for a playground experience\\n3. This checkpoint has a long context so can be used to summarize long documents.\\n\\nIf using this model for zero-shot or few-shot instruction-following, please run inference with do_sample=False to get short, accurate answers to your instructions.\\nIf using this model for open-ended generation, please run inference with do_sample=True and a sampling temperature >= 0.7 for best results.\\n', 'status': 'AvailableToDownload', 'params': {'invalidates_checkpoint': {'max_seq_length': 8192, 'vocab_size': 50260}, 'modifiable': None}, 'metrics': '', 'steps': 0, 'application_field': 'language', 'hyperparams': {'train': {}, 'batch_predict': {}, 'deploy': {}}, 'architecture': 'GPT 13B', 'pipeline': None, 'model_io': {'train': {'input': {'description': '', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'infer': {'input': {'description': '', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'serve': {'input': {'description': '', 'example': ''}, 'output': {'description': 'The output is a concatenation of the prompt (input) string, and the generated completion.', 'example': ''}}}, 'dataset_metadata': {'info': '\\nThe starting point for this checkpoint was the GPT 13B 2k SS C4Pile checkpoint which had been trained on 300B tokens.\\nTo adapt that checkpoint to a sequence size of 8k, we replicated the absolute positional embeddings (which are of\\nsize 2k) four times.\\n\\nWe then pretrained this checkpoint on the following datasets:\\n1. 160GB of Books Corpus\\n2. 450GB of 8k Sequence Size articles from Common Crawl\\n3. 450GB of randomly sampled Common Crawl\\nThese three datasets produced a total of 250B tokens.\\n\\nHence, this checkpoint has seen a total of 300B + 250B = 550B pretraining tokens.\\n\\nAfter pretraining, this checkpoint has been instruction tuned for 39.3B tokens.  The training dataset consisted of:\\n79% IT V2 data:  The data trained on by the IT V2 checkpoint\\n2% Long SS data:  Collection of Long Sequence datasets including SCROLLS, Arxiv, and Pubmed.  These datasets\\n  generally have articles longer than 2048 tokens.\\n19%  New Instruction Data: New instruction datasets targeting multi-hop QA, fill in the middle, aspect-based\\n  sentiment analysis, and chat-based instruction following verticals.\\n', 'url': ''}, 'preset_ids': ['40f810fa-a1ce-4f99-b20c-b42510ea697d', '32a49b27-3604-4468-8554-d7e245b3cf95', '5354e0c5-f085-476c-a613-c2fc0f7d1997', '74afaf37-fb28-4e4f-81bf-656bb5bff375', 'fd9ee9b5-d345-4ee2-a529-7b5b310be94a', '93d68cd3-ee0e-4360-98e3-fad3f8b685bc'], 'system_prompt_ids': ['306d6f42-74ad-4d5c-b4eb-ac430aabaeb7'], 'device': 'rdu', 'evaluation': {'eval_1': {'dataset': {'info': 'BoolQ - Question Answering (yes / no)', 'url': ''}, 'metrics': {'Exact Match': 0.664}}, 'eval_10': {'dataset': {'info': 'Natural Instructions - General Instruction Following', 'url': ''}, 'metrics': {'Rouge L': 0.407}}, 'eval_11': {'dataset': {'info': 'Public Pool of Prompts (P3) - General Instruction Following', 'url': ''}, 'metrics': {'Rouge L': 0.354}}, 'eval_12': {'dataset': {'info': 'ZeroScrolls - Aggregate Performance on Long Sequence Tasks', 'url': ''}, 'metrics': {'Average': 18.03}}, 'eval_2': {'dataset': {'info': 'NarrativeQA - Question Answering', 'url': ''}, 'metrics': {'F1': 0.719}}, 'eval_3': {'dataset': {'info': 'Natural Questions (closed book) - Coreference Resolution 5-Shot', 'url': ''}, 'metrics': {'Accuracy': 0.233}}, 'eval_4': {'dataset': {'info': 'Natural Questions (open book)', 'url': ''}, 'metrics': {'Accuracy': 0.735}}, 'eval_5': {'dataset': {'info': 'TruthfulQA', 'url': ''}, 'metrics': {'F1 Score': 0.24}}, 'eval_6': {'dataset': {'info': 'CNN / DailyMail', 'url': ''}, 'metrics': {'Rouge L': 0.132}}, 'eval_7': {'dataset': {'info': 'XSum - Summarization', 'url': ''}, 'metrics': {'Rouge L': 0.151}}, 'eval_8': {'dataset': {'info': 'IMDB', 'url': ''}, 'metrics': {'Rouge L': 0.961}}, 'eval_9': {'dataset': {'info': 'RAFT', 'url': ''}, 'metrics': {'Rouge L': 0.605}}}, 'time_created': '2024-05-15 18:05:47.941266 +0000 UTC', 'time_updated': '2024-05-15 18:05:47.941266 +0000 UTC', 'app_name': 'Generative Tuning 13B', 'hidden': False, 'jobTypes': ['train', 'batch_predict', 'deploy'], 'shared_with': [], 'type': '', 'dependencies': [], 'replacement_model_id': '', 'replacement_model_name': '', 'deprecation_date': '', 'linked_coe_models': [], 'model_metadata': None, 'model_parallel_rdus': 0, 'metadata': {'labels': {}, 'creationTimestamp': '', 'updateTimestamp': '', 'resourceVersion': '', 'tenant': '', 'owner': '', 'permissions': ['ModelView', 'ModelExport']}}, {'model_id': '67461a4d-0140-4239-8584-772bb6998e2e', 'model_checkpoint_name': 'GPT 13B 8k SS SN Pretrained', 'app_id': '57f6a3c8-1f04-488a-bb39-3cfc5b4a5d7a', 'user_id': '', 'tenant_id': '', 'checkpoint_type': 'pretrained', 'model_size_gb': 51, 'dataset': '', 'dataset_info': '', 'dataset_url': '', 'path': 'common/checkpoints/67461a4d-0140-4239-8584-772bb6998e2e/checkpoint', 'sambanova_provided': True, 'version': 1, 'job_config': '', 'description': 'This checkpoint is a pretrained checkpoint that has been trained on longer sequences of data (up to 8192 tokens).\\nThis checkpoint can be used in the following ways:\\n1.  As a starting point to do further domain specific pretraining.\\n2.  As a starting point for instruction tuning.\\n', 'status': 'AvailableToDownload', 'params': {'invalidates_checkpoint': {'max_seq_length': 8192, 'vocab_size': 50260}, 'modifiable': None}, 'metrics': '', 'steps': 0, 'application_field': 'language', 'hyperparams': {'train': {}, 'batch_predict': {}, 'deploy': {}}, 'architecture': 'GPT 13B', 'pipeline': None, 'model_io': {'train': {'input': {'description': '', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'infer': {'input': {'description': '', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'serve': {'input': {'description': '', 'example': ''}, 'output': {'description': 'The output is a concatenation of the prompt (input) string, and the generated completion.', 'example': ''}}}, 'dataset_metadata': {'info': '\\nThe starting point for this checkpoint was the GPT 13B 2k SS C4Pile checkpoint which had been trained on 300B tokens.\\nTo adapt that checkpoint to a sequence size of 8k, we replicated the absolute positional embeddings (which are of\\nsize 2k) four times.\\n\\nWe then pretrained this checkpoint on the following datasets:\\n1. 160GB of Books Corpus\\n2. 450GB of 8k Sequence Size articles from Common Crawl\\n3. 450GB of randomly sampled Common Crawl\\nThese three datasets produced a total of 250B tokens.\\n\\nHence, this checkpoint has seen a total of 300B + 250B = 550B tokens.\\n', 'url': ''}, 'preset_ids': [], 'system_prompt_ids': [], 'device': 'rdu', 'evaluation': {'eval_1': {'dataset': {'info': 'ANLI - Natural Language Inference 0-shot', 'url': ''}, 'metrics': {'Accuracy': 0.333}}, 'eval_2': {'dataset': {'info': 'ARC Challenge - Reasoning 0-shot', 'url': ''}, 'metrics': {'Normalized Accuracy': 0.3584}}, 'eval_3': {'dataset': {'info': 'ARC Easy - Reasoning 0-shot', 'url': ''}, 'metrics': {'Normalized Accuracy': 0.6389}}, 'eval_4': {'dataset': {'info': 'BoolQ - Question Answering (yes / no) 0-shot', 'url': ''}, 'metrics': {'Accuracy': 0.6642}}, 'eval_5': {'dataset': {'info': 'HellaSwag - Extractive QA (multiple choice) 0-Shot', 'url': ''}, 'metrics': {'Normalized Accuracy': 0.6958}}, 'eval_6': {'dataset': {'info': 'Lambada OpenAI - Sentence Completion 0-shot', 'url': ''}, 'metrics': {'Accuracy': 0.6897}}, 'eval_7': {'dataset': {'info': 'OpenBookQA - Question Answering 0-shot', 'url': ''}, 'metrics': {'Normalized Accuracy': 0.418}}, 'eval_8': {'dataset': {'info': 'PIQA - Question Answering 0-shot', 'url': ''}, 'metrics': {'Normalized Accuracy': 0.7867}}, 'eval_9': {'dataset': {'info': 'Winogrande - Coreference Resolution 0-Shot', 'url': ''}, 'metrics': {'Accuracy': 0.6504}}}, 'time_created': '2024-05-15 18:05:48.476065 +0000 UTC', 'time_updated': '2024-05-15 18:05:48.476065 +0000 UTC', 'app_name': 'Generative Tuning 13B', 'hidden': False, 'jobTypes': ['train', 'batch_predict', 'deploy'], 'shared_with': [], 'type': '', 'dependencies': [], 'replacement_model_id': '', 'replacement_model_name': '', 'deprecation_date': '', 'linked_coe_models': [], 'model_metadata': None, 'model_parallel_rdus': 0, 'metadata': {'labels': {}, 'creationTimestamp': '', 'updateTimestamp': '', 'resourceVersion': '', 'tenant': '', 'owner': '', 'permissions': ['ModelView', 'ModelExport']}}, {'model_id': '7c7ea299-9a29-4d42-a532-f5f0dfd1a403', 'model_checkpoint_name': 'GPT_1.5B_Base_Model', 'app_id': 'e681c226-86be-40b2-9380-d2de11b19842', 'user_id': '', 'tenant_id': '', 'checkpoint_type': 'base', 'model_size_gb': 6, 'dataset': '', 'dataset_info': '', 'dataset_url': '', 'path': '', 'sambanova_provided': True, 'version': 1, 'job_config': '', 'description': 'This is a randomly initialized model, meant to be used to kick off a pre-training job. \\n\\nGenerally speaking, the process of pre-training is expensive both in terms of compute and data. For most use cases, it will be better to fine tune one of the provided checkpoints, rather than starting from scratch.\\n      \\n', 'status': 'AvailableToDownload', 'params': None, 'metrics': '', 'steps': 0, 'application_field': 'language', 'hyperparams': {'train': {}, 'batch_predict': {}, 'deploy': {}}, 'architecture': 'GPT 1.5B', 'pipeline': None, 'model_io': {'train': {'input': {'description': 'The training data can be prepared following the generative data preparation guide in the documentation.', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'infer': {'input': {'description': '', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'serve': {'input': {'description': '', 'example': ''}, 'output': {'description': '', 'example': ''}}}, 'dataset_metadata': {'info': 'N/A\\n', 'url': ''}, 'preset_ids': [], 'system_prompt_ids': [], 'device': 'rdu', 'evaluation': {}, 'time_created': '2024-05-15 18:06:02.305555 +0000 UTC', 'time_updated': '2024-05-15 18:06:02.305555 +0000 UTC', 'app_name': 'Generative Tuning 1.5B', 'hidden': False, 'jobTypes': ['train'], 'shared_with': [], 'type': '', 'dependencies': [], 'replacement_model_id': '', 'replacement_model_name': '', 'deprecation_date': '', 'linked_coe_models': [], 'model_metadata': None, 'model_parallel_rdus': 0, 'metadata': {'labels': {}, 'creationTimestamp': '', 'updateTimestamp': '', 'resourceVersion': '', 'tenant': '', 'owner': '', 'permissions': ['ModelView', 'ModelExport']}}, {'model_id': 'a306fe34-f7ad-4c92-b51d-04bf772dcf44', 'model_checkpoint_name': 'GPT_13B_Instruction_Tuned_V2', 'app_id': '57f6a3c8-1f04-488a-bb39-3cfc5b4a5d7a', 'user_id': '', 'tenant_id': '', 'checkpoint_type': 'finetuned', 'model_size_gb': 49, 'dataset': '', 'dataset_info': '', 'dataset_url': '', 'path': 'common/checkpoints/a306fe34-f7ad-4c92-b51d-04bf772dcf44/checkpoint', 'sambanova_provided': True, 'version': 1, 'job_config': '', 'description': 'This instruction tuned checkpoint has two main functions. \\n1. It can be used as a foundational checkpoint for fine-tuning on any task that requires instruction following. By beginning with an instruction-tuned checkpoint instead of a generally pre-trained one, the fine tuned model’s performance will improve.\\n2. It can be used for \"industrial\" use cases with few shot prompting. Industrial use cases are automated pipelines, and where verbosity is unnecessary and not desirable and there is no direct human interaction with the model. An example of this would be document classification.\\n\\nWhen using this model ensure that you disable sampling. This model is also most effective when used for inference under the few shot setting. \\n\\nIf you want to interact with a general checkpoint in a playground setting, please use the human aligned version of this model GPT_13B_Human_Aligned_Instruction_Tuned_V2.\\n\\nThis model has the new capability of text improvement where if you ask the model to improve the quality, fluency, clarity, simplicity, or neutrality of text it will respond with improved text.\\n', 'status': 'AvailableToDownload', 'params': {'invalidates_checkpoint': {'max_seq_length': 2048, 'vocab_size': 50260}, 'modifiable': None}, 'metrics': '', 'steps': 0, 'application_field': 'language', 'hyperparams': {'train': {}, 'batch_predict': {}, 'deploy': {}}, 'architecture': 'GPT 13B', 'pipeline': None, 'model_io': {'train': {'input': {'description': '', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'infer': {'input': {'description': '', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'serve': {'input': {'description': '', 'example': ''}, 'output': {'description': 'The output is a concatenation of the prompt (input) string, and the generated completion.', 'example': ''}}}, 'dataset_metadata': {'info': 'This model is fine tuned starting from internally pre-trained 13 billion parameter GPT3 checkpoint with an internally curated instruction tuning dataset. Training this model took around 2 weeks on 2 racks.\\n\\nPreTraining Datasets\\nThe base checkpoint was trained on a mixture of C4 and The PILE datasets and saw a total of (possibly duplicated) 300B tokens during its pretraining run. This roughly represents about 1 epoch on the entire dataset. \\n\\nInstruction Tuning Dataset\\nAfter pretraining, this model was instruction tuned on the FLAN V2 collective and internally curated SambaNova Instruction tuning datasets. The FLAN V2 collective is is a combination of P3, Chain of Thought, Dialogue, and Natural Instructions datasets. These datasets were formatted in two ways: zeroshot/fewshot training and with/without options. The SambaNova Instruction tuning dataset is comprised of instruction tuning datasets that capture key capabilities including but not limited to text improvement, extractive QA, summarization and dialogue. \\n\\nThe flanV2 collective dataset can be found at https://github.com/google-research/FLAN/tree/main/flan/v2 (apache 2.0 license)\\nIt is composed of 5 subsplits\\n- P3 (https://huggingface.co/datasets/bigscience/P3/tree/main, )\\n- Natural Instructions (https://huggingface.co/datasets/Muennighoff/natural-instructions)\\n- FLAN (https://github.com/google-research/FLAN)\\n- Dialogue\\n- CoT \\n\\nData Preparation\\nThe data was formatted as a combination of zeroshot and fewshot settings. In other words, for some of the training data, the prompt had additional demonstration data added into the context for the same completion. This was done for about 50% of the data.\\n\\nFinally, when applicable, the data also had some options added to it. Options are essentially giving the model a set of multiple choice options the model can choose to generate from. The model was required to choose from these options to generate its completion. This was done for about 50% of the data.\\n\\nIn other words,\\n- 25% of the data was fewshot w/ options.\\n- 25% of the data was fewshot w/o options.\\n- 25% of the data was zeroshot w/ options.\\n- 25% of the data was zeroshot w/o options.\\n\\nEvaluation benchmarks annotated with \"pretraining\" use log likelihood for evaluation, this data was collected with the Eluether AI evaluation suite. Meanwhile \"Exact match\" annotations mean that the model was allowed open ended generation and exact match is used to determine the accuracy, these number were collected with the HELM evaluation suite. \\n', 'url': ''}, 'preset_ids': ['40f810fa-a1ce-4f99-b20c-b42510ea697d', '32a49b27-3604-4468-8554-d7e245b3cf95', '5354e0c5-f085-476c-a613-c2fc0f7d1997', '74afaf37-fb28-4e4f-81bf-656bb5bff375', 'fd9ee9b5-d345-4ee2-a529-7b5b310be94a', '93d68cd3-ee0e-4360-98e3-fad3f8b685bc'], 'system_prompt_ids': ['306d6f42-74ad-4d5c-b4eb-ac430aabaeb7'], 'device': 'rdu', 'evaluation': {'eval_1': {'dataset': {'info': 'BoolQ Binary Classification - (pretraining, zero shot)', 'url': ''}, 'metrics': {'Accuracy': 0.726}}, 'eval_10': {'dataset': {'info': 'BoolQ Binary Classification - (Exact Match, 5-shot)', 'url': ''}, 'metrics': {'Accuracy': 0.735}}, 'eval_11': {'dataset': {'info': 'NarrativeQA Extractive QA - (Exact Match, 5-shot)', 'url': ''}, 'metrics': {'Accuracy': 0.69}}, 'eval_12': {'dataset': {'info': 'Natural Questions Closed Book QA - (Exact Match, 5-shot)', 'url': ''}, 'metrics': {'Accuracy': 0.214}}, 'eval_13': {'dataset': {'info': 'Natural Questions Extractive QA - (Exact Match, 5-shot)', 'url': ''}, 'metrics': {'Accuracy': 0.689}}, 'eval_14': {'dataset': {'info': 'TruthfulQA Reasoning - (Exact Match, 5-shot)', 'url': ''}, 'metrics': {'Accuracy': 0.219}}, 'eval_15': {'dataset': {'info': 'CNN-DailyMail Summarization - (Exact Match, 5-shot)', 'url': ''}, 'metrics': {'Accuracy': 0.163}}, 'eval_16': {'dataset': {'info': 'XSUM Summarization - (Exact Match, 5-shot)', 'url': ''}, 'metrics': {'Accuracy': 0.138}}, 'eval_17': {'dataset': {'info': 'IMDB Sentiment Analysis - (Exact Match, 5-shot)', 'url': ''}, 'metrics': {'Accuracy': 96.2}}, 'eval_18': {'dataset': {'info': 'RAFT Text Classification - (Exact Match, 5-shot)', 'url': ''}, 'metrics': {'Accuracy': 0.588}}, 'eval_2': {'dataset': {'info': 'Lambada Sentence Completion - (pretraining, zero shot)', 'url': ''}, 'metrics': {'Accuracy': 0.619}}, 'eval_3': {'dataset': {'info': 'ANLI_R1 Natural Language Inference (pretraining, zero shot)', 'url': ''}, 'metrics': {'Accuracy': 0.348}}, 'eval_4': {'dataset': {'info': 'HELLASWAG - Extractive QA (pretraining, zero shot)', 'url': ''}, 'metrics': {'Normalized Accuracy': 0.685}}, 'eval_5': {'dataset': {'info': 'Winogrande - Coreference Resolution (pretraining, zero shot)', 'url': ''}, 'metrics': {'Accuracy': 0.625}}, 'eval_6': {'dataset': {'info': 'OPENBOOKQA - Extractive QA (pretraining, zero-shot)', 'url': ''}, 'metrics': {'Normalized Accuracy': 0.338}}, 'eval_7': {'dataset': {'info': 'PIQA Commonsense Reasoning- (pretraining, zero shot)', 'url': ''}, 'metrics': {'Normalized Accuracy': 0.768}}, 'eval_8': {'dataset': {'info': 'ARC Challenge Extractive QA - (pretraining, zero shot)', 'url': ''}, 'metrics': {'Normalized Accuracy': 0.378}}, 'eval_9': {'dataset': {'info': 'ARC Easy Extractive QA - (pretraining, zero shot)', 'url': ''}, 'metrics': {'Normalized Accuracy': 0.668}}}, 'time_created': '2024-05-15 18:05:50.486058 +0000 UTC', 'time_updated': '2024-05-15 18:05:50.486058 +0000 UTC', 'app_name': 'Generative Tuning 13B', 'hidden': False, 'jobTypes': ['train', 'batch_predict', 'deploy'], 'shared_with': [], 'type': '', 'dependencies': [], 'replacement_model_id': '', 'replacement_model_name': '', 'deprecation_date': '', 'linked_coe_models': [], 'model_metadata': None, 'model_parallel_rdus': 0, 'metadata': {'labels': {}, 'creationTimestamp': '', 'updateTimestamp': '', 'resourceVersion': '', 'tenant': '', 'owner': '', 'permissions': ['ModelView', 'ModelExport']}}, {'model_id': 'f78ba21b-f828-4356-83e8-bd33c6ebf6bb', 'model_checkpoint_name': 'GPT_13B_GT_Base_Model_300k_MaxVocabSize', 'app_id': '57f6a3c8-1f04-488a-bb39-3cfc5b4a5d7a', 'user_id': '', 'tenant_id': '', 'checkpoint_type': 'base', 'model_size_gb': 49, 'dataset': '', 'dataset_info': '', 'dataset_url': '', 'path': '', 'sambanova_provided': True, 'version': 1, 'job_config': '', 'description': 'This is a randomly initialized model with a maximum vocab size of 307200, meant to be used to kick off a pre-training job. \\n\\nGenerally speaking, the process of pre-training is expensive both in terms of compute and data. For most use cases, it will be better to fine tune one of the provided checkpoints, rather than starting from scratch.\\n      \\n', 'status': 'AvailableToDownload', 'params': {'invalidates_checkpoint': {'max_seq_length': 2048, 'vocab_size': 307200}, 'modifiable': None}, 'metrics': '', 'steps': 0, 'application_field': 'language', 'hyperparams': {'train': {}, 'batch_predict': {}, 'deploy': {}}, 'architecture': 'GPT 13B', 'pipeline': None, 'model_io': {'train': {'input': {'description': 'The training data can be prepared following the generative data preparation guide in the documentation.', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'infer': {'input': {'description': '', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'serve': {'input': {'description': '', 'example': ''}, 'output': {'description': '', 'example': ''}}}, 'dataset_metadata': {'info': 'N/A\\n', 'url': ''}, 'preset_ids': [], 'system_prompt_ids': [], 'device': 'rdu', 'evaluation': {}, 'time_created': '2024-05-15 18:05:50.731398 +0000 UTC', 'time_updated': '2024-05-15 18:05:50.731398 +0000 UTC', 'app_name': 'Generative Tuning 13B', 'hidden': False, 'jobTypes': ['train'], 'shared_with': [], 'type': '', 'dependencies': [], 'replacement_model_id': '', 'replacement_model_name': '', 'deprecation_date': '', 'linked_coe_models': [], 'model_metadata': None, 'model_parallel_rdus': 0, 'metadata': {'labels': {}, 'creationTimestamp': '', 'updateTimestamp': '', 'resourceVersion': '', 'tenant': '', 'owner': '', 'permissions': ['ModelView', 'ModelExport']}}, {'model_id': '61b4ff7d-fbaf-444d-9cba-7ac89187e375', 'model_checkpoint_name': 'GPT_13B_Base_Model', 'app_id': '57f6a3c8-1f04-488a-bb39-3cfc5b4a5d7a', 'user_id': '', 'tenant_id': '', 'checkpoint_type': 'base', 'model_size_gb': 49, 'dataset': '', 'dataset_info': '', 'dataset_url': '', 'path': '', 'sambanova_provided': True, 'version': 1, 'job_config': '', 'description': 'This is a randomly initialized model, meant to be used to kick off a pre-training job. \\n\\nGenerally speaking, the process of pre-training is expensive both in terms of compute and data. For most use cases, it will be better to fine tune one of the provided checkpoints, rather than starting from scratch.\\n      \\n', 'status': 'AvailableToDownload', 'params': {'invalidates_checkpoint': {'vocab_size': 50260}, 'modifiable': None}, 'metrics': '', 'steps': 0, 'application_field': 'language', 'hyperparams': {'train': {}, 'batch_predict': {}, 'deploy': {}}, 'architecture': 'GPT 13B', 'pipeline': None, 'model_io': {'train': {'input': {'description': 'The training data can be prepared following the generative data preparation guide in the documentation.', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'infer': {'input': {'description': '', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'serve': {'input': {'description': '', 'example': ''}, 'output': {'description': '', 'example': ''}}}, 'dataset_metadata': {'info': 'N/A\\n', 'url': ''}, 'preset_ids': [], 'system_prompt_ids': [], 'device': 'rdu', 'evaluation': {}, 'time_created': '2024-05-15 18:05:51.262569 +0000 UTC', 'time_updated': '2024-05-15 18:05:51.262569 +0000 UTC', 'app_name': 'Generative Tuning 13B', 'hidden': False, 'jobTypes': ['train'], 'shared_with': [], 'type': '', 'dependencies': [], 'replacement_model_id': '', 'replacement_model_name': '', 'deprecation_date': '', 'linked_coe_models': [], 'model_metadata': None, 'model_parallel_rdus': 0, 'metadata': {'labels': {}, 'creationTimestamp': '', 'updateTimestamp': '', 'resourceVersion': '', 'tenant': '', 'owner': '', 'permissions': ['ModelView', 'ModelExport']}}, {'model_id': '157ce015-19cc-4538-addf-10cf620a33f6', 'model_checkpoint_name': 'GPT_13B_Dialog_Summarization_Finetuned', 'app_id': '57f6a3c8-1f04-488a-bb39-3cfc5b4a5d7a', 'user_id': '', 'tenant_id': '', 'checkpoint_type': 'finetuned', 'model_size_gb': 49, 'dataset': '', 'dataset_info': '', 'dataset_url': '', 'path': 'common/checkpoints/157ce015-19cc-4538-addf-10cf620a33f6/checkpoint', 'sambanova_provided': True, 'version': 1, 'job_config': '', 'description': 'Generates a concise brief of a conversation or dialogue in the third person. Ready to deploy for inference, or further fine tuning to improve summarization performance\\n', 'status': 'AvailableToDownload', 'params': {'invalidates_checkpoint': {'max_seq_length': 2048, 'vocab_size': 50260}, 'modifiable': None}, 'metrics': '', 'steps': 0, 'application_field': 'language', 'hyperparams': {'train': {}, 'batch_predict': {}, 'deploy': {}}, 'architecture': 'GPT 13B', 'pipeline': None, 'model_io': {'train': {'input': {'description': '', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'infer': {'input': {'description': '', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'serve': {'input': {'description': '', 'example': ''}, 'output': {'description': 'The output is a concatenation of the prompt (input) string, and the generated completion.', 'example': ''}}}, 'dataset_metadata': {'info': '', 'url': ''}, 'preset_ids': ['777326d4-95e1-4130-9566-faf715382d19', '66203dab-6788-45a5-9154-72acbd760e93', '244265ea-7513-494b-991f-152018aefbdb', 'd2524d91-2ee6-4aeb-b263-7ac08e41054c', '6e464df2-c694-4a0b-bf3c-2a9e1d08024e', 'fe93cf15-d527-4b39-a026-e5348975937b', '76bca2a3-837f-4383-af41-69a707afb86c', '12b7b141-d0ed-479a-8f10-aa38dd28f477', '461a3992-0f1d-447c-829d-414ba456aaed', '850d310f-1e9f-4a1c-b128-94b97147701a', 'c5d36145-0730-4119-bcea-ad4bb973873c', '33c08191-c2e8-462d-ba8e-7cd82d10dddf', '1a0c5df7-3a13-4d61-96f1-29396e0d40cc', 'e53c63a4-63fb-4de9-bedf-963ee170776a', '2a89dfdd-1eab-46b0-a31a-18f9c6dc7f2c', 'cb1c9a8e-9a94-48d9-b3b6-aa9e5494cc05', 'ad6affed-d3f5-429b-8759-af5badbdec2c', '3d730257-3d5d-46d1-ab17-6ffb13770b70', '2e6b8c10-1f48-459c-b45c-d94408ed667c'], 'system_prompt_ids': [], 'device': 'rdu', 'evaluation': {}, 'time_created': '2024-05-15 18:05:51.918512 +0000 UTC', 'time_updated': '2024-05-15 18:05:51.918512 +0000 UTC', 'app_name': 'Generative Tuning 13B', 'hidden': False, 'jobTypes': ['train', 'batch_predict', 'deploy'], 'shared_with': [], 'type': '', 'dependencies': [], 'replacement_model_id': '', 'replacement_model_name': '', 'deprecation_date': '', 'linked_coe_models': [], 'model_metadata': None, 'model_parallel_rdus': 0, 'metadata': {'labels': {}, 'creationTimestamp': '', 'updateTimestamp': '', 'resourceVersion': '', 'tenant': '', 'owner': '', 'permissions': ['ModelView', 'ModelExport']}}, {'model_id': '08902fc2-b11d-42f9-9d39-2d5832c17e09', 'model_checkpoint_name': 'GPT_1.5B_GT_Pretrained', 'app_id': 'e681c226-86be-40b2-9380-d2de11b19842', 'user_id': '', 'tenant_id': '', 'checkpoint_type': 'pretrained', 'model_size_gb': 5.9, 'dataset': '', 'dataset_info': '', 'dataset_url': '', 'path': 'common/checkpoints/08902fc2-b11d-42f9-9d39-2d5832c17e09/checkpoint', 'sambanova_provided': True, 'version': 1, 'job_config': '', 'description': 'Pretrained model, ready to be fine tuned on your custom dataset for custom generative task. Not suitable for deployment to an endpoint for inference.', 'status': 'AvailableToDownload', 'params': {'invalidates_checkpoint': {'max_seq_length': 1024}, 'modifiable': {'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'batch_size': 16, 'doc_stride': 128, 'learning_rate': 1.5e-05, 'max_answer_length': 30, 'max_query_length': 64, 'max_source_length': 512, 'max_target_length': 114, 'n_best_size': 20, 'num_iterations': 7500, 'warmup_steps': 0, 'weight_decay': 0.1}}, 'metrics': '', 'steps': 0, 'application_field': 'language', 'hyperparams': {'train': {}, 'batch_predict': {}, 'deploy': {}}, 'architecture': 'GPT 1.5B', 'pipeline': None, 'model_io': {'train': {'input': {'description': '', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'infer': {'input': {'description': '', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'serve': {'input': {'description': '', 'example': ''}, 'output': {'description': '', 'example': ''}}}, 'dataset_metadata': {'info': 'A SambaNova curated collection of  datasets that cover Q&A and structured data.', 'url': ''}, 'preset_ids': ['777326d4-95e1-4130-9566-faf715382d19', '66203dab-6788-45a5-9154-72acbd760e93', '244265ea-7513-494b-991f-152018aefbdb', 'd2524d91-2ee6-4aeb-b263-7ac08e41054c', '6e464df2-c694-4a0b-bf3c-2a9e1d08024e', 'fe93cf15-d527-4b39-a026-e5348975937b', '76bca2a3-837f-4383-af41-69a707afb86c', '12b7b141-d0ed-479a-8f10-aa38dd28f477', '461a3992-0f1d-447c-829d-414ba456aaed', '850d310f-1e9f-4a1c-b128-94b97147701a', 'c5d36145-0730-4119-bcea-ad4bb973873c', '33c08191-c2e8-462d-ba8e-7cd82d10dddf', '1a0c5df7-3a13-4d61-96f1-29396e0d40cc', 'e53c63a4-63fb-4de9-bedf-963ee170776a', '2a89dfdd-1eab-46b0-a31a-18f9c6dc7f2c', 'cb1c9a8e-9a94-48d9-b3b6-aa9e5494cc05', 'ad6affed-d3f5-429b-8759-af5badbdec2c', '3d730257-3d5d-46d1-ab17-6ffb13770b70', '2e6b8c10-1f48-459c-b45c-d94408ed667c'], 'system_prompt_ids': [], 'device': 'rdu', 'evaluation': {'eval_1': {'dataset': {'info': 'Generative Tuning Dataset Training (7500 step)', 'url': ''}, 'metrics': {'loss': 2.35362}}}, 'time_created': '2024-05-15 18:06:03.518318 +0000 UTC', 'time_updated': '2024-05-15 18:06:03.518318 +0000 UTC', 'app_name': 'Generative Tuning 1.5B', 'hidden': False, 'jobTypes': ['train', 'batch_predict', 'deploy'], 'shared_with': [], 'type': '', 'dependencies': [], 'replacement_model_id': '', 'replacement_model_name': '', 'deprecation_date': '', 'linked_coe_models': [], 'model_metadata': None, 'model_parallel_rdus': 0, 'metadata': {'labels': {}, 'creationTimestamp': '', 'updateTimestamp': '', 'resourceVersion': '', 'tenant': '', 'owner': '', 'permissions': ['ModelView', 'ModelExport']}}, {'model_id': 'ea03da97-7162-47c3-942b-0b13687e16b5', 'model_checkpoint_name': 'Llama-2-13b-chat-hf', 'app_id': '1bf617cb-8afb-4bbd-b92f-c15ebfdca10b', 'user_id': '', 'tenant_id': '', 'checkpoint_type': 'pretrained', 'model_size_gb': 25, 'dataset': '', 'dataset_info': '', 'dataset_url': '', 'path': 'common/checkpoints/ea03da97-7162-47c3-942b-0b13687e16b5/checkpoint', 'sambanova_provided': True, 'version': 1, 'job_config': '', 'description': 'Llama 2 is the latest iteration of the large language model (LLM) developed and released by Meta. It is pretrained on 2 trillion tokens of publicly available data and is designed to enable developers to build generative AI applications. Please refer to the provided link for detailed information on licensing agreements and terms of use.\\n\\nLlama 2 Community License Agreement: https://ai.meta.com/llama/license/\\nView on HuggingFace: https://huggingface.co/meta-llama/Llama-2-13b-chat-hf\\n', 'status': 'AvailableToDownload', 'params': {'invalidates_checkpoint': {'max_seq_length': 4096, 'vocab_size': 32000}, 'modifiable': None}, 'metrics': '', 'steps': 0, 'application_field': 'language', 'hyperparams': {'train': {}, 'batch_predict': {}, 'deploy': {}}, 'architecture': 'LLaMA v2 13B', 'pipeline': None, 'model_io': {'train': {'input': {'description': 'A folder containing .hdf5 files produced by the SambaNova generative_data_prep package (please see documentation for usage instructions).', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'infer': {'input': {'description': 'A folder containing .jsonl files where each line is a json object containing a \"prompt\" field and a \"completion\" field.', 'example': ''}, 'output': {'description': 'A file called predictions.json which contains the list of generated sequences per input prompt', 'example': ''}}, 'serve': {'input': {'description': 'The input prompt to be completed by the generative inference model.', 'example': ''}, 'output': {'description': 'The output is a concatenation of the prompt (input) string, and the generated completion.', 'example': ''}}}, 'dataset_metadata': {'info': 'View on HuggingFace: https://huggingface.co/meta-llama/Llama-2-13b-chat-hf\\n', 'url': ''}, 'preset_ids': [], 'system_prompt_ids': [], 'device': 'rdu', 'evaluation': {}, 'time_created': '2024-05-15 18:07:01.102523 +0000 UTC', 'time_updated': '2024-05-15 18:07:01.102523 +0000 UTC', 'app_name': 'Llama 2 13B', 'hidden': False, 'jobTypes': ['train', 'batch_predict', 'deploy'], 'shared_with': [], 'type': '', 'dependencies': [], 'replacement_model_id': '', 'replacement_model_name': '', 'deprecation_date': '', 'linked_coe_models': [], 'model_metadata': None, 'model_parallel_rdus': 0, 'metadata': {'labels': {}, 'creationTimestamp': '', 'updateTimestamp': '', 'resourceVersion': '', 'tenant': '', 'owner': '', 'permissions': ['ModelView', 'ModelExport']}}, {'model_id': '4bd7565f-53c3-4b5b-bf78-a109bd1fdfbc', 'model_checkpoint_name': 'Llama-2-70b-hf', 'app_id': '82254d3b-7239-458b-9da8-da1aca9b7fba', 'user_id': '', 'tenant_id': '', 'checkpoint_type': 'pretrained', 'model_size_gb': 129, 'dataset': '', 'dataset_info': '', 'dataset_url': '', 'path': 'common/checkpoints/4bd7565f-53c3-4b5b-bf78-a109bd1fdfbc/checkpoint', 'sambanova_provided': True, 'version': 1, 'job_config': '', 'description': 'Llama 2 is the latest iteration of the large language model (LLM) developed and released by Meta. It is pretrained on 2 trillion tokens of publicly available data and is designed to enable developers to build generative AI applications. Please refer to the provided link for detailed information on licensing agreements and terms of use.\\n\\nLlama 2 Community License Agreement: https://ai.meta.com/llama/license/\\nView on HuggingFace: https://huggingface.co/meta-llama/Llama-2-70b-hf\\n', 'status': 'AvailableToDownload', 'params': {'invalidates_checkpoint': {'max_seq_length': 4096, 'vocab_size': 32000}, 'modifiable': None}, 'metrics': '', 'steps': 0, 'application_field': 'language', 'hyperparams': {'train': {}, 'batch_predict': {}, 'deploy': {}}, 'architecture': 'LLaMA v2 70B', 'pipeline': None, 'model_io': {'train': {'input': {'description': 'A folder containing .hdf5 files produced by the SambaNova generative_data_prep package (please see documentation for usage instructions).', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'infer': {'input': {'description': 'A folder containing .jsonl files where each line is a json object containing a \"prompt\" field and a \"completion\" field.', 'example': ''}, 'output': {'description': 'A file called predictions.json which contains the list of generated sequences per input prompt', 'example': ''}}, 'serve': {'input': {'description': 'The input prompt to be completed by the generative inference model.', 'example': ''}, 'output': {'description': 'The output is a concatenation of the prompt (input) string, and the generated completion.', 'example': ''}}}, 'dataset_metadata': {'info': 'View on HuggingFace: https://huggingface.co/meta-llama/Llama-2-70b-hf\\n', 'url': ''}, 'preset_ids': [], 'system_prompt_ids': [], 'device': 'rdu', 'evaluation': {}, 'time_created': '2024-05-15 18:08:02.37471 +0000 UTC', 'time_updated': '2024-05-15 18:08:02.37471 +0000 UTC', 'app_name': 'Llama 2 70B', 'hidden': False, 'jobTypes': ['train'], 'shared_with': [], 'type': '', 'dependencies': [], 'replacement_model_id': '', 'replacement_model_name': '', 'deprecation_date': '', 'linked_coe_models': [], 'model_metadata': None, 'model_parallel_rdus': 0, 'metadata': {'labels': {}, 'creationTimestamp': '', 'updateTimestamp': '', 'resourceVersion': '', 'tenant': '', 'owner': '', 'permissions': ['ModelView', 'ModelExport']}}, {'model_id': 'e35bd321-209a-4112-bd4f-8415c20de6de', 'model_checkpoint_name': 'Llama-2-13b-hf', 'app_id': '1bf617cb-8afb-4bbd-b92f-c15ebfdca10b', 'user_id': '', 'tenant_id': '', 'checkpoint_type': 'pretrained', 'model_size_gb': 25, 'dataset': '', 'dataset_info': '', 'dataset_url': '', 'path': 'common/checkpoints/e35bd321-209a-4112-bd4f-8415c20de6de/checkpoint', 'sambanova_provided': True, 'version': 1, 'job_config': '', 'description': 'Llama 2 is the latest iteration of the large language model (LLM) developed and released by Meta. It is pretrained on 2 trillion tokens of publicly available data and is designed to enable developers to build generative AI applications. Please refer to the provided link for detailed information on licensing agreements and terms of use.\\n\\nLlama 2 Community License Agreement: https://ai.meta.com/llama/license/\\nView on HuggingFace: https://huggingface.co/meta-llama/Llama-2-13b-hf\\n', 'status': 'AvailableToDownload', 'params': {'invalidates_checkpoint': {'max_seq_length': 4096, 'vocab_size': 32000}, 'modifiable': None}, 'metrics': '', 'steps': 0, 'application_field': 'language', 'hyperparams': {'train': {}, 'batch_predict': {}, 'deploy': {}}, 'architecture': 'LLaMA v2 13B', 'pipeline': None, 'model_io': {'train': {'input': {'description': 'A folder containing .hdf5 files produced by the SambaNova generative_data_prep package (please see documentation for usage instructions).', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'infer': {'input': {'description': 'A folder containing .jsonl files where each line is a json object containing a \"prompt\" field and a \"completion\" field.', 'example': ''}, 'output': {'description': 'A file called predictions.json which contains the list of generated sequences per input prompt', 'example': ''}}, 'serve': {'input': {'description': 'The input prompt to be completed by the generative inference model.', 'example': ''}, 'output': {'description': 'The output is a concatenation of the prompt (input) string, and the generated completion.', 'example': ''}}}, 'dataset_metadata': {'info': 'View on HuggingFace: https://huggingface.co/meta-llama/Llama-2-13b-hf\\n', 'url': ''}, 'preset_ids': [], 'system_prompt_ids': [], 'device': 'rdu', 'evaluation': {}, 'time_created': '2024-05-15 18:06:50.612744 +0000 UTC', 'time_updated': '2024-05-15 18:06:50.612744 +0000 UTC', 'app_name': 'Llama 2 13B', 'hidden': False, 'jobTypes': ['train', 'batch_predict', 'deploy'], 'shared_with': [], 'type': '', 'dependencies': [], 'replacement_model_id': '', 'replacement_model_name': '', 'deprecation_date': '', 'linked_coe_models': [], 'model_metadata': None, 'model_parallel_rdus': 0, 'metadata': {'labels': {}, 'creationTimestamp': '', 'updateTimestamp': '', 'resourceVersion': '', 'tenant': '', 'owner': '', 'permissions': ['ModelView', 'ModelExport']}}, {'model_id': 'fdd76236-4df2-4d39-a53c-e567846c61da', 'model_checkpoint_name': 'Llama-2-70-16k-hf', 'app_id': '82254d3b-7239-458b-9da8-da1aca9b7fba', 'user_id': '', 'tenant_id': '', 'checkpoint_type': 'pretrained', 'model_size_gb': 129, 'dataset': '', 'dataset_info': '', 'dataset_url': '', 'path': 'common/checkpoints/fdd76236-4df2-4d39-a53c-e567846c61da/checkpoint', 'sambanova_provided': True, 'version': 1, 'job_config': '', 'description': 'Llama 2 is the latest iteration of the large language model (LLM) developed and released by Meta. It is pretrained on 2 trillion tokens of publicly available data and is designed to enable developers to build generative AI applications. Please refer to the provided link for detailed information on licensing agreements and terms of use.\\n\\nLlama 2 Community License Agreement: https://ai.meta.com/llama/license/\\nView on HuggingFace: https://huggingface.co/meta-llama/Llama-2-70b-hf\\n\\nAvailability on RDU architectures: This model is currently only supported for training and inference (both batch inference and endpoints) on SN40.\\n', 'status': 'AvailableToDownload', 'params': {'invalidates_checkpoint': {'max_seq_length': 16384, 'vocab_size': 32000}, 'modifiable': None}, 'metrics': '', 'steps': 0, 'application_field': 'language', 'hyperparams': {'train': {}, 'batch_predict': {}, 'deploy': {}}, 'architecture': 'LLaMA v2 70B', 'pipeline': None, 'model_io': {'train': {'input': {'description': 'A folder containing .hdf5 files produced by the SambaNova generative_data_prep package (please see documentation for usage instructions).', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'infer': {'input': {'description': 'A folder containing .jsonl files where each line is a json object containing a \"prompt\" field and a \"completion\" field.', 'example': ''}, 'output': {'description': 'A file called predictions.json which contains the list of generated sequences per input prompt', 'example': ''}}, 'serve': {'input': {'description': 'The input prompt to be completed by the generative inference model.', 'example': ''}, 'output': {'description': 'The output is a concatenation of the prompt (input) string, and the generated completion.', 'example': ''}}}, 'dataset_metadata': {'info': 'View on HuggingFace: https://huggingface.co/meta-llama/Llama-2-70b-hf\\n', 'url': ''}, 'preset_ids': [], 'system_prompt_ids': [], 'device': 'rdu', 'evaluation': {}, 'time_created': '2024-05-15 18:07:36.643561 +0000 UTC', 'time_updated': '2024-05-15 18:07:36.643561 +0000 UTC', 'app_name': 'Llama 2 70B', 'hidden': False, 'jobTypes': ['train'], 'shared_with': [], 'type': '', 'dependencies': [], 'replacement_model_id': '', 'replacement_model_name': '', 'deprecation_date': '', 'linked_coe_models': [], 'model_metadata': None, 'model_parallel_rdus': 0, 'metadata': {'labels': {}, 'creationTimestamp': '', 'updateTimestamp': '', 'resourceVersion': '', 'tenant': '', 'owner': '', 'permissions': ['ModelView', 'ModelExport']}}, {'model_id': '8dd301fa-b79b-40a7-b795-1f356be17fe3', 'model_checkpoint_name': 'Llama-2-7b-hf', 'app_id': 'ec012370-6ffa-4a3a-b230-2c62613f1d89', 'user_id': '', 'tenant_id': '', 'checkpoint_type': 'pretrained', 'model_size_gb': 13, 'dataset': '', 'dataset_info': '', 'dataset_url': '', 'path': 'common/checkpoints/8dd301fa-b79b-40a7-b795-1f356be17fe3/checkpoint', 'sambanova_provided': True, 'version': 1, 'job_config': '', 'description': 'Llama 2 is the latest iteration of the large language model (LLM) developed and released by Meta. It is pretrained on 2 trillion tokens of publicly available data and is designed to enable developers to build generative AI applications. Please refer to the provided link for detailed information on licensing agreements and terms of use.\\n\\nLlama 2 Community License Agreement: https://ai.meta.com/llama/license/\\nView on HuggingFace: https://huggingface.co/meta-llama/Llama-2-7b-hf\\n', 'status': 'AvailableToDownload', 'params': {'invalidates_checkpoint': {'max_seq_length': 4096, 'vocab_size': 32000}, 'modifiable': None}, 'metrics': '', 'steps': 0, 'application_field': 'language', 'hyperparams': {'train': {}, 'batch_predict': {}, 'deploy': {}}, 'architecture': 'LLaMA v2 7B', 'pipeline': None, 'model_io': {'train': {'input': {'description': 'A folder containing .hdf5 files produced by the SambaNova generative_data_prep package (please see documentation for usage instructions).', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'infer': {'input': {'description': 'A folder containing .jsonl files where each line is a json object containing a \"prompt\" field and a \"completion\" field.', 'example': ''}, 'output': {'description': 'A file called predictions.json which contains the list of generated sequences per input prompt', 'example': ''}}, 'serve': {'input': {'description': 'The input prompt to be completed by the generative inference model.', 'example': ''}, 'output': {'description': 'The output is a concatenation of the prompt (input) string, and the generated completion.', 'example': ''}}}, 'dataset_metadata': {'info': 'View on HuggingFace: https://huggingface.co/meta-llama/Llama-2-7b-hf\\n', 'url': ''}, 'preset_ids': [], 'system_prompt_ids': [], 'device': 'rdu', 'evaluation': {}, 'time_created': '2024-05-15 18:09:18.606386 +0000 UTC', 'time_updated': '2024-05-15 18:09:18.606386 +0000 UTC', 'app_name': 'Llama 2 7B', 'hidden': False, 'jobTypes': ['train', 'batch_predict', 'deploy'], 'shared_with': [], 'type': '', 'dependencies': [], 'replacement_model_id': '', 'replacement_model_name': '', 'deprecation_date': '', 'linked_coe_models': [], 'model_metadata': None, 'model_parallel_rdus': 0, 'metadata': {'labels': {}, 'creationTimestamp': '', 'updateTimestamp': '', 'resourceVersion': '', 'tenant': '', 'owner': '', 'permissions': ['ModelView', 'ModelExport']}}, {'model_id': 'efbdff4b-0db7-46de-a5d2-2d21cf7ef833', 'model_checkpoint_name': 'Llama-2-7b-16k-hf', 'app_id': 'ec012370-6ffa-4a3a-b230-2c62613f1d89', 'user_id': '', 'tenant_id': '', 'checkpoint_type': 'pretrained', 'model_size_gb': 13, 'dataset': '', 'dataset_info': '', 'dataset_url': '', 'path': 'common/checkpoints/efbdff4b-0db7-46de-a5d2-2d21cf7ef833/checkpoint', 'sambanova_provided': True, 'version': 1, 'job_config': '', 'description': 'Llama v2 7B checkpoint for 16K sequence length size.\\nThis model currently only supports 1 RDU inference\\n\\nLlama 2 Community License Agreement: https://ai.meta.com/llama/license/\\nView on HuggingFace: https://huggingface.co/meta-llama/Llama-2-7b-hf\\n\\nAvailability on RDU architectures: This model is currently only supported for training and inference (both batch inference and endpoints) on SN40.\\n', 'status': 'AvailableToDownload', 'params': {'invalidates_checkpoint': {'max_seq_length': 16384, 'vocab_size': 32000}, 'modifiable': None}, 'metrics': '', 'steps': 0, 'application_field': 'language', 'hyperparams': {'train': {}, 'batch_predict': {}, 'deploy': {}}, 'architecture': 'LLaMA v2 7B', 'pipeline': None, 'model_io': {'train': {'input': {'description': '', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'infer': {'input': {'description': '', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'serve': {'input': {'description': '', 'example': ''}, 'output': {'description': 'The output is a concatenation of the prompt (input) string, and the generated completion.', 'example': ''}}}, 'dataset_metadata': {'info': 'View on HuggingFace: https://huggingface.co/meta-llama/Llama-2-7b-hf\\n', 'url': ''}, 'preset_ids': [], 'system_prompt_ids': [], 'device': 'rdu', 'evaluation': {}, 'time_created': '2024-05-15 18:08:43.259594 +0000 UTC', 'time_updated': '2024-05-15 18:08:43.259594 +0000 UTC', 'app_name': 'Llama 2 7B', 'hidden': False, 'jobTypes': ['train', 'batch_predict', 'deploy'], 'shared_with': [], 'type': '', 'dependencies': [], 'replacement_model_id': '', 'replacement_model_name': '', 'deprecation_date': '', 'linked_coe_models': [], 'model_metadata': None, 'model_parallel_rdus': 0, 'metadata': {'labels': {}, 'creationTimestamp': '', 'updateTimestamp': '', 'resourceVersion': '', 'tenant': '', 'owner': '', 'permissions': ['ModelView', 'ModelExport']}}, {'model_id': 'be1ff942-1771-4c3f-b282-ec934e6d6345', 'model_checkpoint_name': 'Llama-2-7b-chat-16k-hf', 'app_id': 'ec012370-6ffa-4a3a-b230-2c62613f1d89', 'user_id': '', 'tenant_id': '', 'checkpoint_type': 'pretrained', 'model_size_gb': 13, 'dataset': '', 'dataset_info': '', 'dataset_url': '', 'path': 'common/checkpoints/be1ff942-1771-4c3f-b282-ec934e6d6345/checkpoint', 'sambanova_provided': True, 'version': 1, 'job_config': '', 'description': 'Llama 2 is the latest iteration of the large language model (LLM) developed and released by Meta. It is pretrained on 2 trillion tokens of publicly available data and is designed to enable developers to build generative AI applications. Please refer to the provided link for detailed information on licensing agreements and terms of use.\\n\\nLlama 2 Community License Agreement: https://ai.meta.com/llama/license/\\nView on HuggingFace: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\\n\\nAvailability on RDU architectures: This model is currently only supported for training and inference (both batch inference and endpoints) on SN40.\\n', 'status': 'AvailableToDownload', 'params': {'invalidates_checkpoint': {'max_seq_length': 16384, 'vocab_size': 32000}, 'modifiable': None}, 'metrics': '', 'steps': 0, 'application_field': 'language', 'hyperparams': {'train': {}, 'batch_predict': {}, 'deploy': {}}, 'architecture': 'LLaMA v2 7B', 'pipeline': None, 'model_io': {'train': {'input': {'description': 'A folder containing .hdf5 files produced by the SambaNova generative_data_prep package (please see documentation for usage instructions).', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'infer': {'input': {'description': 'A folder containing .jsonl files where each line is a json object containing a \"prompt\" field and a \"completion\" field.', 'example': ''}, 'output': {'description': 'A file called predictions.json which contains the list of generated sequences per input prompt', 'example': ''}}, 'serve': {'input': {'description': 'The input prompt to be completed by the generative inference model.', 'example': ''}, 'output': {'description': 'The output is a concatenation of the prompt (input) string, and the generated completion.', 'example': ''}}}, 'dataset_metadata': {'info': 'View on Huggingface: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\\n', 'url': ''}, 'preset_ids': [], 'system_prompt_ids': [], 'device': 'rdu', 'evaluation': {}, 'time_created': '2024-05-15 18:09:34.03372 +0000 UTC', 'time_updated': '2024-05-15 18:09:34.03372 +0000 UTC', 'app_name': 'Llama 2 7B', 'hidden': False, 'jobTypes': ['train', 'batch_predict', 'deploy'], 'shared_with': [], 'type': '', 'dependencies': [], 'replacement_model_id': '', 'replacement_model_name': '', 'deprecation_date': '', 'linked_coe_models': [], 'model_metadata': None, 'model_parallel_rdus': 0, 'metadata': {'labels': {}, 'creationTimestamp': '', 'updateTimestamp': '', 'resourceVersion': '', 'tenant': '', 'owner': '', 'permissions': ['ModelView', 'ModelExport']}}, {'model_id': '3f2da53d-703c-462e-b232-770b2dda466e', 'model_checkpoint_name': 'LlamaGuard_7b', 'app_id': 'ec012370-6ffa-4a3a-b230-2c62613f1d89', 'user_id': '', 'tenant_id': '', 'checkpoint_type': 'pretrained', 'model_size_gb': 13, 'dataset': '', 'dataset_info': '', 'dataset_url': '', 'path': 'common/checkpoints/3f2da53d-703c-462e-b232-770b2dda466e/checkpoint', 'sambanova_provided': True, 'version': 1, 'job_config': '', 'description': 'Llama-Guard is a 7B parameter Llama 2-based input-output safeguard model. It can be used for classifying content in both LLM inputs (prompt classification) and in LLM responses (response classification). Please refer to the provided link for detailed information on licensing agreements and terms of use.\\n\\nLlama 2 Community License Agreement: https://ai.meta.com/llama/license/\\nView on HuggingFace: https://huggingface.co/meta-llama/LlamaGuard-7b\\n', 'status': 'AvailableToDownload', 'params': {'invalidates_checkpoint': {'max_seq_length': 4096, 'vocab_size': 32000}, 'modifiable': None}, 'metrics': '', 'steps': 0, 'application_field': 'language', 'hyperparams': {'train': {}, 'batch_predict': {}, 'deploy': {}}, 'architecture': 'LLaMA v2 7B', 'pipeline': None, 'model_io': {'train': {'input': {'description': 'A folder containing .hdf5 files produced by the SambaNova generative_data_prep package (please see documentation for usage instructions).', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'infer': {'input': {'description': 'A folder containing .jsonl files where each line is a json object containing a \"prompt\" field and a \"completion\" field.', 'example': ''}, 'output': {'description': 'A file called predictions.json which contains the list of generated sequences per input prompt', 'example': ''}}, 'serve': {'input': {'description': 'The input prompt to be completed by the generative inference model.', 'example': ''}, 'output': {'description': 'The output is a concatenation of the prompt (input) string, and the generated completion.', 'example': ''}}}, 'dataset_metadata': {'info': 'View on Huggingface: https://huggingface.co/meta-llama/LlamaGuard-7b\\n', 'url': ''}, 'preset_ids': [], 'system_prompt_ids': [], 'device': 'rdu', 'evaluation': {}, 'time_created': '2024-05-15 18:09:49.790716 +0000 UTC', 'time_updated': '2024-05-15 18:09:49.790716 +0000 UTC', 'app_name': 'Llama 2 7B', 'hidden': False, 'jobTypes': ['train', 'batch_predict', 'deploy'], 'shared_with': [], 'type': '', 'dependencies': [], 'replacement_model_id': '', 'replacement_model_name': '', 'deprecation_date': '', 'linked_coe_models': [], 'model_metadata': None, 'model_parallel_rdus': 0, 'metadata': {'labels': {}, 'creationTimestamp': '', 'updateTimestamp': '', 'resourceVersion': '', 'tenant': '', 'owner': '', 'permissions': ['ModelView', 'ModelExport']}}, {'model_id': '135ea78e-8c21-4693-9f89-f04cab6544c9', 'model_checkpoint_name': 'NSQL-Llama-2-7B', 'app_id': 'ec012370-6ffa-4a3a-b230-2c62613f1d89', 'user_id': '', 'tenant_id': '', 'checkpoint_type': 'pretrained', 'model_size_gb': 13, 'dataset': '', 'dataset_info': '', 'dataset_url': '', 'path': 'common/checkpoints/135ea78e-8c21-4693-9f89-f04cab6544c9/checkpoint', 'sambanova_provided': True, 'version': 1, 'job_config': '', 'description': 'NSQL-Llama-2-7B is an open-source large language model designed specifically for SQL generation tasks by Numbers Station. It is based on Meta’s Llama-2-7B model and further pretrained on general SQL queries dataset. It is then fine-tuned on a dataset composed of text-to-SQL pairs. Please refer to the provided link for detailed information on licensing agreements and terms of use.\\n\\nLlama 2 Community License Agreement: https://ai.meta.com/llama/license/\\nView on Huggingface: https://huggingface.co/NumbersStation/nsql-llama-2-7B\\n', 'status': 'AvailableToDownload', 'params': {'invalidates_checkpoint': {'max_seq_length': 4096, 'vocab_size': 32000}, 'modifiable': None}, 'metrics': '', 'steps': 0, 'application_field': 'language', 'hyperparams': {'train': {}, 'batch_predict': {}, 'deploy': {}}, 'architecture': 'LLaMA v2 7B', 'pipeline': None, 'model_io': {'train': {'input': {'description': 'A folder containing .hdf5 files produced by the SambaNova generative_data_prep package (please see documentation for usage instructions).', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'infer': {'input': {'description': 'A folder containing .jsonl files where each line is a json object containing a \"prompt\" field and a \"completion\" field.', 'example': ''}, 'output': {'description': 'A file called predictions.json which contains the list of generated sequences per input prompt', 'example': ''}}, 'serve': {'input': {'description': 'The input prompt to be completed by the generative inference model.', 'example': ''}, 'output': {'description': 'The output is a concatenation of the prompt (input) string, and the generated completion.', 'example': ''}}}, 'dataset_metadata': {'info': 'View on Huggingface: https://huggingface.co/NumbersStation/nsql-llama-2-7B\\n', 'url': ''}, 'preset_ids': [], 'system_prompt_ids': [], 'device': 'rdu', 'evaluation': {}, 'time_created': '2024-05-15 18:10:30.088328 +0000 UTC', 'time_updated': '2024-05-15 18:10:30.088328 +0000 UTC', 'app_name': 'Llama 2 7B', 'hidden': False, 'jobTypes': ['train', 'batch_predict', 'deploy'], 'shared_with': [], 'type': '', 'dependencies': [], 'replacement_model_id': '', 'replacement_model_name': '', 'deprecation_date': '', 'linked_coe_models': [], 'model_metadata': None, 'model_parallel_rdus': 0, 'metadata': {'labels': {}, 'creationTimestamp': '', 'updateTimestamp': '', 'resourceVersion': '', 'tenant': '', 'owner': '', 'permissions': ['ModelView', 'ModelExport']}}, {'model_id': 'db9ef7f0-a382-11ed-a8fc-0242ac120002', 'model_checkpoint_name': 'GPT_1.5B_NER_Finetuned', 'app_id': 'c27a105f-d0be-4bef-b2a4-4d6bf747ebdc', 'user_id': '', 'tenant_id': '', 'checkpoint_type': 'finetuned', 'model_size_gb': 6, 'dataset': '', 'dataset_info': '', 'dataset_url': '', 'path': 'common/checkpoints/db9ef7f0-a382-11ed-a8fc-0242ac120002/checkpoint', 'sambanova_provided': True, 'version': 1, 'job_config': '', 'description': 'This is a traditionally fine-tuned model used for Named Entity Recognition. The model categorizes the text into: Persons, Organizations, Locations, Misc named entities, and non-entities', 'status': 'AvailableToDownload', 'params': {'invalidates_checkpoint': {'max_seq_length': 512, 'num_intended_classes': 9}, 'modifiable': {'batch_size': 8, 'learning_rate': 5e-06, 'num_iterations': 10536, 'warmup_steps': 0, 'weight_decay': 1}}, 'metrics': '', 'steps': 0, 'application_field': 'language', 'hyperparams': {'train': {}, 'batch_predict': {}, 'deploy': {}}, 'architecture': 'GPT 1.5B', 'pipeline': None, 'model_io': {'train': {'input': {'description': 'A csv file with two columns: token, NER label', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'infer': {'input': {'description': '', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'serve': {'input': {'description': '', 'example': ''}, 'output': {'description': 'The output is a dictionary of the named entities in the input, including the category of entity, and the location within the input.', 'example': '{\"entities\": [{\"begin_offset\": 9, \"end_offset\": 14, \"entity\": \"B-LOC\", \"word\": \"JAPAN\"}, ...'}}}, 'dataset_metadata': {'info': 'CoNLL-2003 is a multitask dataset that was used to fine tune the model for named entity recognition.', 'url': 'https://huggingface.co/datasets/conll2003'}, 'preset_ids': [], 'system_prompt_ids': [], 'device': 'rdu', 'evaluation': {'eval_1': {'dataset': {'info': 'CoNLL-2003 test set', 'url': ''}, 'metrics': {'accuracy': 0.98, 'f1': 0.91, 'precision': 0.91, 'recall': 0.91}}}, 'time_created': '2024-05-15 18:11:26.619848 +0000 UTC', 'time_updated': '2024-05-15 18:11:26.619848 +0000 UTC', 'app_name': 'Named Entity Recognition', 'hidden': False, 'jobTypes': ['train', 'batch_predict', 'deploy'], 'shared_with': [], 'type': '', 'dependencies': [], 'replacement_model_id': '', 'replacement_model_name': '', 'deprecation_date': '', 'linked_coe_models': [], 'model_metadata': None, 'model_parallel_rdus': 0, 'metadata': {'labels': {}, 'creationTimestamp': '', 'updateTimestamp': '', 'resourceVersion': '', 'tenant': '', 'owner': '', 'permissions': ['ModelView', 'ModelExport']}}, {'model_id': '4ed0fef5-63cf-443a-8f9d-6c0437ee803b', 'model_checkpoint_name': 'Llama-2-7b-chat-80k-hf', 'app_id': 'ec012370-6ffa-4a3a-b230-2c62613f1d89', 'user_id': '', 'tenant_id': '', 'checkpoint_type': 'pretrained', 'model_size_gb': 13, 'dataset': '', 'dataset_info': '', 'dataset_url': '', 'path': 'common/checkpoints/4ed0fef5-63cf-443a-8f9d-6c0437ee803b/checkpoint', 'sambanova_provided': True, 'version': 1, 'job_config': '', 'description': 'Llama v2 7B chat checkpoint for up to 80k sequence length size, finedtuned on top of the following model.\\n\\nLlama 2 Community License Agreement: https://ai.meta.com/llama/license/\\nView on HuggingFace: https://huggingface.co/yaofu/llama-2-7b-80k\\n\\nThis model is currently only supported with on SN40L platform.\\n', 'status': 'AvailableToDownload', 'params': {'invalidates_checkpoint': {'max_seq_length': 65536, 'vocab_size': 32000}, 'modifiable': None}, 'metrics': '', 'steps': 0, 'application_field': 'language', 'hyperparams': {'train': {}, 'batch_predict': {}, 'deploy': {}}, 'architecture': 'LLaMA v2 7B', 'pipeline': None, 'model_io': {'train': {'input': {'description': '', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'infer': {'input': {'description': 'A folder containing .jsonl files where each line is a json object containing a \"prompt\" field and a \"completion\" field.', 'example': ''}, 'output': {'description': 'A file called predictions.json which contains the list of generated sequences per input prompt.', 'example': ''}}, 'serve': {'input': {'description': 'The input prompt to be completed by the generative inference model.', 'example': ''}, 'output': {'description': 'The output is a concatenation of the prompt (input) string, and the generated completion.', 'example': ''}}}, 'dataset_metadata': {'info': 'View on Huggingface: https://huggingface.co/yaofu/llama-2-7b-80k\\n', 'url': ''}, 'preset_ids': [], 'system_prompt_ids': [], 'device': 'rdu', 'evaluation': {}, 'time_created': '2024-05-21 19:16:49.709635 +0000 UTC', 'time_updated': '2024-05-21 19:16:49.709635 +0000 UTC', 'app_name': 'Llama 2 7B', 'hidden': False, 'jobTypes': ['train', 'batch_predict', 'deploy'], 'shared_with': [], 'type': '', 'dependencies': [], 'replacement_model_id': '', 'replacement_model_name': '', 'deprecation_date': '', 'linked_coe_models': [], 'model_metadata': None, 'model_parallel_rdus': 0, 'metadata': {'labels': {}, 'creationTimestamp': '', 'updateTimestamp': '', 'resourceVersion': '', 'tenant': '', 'owner': '', 'permissions': ['ModelView', 'ModelExport']}}, {'model_id': 'ee2e89dc-06ce-49ba-83d0-17bf70ef8486', 'model_checkpoint_name': 'bert-qa', 'app_id': 'f25c8247-f73a-4dd9-9871-d5c10675239c', 'user_id': '', 'tenant_id': '', 'checkpoint_type': 'finetuned', 'model_size_gb': 0.42, 'dataset': '', 'dataset_info': '', 'dataset_url': '', 'path': 'common/checkpoints/ee2e89dc-06ce-49ba-83d0-17bf70ef8486/checkpoint', 'sambanova_provided': True, 'version': 1, 'job_config': '', 'description': 'Search algorithms for large document spaces are often great at retreiving relevant information, but can be plauged with a large number of false-positive results (results that may appear relevant, but are not). \\n\\nLarge Lanugage models can analyze these documents in the context of the query to eliminate false positives and rank the relevance of a result. \\nThis checkpoint has been trained to perform this analyzation and reranking task, and can be paired with a document retreival application for a robust and powerful semantic search pipeline. \\n', 'status': 'AvailableToDownload', 'params': {'invalidates_checkpoint': None, 'modifiable': {'batch_size': 1, 'max_seq_length': 512, 'top_k_rerank': 10}}, 'metrics': '', 'steps': 0, 'application_field': 'language', 'hyperparams': {'train': {}, 'batch_predict': {}, 'deploy': {}}, 'architecture': 'BERT base', 'pipeline': None, 'model_io': {'train': {'input': {'description': '', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'infer': {'input': {'description': 'A path to a file containing a document-query pairs.', 'example': ''}, 'output': {'description': 'The top document-query pairs are returned, and if top_k_rerank is equal to the number of documents, all pairs are scored according to the documents relevance to the input query.', 'example': ''}}, 'serve': {'input': {'description': 'A single query with a collection of documents to be reranked based on the query.', 'example': ''}, 'output': {'description': 'The top k documents (based on top_k_rerank flag) in order of their relevence to the query, each document has a corresponding probabliity score that rates its relevence to the query.', 'example': ''}}}, 'dataset_metadata': {'info': '\\nMS MARCO (Microsoft MAchine Reading Comprehension) is a large scale dataset aimed to improve reading comprehension and question answering. The queries are derived from real user queries and the relevant documents are extracted from Bing, which are then further annotated. This checkpoint utlizes this dataset by extracting out the queries and using BM25 to retrieve the most relevant passages in the dataset. The paper states that each query has one releavant passage on average.\\n', 'url': ''}, 'preset_ids': [], 'system_prompt_ids': [], 'device': 'rdu', 'evaluation': {'eval_1': {'dataset': {'info': 'MS MARCO - Dev set', 'url': 'https://microsoft.github.io/msmarco/'}, 'metrics': {'MRR': 34.7}}}, 'time_created': '2024-05-15 18:11:27.081251 +0000 UTC', 'time_updated': '2024-05-15 18:11:27.081251 +0000 UTC', 'app_name': 'Reranking', 'hidden': False, 'jobTypes': ['batch_predict', 'deploy'], 'shared_with': [], 'type': '', 'dependencies': [], 'replacement_model_id': '', 'replacement_model_name': '', 'deprecation_date': '', 'linked_coe_models': [], 'model_metadata': None, 'model_parallel_rdus': 0, 'metadata': {'labels': {}, 'creationTimestamp': '', 'updateTimestamp': '', 'resourceVersion': '', 'tenant': '', 'owner': '', 'permissions': ['ModelView', 'ModelExport']}}, {'model_id': '2fed8f06-4f6c-41c7-97a1-eda878f8f4ab', 'model_checkpoint_name': 'Bert_Base', 'app_id': 'f67c5390-da52-4105-ae17-12434fa7d03b', 'user_id': '', 'tenant_id': '', 'checkpoint_type': 'finetuned', 'model_size_gb': 0.41, 'dataset': '', 'dataset_info': '', 'dataset_url': '', 'path': 'common/checkpoints/2fed8f06-4f6c-41c7-97a1-eda878f8f4ab/checkpoint', 'sambanova_provided': True, 'version': 1, 'job_config': '', 'description': 'The model predicts the punctuation and upper-casing of plain, lower-cased text. \\nExamples of where this model can be applied is in punctuating raw transcripts from ASR models, or other cases when text has lost punctuation. \\nThis model restores the following punctuations -- [, . ?].\\n', 'status': 'AvailableToDownload', 'params': {'invalidates_checkpoint': {'max_seq_length': 512, 'num_intended_classes': 8}, 'modifiable': {'batch_size': 1}}, 'metrics': '', 'steps': 0, 'application_field': 'language', 'hyperparams': {'train': {}, 'batch_predict': {}, 'deploy': {}}, 'architecture': 'BERT base', 'pipeline': None, 'model_io': {'train': {'input': {'description': '', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'infer': {'input': {'description': 'A csv file containing multiple raw texts, one in each row.', 'example': ''}, 'output': {'description': 'A csv file containing the input text with punctuation, one text per row.', 'example': ''}}, 'serve': {'input': {'description': 'Raw text (no casing, no punctuation).', 'example': ''}, 'output': {'description': 'The input text with casing and punctuation.', 'example': ''}}}, 'dataset_metadata': {'info': '\\nTraining Datasets\\n1. Yelp reviews - https://www.tensorflow.org/datasets/catalog/yelp_polarity_reviews\\n2. OpenSubtitles - https://github.com/PolyAI-LDN/conversational-datasets/tree/master/opensubtitles\\n\\nData preparation\\nWe removed all the punctuation and casing from the text and let the model to recover it.\\n', 'url': ''}, 'preset_ids': [], 'system_prompt_ids': [], 'device': 'rdu', 'evaluation': {'eval_1': {'dataset': {'info': 'OpenSubtitles', 'url': ''}, 'metrics': {'Accuracy': 0.9}}}, 'time_created': '2024-05-15 18:11:32.488481 +0000 UTC', 'time_updated': '2024-05-15 18:11:32.488481 +0000 UTC', 'app_name': 'Sentence Detection', 'hidden': False, 'jobTypes': ['batch_predict', 'deploy'], 'shared_with': [], 'type': '', 'dependencies': [], 'replacement_model_id': '', 'replacement_model_name': '', 'deprecation_date': '', 'linked_coe_models': [], 'model_metadata': None, 'model_parallel_rdus': 0, 'metadata': {'labels': {}, 'creationTimestamp': '', 'updateTimestamp': '', 'resourceVersion': '', 'tenant': '', 'owner': '', 'permissions': ['ModelView', 'ModelExport']}}, {'model_id': 'cd9d5e63-3836-4f3e-971a-b115870cd96c', 'model_checkpoint_name': 'Speaker_Diarization', 'app_id': 'cbba6d31-104a-4295-ac21-7e91da09ab9b', 'user_id': '', 'tenant_id': '', 'checkpoint_type': 'finetuned', 'model_size_gb': 0, 'dataset': '', 'dataset_info': '', 'dataset_url': '', 'path': 'common/checkpoints/cd9d5e63-3836-4f3e-971a-b115870cd96c/checkpoint', 'sambanova_provided': True, 'version': 1, 'job_config': '', 'description': '', 'status': 'AvailableToDownload', 'params': None, 'metrics': '', 'steps': 0, 'application_field': 'speech', 'hyperparams': {'train': {}, 'batch_predict': {}, 'deploy': {}}, 'architecture': 'Neural Diarization', 'pipeline': None, 'model_io': {'train': {'input': {'description': '', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'infer': {'input': {'description': '', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'serve': {'input': {'description': '', 'example': ''}, 'output': {'description': '', 'example': ''}}}, 'dataset_metadata': {'info': '', 'url': ''}, 'preset_ids': [], 'system_prompt_ids': [], 'device': 'rdu', 'evaluation': {}, 'time_created': '2024-05-15 18:11:37.72124 +0000 UTC', 'time_updated': '2024-05-15 18:11:37.72124 +0000 UTC', 'app_name': 'Speaker Diarization', 'hidden': False, 'jobTypes': ['batch_predict', 'deploy'], 'shared_with': [], 'type': '', 'dependencies': [], 'replacement_model_id': '', 'replacement_model_name': '', 'deprecation_date': '', 'linked_coe_models': [], 'model_metadata': None, 'model_parallel_rdus': 0, 'metadata': {'labels': {}, 'creationTimestamp': '', 'updateTimestamp': '', 'resourceVersion': '', 'tenant': '', 'owner': '', 'permissions': ['ModelView', 'ModelExport']}}, {'model_id': '47c0113c-4b20-4adf-bde3-be585be8b723', 'model_checkpoint_name': 'Hubert_ASR', 'app_id': 'ecf84906-0924-4ce1-a1a2-c008f5334820', 'user_id': '', 'tenant_id': '', 'checkpoint_type': 'finetuned', 'model_size_gb': 14, 'dataset': '', 'dataset_info': '', 'dataset_url': '', 'path': 'common/checkpoints/47c0113c-4b20-4adf-bde3-be585be8b723/checkpoint', 'sambanova_provided': True, 'version': 1, 'job_config': '', 'description': 'Models pre-trained on raw audio are able to learn powerful representations for downstream speech related tasks, including automatic speech recognition (ASR), similar to pre-trained large language models. \\nThis model is pre-trained on large amount of audio ranging from narrative speech to conversational speech. It is then finetuned on 8kHz conversational phone call dataset to improve performance on low quality audio, such as those from phone calls. \\nIt uses the well known wav2vec 2.0 architecture, derived directly from this paper - https://arxiv.org/abs/2104.01027. \\nWe also paired it with a language model to post-process the transcripts for better quality, and will only predict words from a fixed vocabulary containing approx. 200k words.\\n', 'status': 'AvailableToDownload', 'params': {'invalidates_checkpoint': {'max_seq_length': 245760, 'num_intended_classes': 32}, 'modifiable': {'batch_size': 1}}, 'metrics': '', 'steps': 0, 'application_field': 'speech', 'hyperparams': {'train': {}, 'batch_predict': {}, 'deploy': {}}, 'architecture': 'HuBERT', 'pipeline': None, 'model_io': {'train': {'input': {'description': '', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'infer': {'input': {'description': 'path to a folder containing one or more audio files', 'example': ''}, 'output': {'description': 'A .csv file containing raw (no casing, no punctuation) transcripts of all input audio files, one row per file', 'example': ''}}, 'serve': {'input': {'description': 'A path to a single audio file under 15s.', 'example': ''}, 'output': {'description': 'The raw (no casing, no punctuation) transcript of the input audio.', 'example': ''}}}, 'dataset_metadata': {'info': '\\nPreTraining Datasets\\nThis model was pretrained on C4PILE and saw a total of (possibly duplicated) 300B tokens during its pretraining run. This represents about 1 epoch on the entire dataset very roughly. \\n- Libri-Light (https://arxiv.org/abs/1912.07875): 60k hours of speech from audio books in 16kHz.\\n- Switchboard and Fisher (https://catalog.ldc.upenn.edu/LDC97S62, https://catalog.ldc.upenn.edu/LDC2004S13, https://catalog.ldc.upenn.edu/LDC2005S13): 2.5k hours telephone conversational speech recorded in 8kHz.\\n- CommonVoice (https://commonvoice.mozilla.org/): 700 hours of crowd-sourced recordings of Wikipedia sentences in 48kHz.\\n\\nFinetuning Dataset\\n- Switchboard (https://catalog.ldc.upenn.edu/LDC97S62): 300 hours telephone conversational speech recorded in 8kHz.\\n\\nData Preparation\\nWe follow the standard steps described in https://arxiv.org/abs/2104.01027\\n', 'url': ''}, 'preset_ids': [], 'system_prompt_ids': [], 'device': 'rdu', 'evaluation': {'eval_1': {'dataset': {'info': 'CallHome (https://catalog.ldc.upenn.edu/LDC2002S09) - Phone call conversational 8kHZ', 'url': ''}, 'metrics': {'Word Error Rate': 0.099}}, 'eval_2': {'dataset': {'info': 'Librispeech (https://www.openslr.org/12) test-clean - Audio book 16kHZ', 'url': ''}, 'metrics': {'Word Error Rate': 0.033}}, 'eval_3': {'dataset': {'info': 'Librispeech (https://www.openslr.org/12) test-other - Audio book 16kHZ', 'url': ''}, 'metrics': {'Word Error Rate': 0.071}}}, 'time_created': '2024-05-15 18:11:53.372674 +0000 UTC', 'time_updated': '2024-05-15 18:11:53.372674 +0000 UTC', 'app_name': 'Speech Recognition', 'hidden': False, 'jobTypes': ['train', 'batch_predict', 'deploy'], 'shared_with': [], 'type': '', 'dependencies': [], 'replacement_model_id': '', 'replacement_model_name': '', 'deprecation_date': '', 'linked_coe_models': [], 'model_metadata': None, 'model_parallel_rdus': 0, 'metadata': {'labels': {}, 'creationTimestamp': '', 'updateTimestamp': '', 'resourceVersion': '', 'tenant': '', 'owner': '', 'permissions': ['ModelView', 'ModelExport']}}, {'model_id': '79b4bbdb-e40a-4eb0-a495-07a64393cc1f', 'model_checkpoint_name': 'CLIP-ViT-B-32-laion2B-s34B-b79k-v2', 'app_id': '6c14325a-1be7-4e48-b38f-19b33745fc3b', 'user_id': '', 'tenant_id': '', 'checkpoint_type': 'pretrained', 'model_size_gb': 0.57, 'dataset': '', 'dataset_info': '', 'dataset_url': '', 'path': 'common/checkpoints/79b4bbdb-e40a-4eb0-a495-07a64393cc1f/checkpoint', 'sambanova_provided': True, 'version': 1, 'job_config': '', 'description': 'CLIP is a multi-modal neural network trained on (image, text) pairs. Using a ViT-B-32 Backbone for visual features and a language model for text features, CLIP projects both modalities to a shared latent space, with the dot product between them used as a similarity score. This CLIP-ViT checkpoint (adapted from huggingface: https://huggingface.co/laion/CLIP-ViT-B-32-laion2B-s34B-b79K) provides the learned knowledge after pretraining on the Laion2B dataset, with a primary usecase is Zero-shot image classification, as well as image and text retrieval.  \\n<br>\\n\\n## Running batch inference on a dataset: \\nFor a batch inference job, a dataset can be uploaded in the form of a directory. The directory should include the images and a metafile `predictions.csv` with the following 4 columns: `image_path`, `description`, `subset`, `metadata`. The columns with subset and metadata can be left empty.\\n  - `image_path`: the relative path to a given image inside of the dataset directory\\n  - `description`: A text description to encode\\n\\nNote: A given row should contain only one of `image_path` or `description`. \\n<br> \\nLicense: https://choosealicense.com/licenses/mit/\\n', 'status': 'Available', 'params': None, 'metrics': '', 'steps': 0, 'application_field': 'vision', 'hyperparams': {'train': {}, 'batch_predict': {}, 'deploy': {}}, 'architecture': 'CLIP ViT B', 'pipeline': None, 'model_io': {'train': {'input': {'description': '', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'infer': {'input': {'description': 'A dataset folder of examples as described above. \\n', 'example': ''}, 'output': {'description': 'The output of predictions is saved to a json lines file.\\nIt contains a list of per input prediction values.\\n', 'example': ''}}, 'serve': {'input': {'description': 'Either an image file, or a text string\\n', 'example': ''}, 'output': {'description': 'The generated image or text embeddings.\\n', 'example': ''}}}, 'dataset_metadata': {'info': 'Pretraining Dataset\\nThis model was trained with a 2 Billion sample subset of LAION-5B (https://laion.ai/blog/laion-5b/) consisting of descriptions in the English language.\\n', 'url': ''}, 'preset_ids': [], 'system_prompt_ids': [], 'device': 'rdu', 'evaluation': {'ImageNet2012': {'dataset': {'info': 'ImageNet2012 validation set', 'url': ''}, 'metrics': {'accuracy': 0.66}}}, 'time_created': '2024-05-22 08:50:15.026826 +0000 UTC', 'time_updated': '2024-07-22 14:42:08.724055 +0000 UTC', 'app_name': 'CLIP', 'hidden': False, 'jobTypes': ['batch_predict', 'deploy'], 'shared_with': [], 'type': '', 'dependencies': [], 'replacement_model_id': '', 'replacement_model_name': '', 'deprecation_date': '', 'linked_coe_models': [], 'model_metadata': None, 'model_parallel_rdus': 0, 'metadata': {'labels': {}, 'creationTimestamp': '', 'updateTimestamp': '', 'resourceVersion': '', 'tenant': '', 'owner': '', 'permissions': ['ModelView', 'ModelExport']}}, {'model_id': '39300da3-7f6a-4f7d-a365-67b4e32b6573', 'model_checkpoint_name': 'CLIP-ViT-B-32-laion2B-s34B-b79k', 'app_id': '6c14325a-1be7-4e48-b38f-19b33745fc3b', 'user_id': '', 'tenant_id': '', 'checkpoint_type': 'pretrained', 'model_size_gb': 0.57, 'dataset': '', 'dataset_info': '', 'dataset_url': '', 'path': 'common/checkpoints/39300da3-7f6a-4f7d-a365-67b4e32b6573/checkpoint', 'sambanova_provided': True, 'version': 1, 'job_config': '', 'description': 'CLIP is a multi-modal neural network trained on (image, text) pairs. Using a ViT-B-32 Backbone for visual features and a language model for text features, CLIP projects both modalities to a shared latent space, with the dot product between them used as a similarity score. This CLIP-ViT checkpoint (adapted from huggingface: https://huggingface.co/laion/CLIP-ViT-B-32-laion2B-s34B-b79K) provides the learned knowledge after pretraining on the Laion2B dataset, with a primary usecase is Zero-shot image classification, as well as image and text retrieval.  \\n<br>\\n\\n## Running batch inference on a dataset: \\nFor a batch inference job, a dataset can be uploaded in the form of a directory. The directory should include the images and a metafile `predictions.csv` with the following 4 columns: `image_path`, `description`, `subset`, `metadata`. The columns with subset and metadata can be left empty.\\n  - `image_path`: the relative path to a given image inside of the dataset directory\\n  - `description`: A text description to encode\\n\\nNote: A given row should contain only one of `image_path` or `description`. \\n<br> \\nLicense: https://choosealicense.com/licenses/mit/\\n', 'status': 'AvailableToDownload', 'params': None, 'metrics': '', 'steps': 0, 'application_field': 'vision', 'hyperparams': {'train': {}, 'batch_predict': {}, 'deploy': {}}, 'architecture': 'CLIP ViT B', 'pipeline': None, 'model_io': {'train': {'input': {'description': '', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'infer': {'input': {'description': 'A dataset folder of examples as described above. \\n', 'example': ''}, 'output': {'description': 'The output of predictions is saved to a json lines file.\\nIt contains a list of per input prediction values.\\n', 'example': ''}}, 'serve': {'input': {'description': '', 'example': ''}, 'output': {'description': '', 'example': ''}}}, 'dataset_metadata': {'info': 'Pretraining Dataset\\nThis model was trained with a 2 Billion sample subset of LAION-5B (https://laion.ai/blog/laion-5b/) consisting of descriptions in the English language.\\n', 'url': ''}, 'preset_ids': [], 'system_prompt_ids': [], 'device': 'rdu', 'evaluation': {'ImageNet2012': {'dataset': {'info': 'ImageNet2012 validation set', 'url': ''}, 'metrics': {'accuracy': 0.66}}}, 'time_created': '2024-05-15 18:05:20.970505 +0000 UTC', 'time_updated': '2024-05-15 18:05:20.970505 +0000 UTC', 'app_name': 'CLIP', 'hidden': False, 'jobTypes': ['train', 'batch_predict', 'deploy'], 'shared_with': [], 'type': '', 'dependencies': [], 'replacement_model_id': '', 'replacement_model_name': '', 'deprecation_date': '', 'linked_coe_models': [], 'model_metadata': None, 'model_parallel_rdus': 0, 'metadata': {'labels': {}, 'creationTimestamp': '', 'updateTimestamp': '', 'resourceVersion': '', 'tenant': '', 'owner': '', 'permissions': ['ModelView', 'ModelExport']}}, {'model_id': '96252b4d-5a9f-41fa-9159-5aa88e4b4021', 'model_checkpoint_name': 'meta-llama-3-8b', 'app_id': 'ad39e323-9878-4914-8e29-82c9f2939475', 'user_id': '', 'tenant_id': '', 'checkpoint_type': 'pretrained', 'model_size_gb': 15, 'dataset': '', 'dataset_info': '', 'dataset_url': '', 'path': 'common/checkpoints/96252b4d-5a9f-41fa-9159-5aa88e4b4021/checkpoint', 'sambanova_provided': True, 'version': 1, 'job_config': '', 'description': 'Llama 3 is the latest iteration of the large language model (LLM) developed and released by Meta. It was pretrained on over 15 trillion tokens of data from publicly available sources. Llama 3 is intended for commercial and research use in English. Instruction tuned models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. Please refer to the provided link for detailed information on licensing agreements and terms of use.\\n\\nLlama 3 Community License Agreement: https://llama.meta.com/llama3/license/\\nView on HuggingFace: https://huggingface.co/meta-llama/Meta-Llama-3-8B\\n', 'status': 'Available', 'params': {'invalidates_checkpoint': {'max_seq_length': 8192, 'model_parameter_count': '8b', 'vocab_size': 128384}, 'modifiable': None}, 'metrics': '', 'steps': 0, 'application_field': 'language', 'hyperparams': {'train': {}, 'batch_predict': {}, 'deploy': {}}, 'architecture': 'Llama v3', 'pipeline': None, 'model_io': {'train': {'input': {'description': 'A folder containing .hdf5 files produced by the SambaNova generative_data_prep package (please see documentation for usage instructions).', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'infer': {'input': {'description': '', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'serve': {'input': {'description': '', 'example': ''}, 'output': {'description': '', 'example': ''}}}, 'dataset_metadata': {'info': 'View on HuggingFace: https://huggingface.co/meta-llama/Meta-Llama-3-8B\\n', 'url': ''}, 'preset_ids': [], 'system_prompt_ids': [], 'device': 'rdu', 'evaluation': {}, 'time_created': '2024-06-05 22:06:18.583188 +0000 UTC', 'time_updated': '2024-07-08 21:32:34.209554 +0000 UTC', 'app_name': 'Llama 3', 'hidden': False, 'jobTypes': ['train'], 'shared_with': [], 'type': '', 'dependencies': [], 'replacement_model_id': '', 'replacement_model_name': '', 'deprecation_date': '', 'linked_coe_models': [], 'model_metadata': None, 'model_parallel_rdus': 0, 'metadata': {'labels': {}, 'creationTimestamp': '', 'updateTimestamp': '', 'resourceVersion': '', 'tenant': '', 'owner': '', 'permissions': ['ModelView', 'ModelExport']}}, {'model_id': '1eec815d-1d9f-4759-81e9-e77c73892c07', 'model_checkpoint_name': 'meta-llama-3-8b-instruct', 'app_id': 'ad39e323-9878-4914-8e29-82c9f2939475', 'user_id': '', 'tenant_id': '', 'checkpoint_type': 'finetuned', 'model_size_gb': 15, 'dataset': '', 'dataset_info': '', 'dataset_url': '', 'path': 'common/checkpoints/1eec815d-1d9f-4759-81e9-e77c73892c07/checkpoint', 'sambanova_provided': True, 'version': 1, 'job_config': '', 'description': 'Llama 3 is the latest iteration of the large language model (LLM) developed and released by Meta. It comes with 8B and a 70B parameter version. Each was pretrained on over 15 trillion tokens of data from publicly available sources. The instruct model is optimized for dialogue use cases and prioritizing helpfulness and safety. These models outperform many open-source chat models on industry benchmarks and employ supervised fine-tuning and reinforcement learning with human feedback. The models are auto-regressive, using an optimized transformer architecture, and trained on publicly available online data. The models are designed for English language use in commercial and research settings, suitable for chat assistant applications, and natural language generation tasks. However, they are not intended for use in violation of any laws or regulations or in languages other than English unless developers comply with specific licensing and policy requirements.\\n\\nThis model requires the following prompt template to be used in order for it to respond properly. At inference time, please make sure the inference parameter: \"process_prompt\" is set to true. When \"process_prompt\" is set to true the platform will construct the template automatically behind the scene. When it is set to false, please format your input according to the below prompt template.\\n\\nPlease refer to the provided link for detailed information on licensing agreements and terms of use.\\nLlama 3 Community License Agreement: https://llama.meta.com/llama3/license/\\nView on HuggingFace: https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct\\n\\n<br>\\n\\n## Prompt template\\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n', 'status': 'Available', 'params': {'invalidates_checkpoint': {'max_seq_length': 8192, 'model_parameter_count': '8b', 'vocab_size': 128384}, 'modifiable': None}, 'metrics': '', 'steps': 0, 'application_field': 'language', 'hyperparams': {'train': {}, 'batch_predict': {}, 'deploy': {}}, 'architecture': 'Llama v3', 'pipeline': None, 'model_io': {'train': {'input': {'description': 'A folder containing .hdf5 files produced by the SambaNova generative_data_prep package (please see documentation for usage instructions).', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'infer': {'input': {'description': '', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'serve': {'input': {'description': '', 'example': ''}, 'output': {'description': '', 'example': ''}}}, 'dataset_metadata': {'info': 'View on HuggingFace: https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct\\n', 'url': ''}, 'preset_ids': [], 'system_prompt_ids': [], 'device': 'rdu', 'evaluation': {}, 'time_created': '2024-06-05 22:06:23.74952 +0000 UTC', 'time_updated': '2024-07-08 21:32:53.038759 +0000 UTC', 'app_name': 'Llama 3', 'hidden': False, 'jobTypes': ['train'], 'shared_with': [], 'type': '', 'dependencies': [], 'replacement_model_id': '', 'replacement_model_name': '', 'deprecation_date': '', 'linked_coe_models': [], 'model_metadata': None, 'model_parallel_rdus': 0, 'metadata': {'labels': {}, 'creationTimestamp': '', 'updateTimestamp': '', 'resourceVersion': '', 'tenant': '', 'owner': '', 'permissions': ['ModelView', 'ModelExport']}}, {'model_id': 'c828d8d6-bbc6-4315-8573-014a183ffa58', 'model_checkpoint_name': 'llama2_7b_fine_tuned_nstext2sql', 'app_id': 'ec012370-6ffa-4a3a-b230-2c62613f1d89', 'user_id': 'jorge.piedrahita', 'tenant_id': '41ceaded-9f08-47ae-aa02-15f39c899618', 'checkpoint_type': 'finetuned', 'model_size_gb': 0, 'dataset': 'nstext2sql_sql_fine_tuning_mm', 'dataset_info': '', 'dataset_url': '', 'path': 'default/default/checkpoints/f00108a2-d35c-4d59-b7a9-f82b4d0c0ede-10000', 'sambanova_provided': False, 'version': 1, 'job_config': \"{'debug_mode': 'false', 'do_eval': 'false', 'dump_inputs': 'false', 'eval_steps': '50', 'evaluation_strategy': 'no', 'fix_rank_rdu_mapping': 'false', 'learning_rate': '0.00001', 'logging_steps': '1', 'lr_schedule': 'fixed_lr', 'max_seq_length': '4096', 'num_iterations': '75000', 'prompt_loss_weight': '0', 'save_optimizer_state': 'true', 'save_steps': '10000', 'skip_checkpoint': 'false', 'sockets': '1', 'subsample_eval': '0.01', 'subsample_eval_seed': '123', 'use_token_type_ids': 'true', 'vocab_size': '32000', 'warmup_steps': '0', 'weight_decay': '0.1'}\", 'description': 'finetuned llama2_7b model for nstext2sql', 'status': 'Available', 'params': {'invalidates_checkpoint': {'max_seq_length': 4096, 'vocab_size': 32000}, 'modifiable': None}, 'metrics': '', 'steps': 10000, 'application_field': 'language', 'hyperparams': {'train': {}, 'batch_predict': {}, 'deploy': {}}, 'architecture': 'LLaMA v2 7B', 'pipeline': None, 'model_io': {'train': {'input': {'description': 'A folder containing .hdf5 files produced by the SambaNova generative_data_prep package (please see documentation for usage instructions).', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'infer': {'input': {'description': 'A folder containing .jsonl files where each line is a json object containing a \"prompt\" field and a \"completion\" field.', 'example': ''}, 'output': {'description': 'A file called predictions.json which contains the list of generated sequences per input prompt', 'example': ''}}, 'serve': {'input': {'description': 'The input prompt to be completed by the generative inference model.', 'example': ''}, 'output': {'description': 'The output is a concatenation of the prompt (input) string, and the generated completion.', 'example': ''}}}, 'dataset_metadata': {'info': '', 'url': ''}, 'preset_ids': [], 'system_prompt_ids': [], 'device': 'rdu', 'evaluation': {}, 'time_created': '2024-07-15 18:27:53.64838 +0000 UTC', 'time_updated': '2024-07-15 18:27:53.657121 +0000 UTC', 'app_name': 'Llama 2 7B', 'hidden': False, 'jobTypes': ['train', 'batch_predict', 'deploy'], 'shared_with': [], 'type': '', 'dependencies': [], 'replacement_model_id': '', 'replacement_model_name': '', 'deprecation_date': '', 'linked_coe_models': [], 'model_metadata': {'publisher': '', 'category': '', 'license': '', 'languages': '', 'modelArchitecture': '', 'isMultiturnChat': False}, 'model_parallel_rdus': 0, 'metadata': {'labels': {}, 'creationTimestamp': '', 'updateTimestamp': '', 'resourceVersion': '', 'tenant': '', 'owner': '', 'permissions': ['ModelView', 'ModelExport', 'ModelDelete', 'ModelEdit', 'ModelShare']}}, {'model_id': '0056bc7b-8a30-456a-92c3-b4e640e64076', 'model_checkpoint_name': 'Llama-2-70b-chat-hf', 'app_id': '82254d3b-7239-458b-9da8-da1aca9b7fba', 'user_id': '', 'tenant_id': '', 'checkpoint_type': 'pretrained', 'model_size_gb': 129, 'dataset': '', 'dataset_info': '', 'dataset_url': '', 'path': 'common/checkpoints/0056bc7b-8a30-456a-92c3-b4e640e64076/checkpoint', 'sambanova_provided': True, 'version': 1, 'job_config': '', 'description': 'Llama 2 is the latest iteration of the large language model (LLM) developed and released by Meta. It is pretrained on 2 trillion tokens of publicly available data and is designed to enable developers to build generative AI applications. Please refer to the provided link for detailed information on licensing agreements and terms of use.\\n\\nLlama 2 Community License Agreement: https://ai.meta.com/llama/license/\\nView on HuggingFace: https://huggingface.co/meta-llama/Llama-2-70b-chat-hf\\n', 'status': 'Available', 'params': {'invalidates_checkpoint': {'max_seq_length': 4096, 'vocab_size': 32000}, 'modifiable': None}, 'metrics': '', 'steps': 0, 'application_field': 'language', 'hyperparams': {'train': {}, 'batch_predict': {}, 'deploy': {}}, 'architecture': 'LLaMA v2 70B', 'pipeline': None, 'model_io': {'train': {'input': {'description': 'A folder containing .hdf5 files produced by the SambaNova generative_data_prep package (please see documentation for usage instructions).', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'infer': {'input': {'description': 'A folder containing .jsonl files where each line is a json object containing a \"prompt\" field and a \"completion\" field.', 'example': ''}, 'output': {'description': 'A file called predictions.json which contains the list of generated sequences per input prompt', 'example': ''}}, 'serve': {'input': {'description': 'The input prompt to be completed by the generative inference model.', 'example': ''}, 'output': {'description': 'The output is a concatenation of the prompt (input) string, and the generated completion.', 'example': ''}}}, 'dataset_metadata': {'info': 'View on HuggingFace: https://huggingface.co/meta-llama/Llama-2-70b-chat-hf\\n', 'url': ''}, 'preset_ids': [], 'system_prompt_ids': [], 'device': 'rdu', 'evaluation': {}, 'time_created': '2024-05-15 18:07:41.987347 +0000 UTC', 'time_updated': '2024-06-03 15:50:51.460972 +0000 UTC', 'app_name': 'Llama 2 70B', 'hidden': False, 'jobTypes': ['train'], 'shared_with': [], 'type': '', 'dependencies': [], 'replacement_model_id': '', 'replacement_model_name': '', 'deprecation_date': '', 'linked_coe_models': [], 'model_metadata': None, 'model_parallel_rdus': 0, 'metadata': {'labels': {}, 'creationTimestamp': '', 'updateTimestamp': '', 'resourceVersion': '', 'tenant': '', 'owner': '', 'permissions': ['ModelView', 'ModelExport']}}, {'model_id': '6090d4ac-a7bd-4c46-b417-7f8e42cf7bdb', 'model_checkpoint_name': 'Llama-2-7b-chat-hf', 'app_id': 'ec012370-6ffa-4a3a-b230-2c62613f1d89', 'user_id': '', 'tenant_id': '', 'checkpoint_type': 'pretrained', 'model_size_gb': 13, 'dataset': '', 'dataset_info': '', 'dataset_url': '', 'path': 'common/checkpoints/6090d4ac-a7bd-4c46-b417-7f8e42cf7bdb/checkpoint', 'sambanova_provided': True, 'version': 1, 'job_config': '', 'description': 'Llama 2 is the latest iteration of the large language model (LLM) developed and released by Meta. It is pretrained on 2 trillion tokens of publicly available data and is designed to enable developers to build generative AI applications. Please refer to the provided link for detailed information on licensing agreements and terms of use.\\n\\nLlama 2 Community License Agreement: https://ai.meta.com/llama/license/\\nView on HuggingFace: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\\n', 'status': 'Available', 'params': {'invalidates_checkpoint': {'max_seq_length': 4096, 'vocab_size': 32000}, 'modifiable': None}, 'metrics': '', 'steps': 0, 'application_field': 'language', 'hyperparams': {'train': {}, 'batch_predict': {}, 'deploy': {}}, 'architecture': 'LLaMA v2 7B', 'pipeline': None, 'model_io': {'train': {'input': {'description': 'A folder containing .hdf5 files produced by the SambaNova generative_data_prep package (please see documentation for usage instructions).', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'infer': {'input': {'description': 'A folder containing .jsonl files where each line is a json object containing a \"prompt\" field and a \"completion\" field.', 'example': ''}, 'output': {'description': 'A file called predictions.json which contains the list of generated sequences per input prompt', 'example': ''}}, 'serve': {'input': {'description': 'The input prompt to be completed by the generative inference model.', 'example': ''}, 'output': {'description': 'The output is a concatenation of the prompt (input) string, and the generated completion.', 'example': ''}}}, 'dataset_metadata': {'info': 'View on Huggingface: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\\n', 'url': ''}, 'preset_ids': [], 'system_prompt_ids': [], 'device': 'rdu', 'evaluation': {}, 'time_created': '2024-05-15 18:09:44.410252 +0000 UTC', 'time_updated': '2024-05-16 21:08:35.964223 +0000 UTC', 'app_name': 'Llama 2 7B', 'hidden': False, 'jobTypes': ['train', 'batch_predict', 'deploy'], 'shared_with': [], 'type': '', 'dependencies': [], 'replacement_model_id': '', 'replacement_model_name': '', 'deprecation_date': '', 'linked_coe_models': [], 'model_metadata': None, 'model_parallel_rdus': 0, 'metadata': {'labels': {}, 'creationTimestamp': '', 'updateTimestamp': '', 'resourceVersion': '', 'tenant': '', 'owner': '', 'permissions': ['ModelView', 'ModelExport']}}, {'model_id': '5ec326b1-be32-4fce-b335-74097eb3409c', 'model_checkpoint_name': 'Llama-2-7b-80k-hf', 'app_id': 'ec012370-6ffa-4a3a-b230-2c62613f1d89', 'user_id': '', 'tenant_id': '', 'checkpoint_type': 'pretrained', 'model_size_gb': 13, 'dataset': '', 'dataset_info': '', 'dataset_url': '', 'path': 'common/checkpoints/5ec326b1-be32-4fce-b335-74097eb3409c/checkpoint', 'sambanova_provided': True, 'version': 1, 'job_config': '', 'description': 'Llama v2 7B checkpoint for up to 80k sequence length size.\\n\\nLlama 2 Community License Agreement: https://ai.meta.com/llama/license/\\nView on HuggingFace: https://huggingface.co/yaofu/llama-2-7b-80k\\n\\nThis model is currently only supported with on SN40L platform.\\n', 'status': 'AvailableToDownload', 'params': {'invalidates_checkpoint': {'max_seq_length': 65536, 'vocab_size': 32000}, 'modifiable': None}, 'metrics': '', 'steps': 0, 'application_field': 'language', 'hyperparams': {'train': {}, 'batch_predict': {}, 'deploy': {}}, 'architecture': 'LLaMA v2 7B', 'pipeline': None, 'model_io': {'train': {'input': {'description': '', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'infer': {'input': {'description': 'A folder containing .jsonl files where each line is a json object containing a \"prompt\" field and a \"completion\" field.', 'example': ''}, 'output': {'description': 'A file called predictions.json which contains the list of generated sequences per input prompt.', 'example': ''}}, 'serve': {'input': {'description': 'The input prompt to be completed by the generative inference model.', 'example': ''}, 'output': {'description': 'The output is a concatenation of the prompt (input) string, and the generated completion.', 'example': ''}}}, 'dataset_metadata': {'info': 'View on HuggingFace: https://huggingface.co/yaofu/llama-2-7b-80k\\n', 'url': ''}, 'preset_ids': [], 'system_prompt_ids': [], 'device': 'rdu', 'evaluation': {}, 'time_created': '2024-05-21 19:16:44.031295 +0000 UTC', 'time_updated': '2024-05-21 19:16:44.031295 +0000 UTC', 'app_name': 'Llama 2 7B', 'hidden': False, 'jobTypes': ['train', 'batch_predict', 'deploy'], 'shared_with': [], 'type': '', 'dependencies': [], 'replacement_model_id': '', 'replacement_model_name': '', 'deprecation_date': '', 'linked_coe_models': [], 'model_metadata': None, 'model_parallel_rdus': 0, 'metadata': {'labels': {}, 'creationTimestamp': '', 'updateTimestamp': '', 'resourceVersion': '', 'tenant': '', 'owner': '', 'permissions': ['ModelView', 'ModelExport']}}, {'model_id': '24d8fb20-2515-492d-a0d1-f5f30d81d6a3', 'model_checkpoint_name': 'Llama-2-70b-chat-16k-hf', 'app_id': '82254d3b-7239-458b-9da8-da1aca9b7fba', 'user_id': '', 'tenant_id': '', 'checkpoint_type': 'pretrained', 'model_size_gb': 129, 'dataset': '', 'dataset_info': '', 'dataset_url': '', 'path': 'common/checkpoints/24d8fb20-2515-492d-a0d1-f5f30d81d6a3/checkpoint', 'sambanova_provided': True, 'version': 1, 'job_config': '', 'description': 'Llama 2 is the latest iteration of the large language model (LLM) developed and released by Meta. It is pretrained on 2 trillion tokens of publicly available data and is designed to enable developers to build generative AI applications. Please refer to the provided link for detailed information on licensing agreements and terms of use.\\n\\nLlama 2 Community License Agreement: https://ai.meta.com/llama/license/\\nView on HuggingFace: https://huggingface.co/meta-llama/Llama-2-70b-chat-hf\\n\\nAvailability on RDU architectures: This model is currently only supported for training and inference (both batch inference and endpoints) on SN40.\\n', 'status': 'AvailableToDownload', 'params': {'invalidates_checkpoint': {'max_seq_length': 16384, 'vocab_size': 32000}, 'modifiable': None}, 'metrics': '', 'steps': 0, 'application_field': 'language', 'hyperparams': {'train': {}, 'batch_predict': {}, 'deploy': {}}, 'architecture': 'LLaMA v2 70B', 'pipeline': None, 'model_io': {'train': {'input': {'description': 'A folder containing .hdf5 files produced by the SambaNova generative_data_prep package (please see documentation for usage instructions).', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'infer': {'input': {'description': 'A folder containing .jsonl files where each line is a json object containing a \"prompt\" field and a \"completion\" field.', 'example': ''}, 'output': {'description': 'A file called predictions.json which contains the list of generated sequences per input prompt', 'example': ''}}, 'serve': {'input': {'description': 'The input prompt to be completed by the generative inference model.', 'example': ''}, 'output': {'description': 'The output is a concatenation of the prompt (input) string, and the generated completion.', 'example': ''}}}, 'dataset_metadata': {'info': 'View on HuggingFace: https://huggingface.co/meta-llama/Llama-2-70b-chat-hf\\n', 'url': ''}, 'preset_ids': [], 'system_prompt_ids': [], 'device': 'rdu', 'evaluation': {}, 'time_created': '2024-05-15 18:07:21.356531 +0000 UTC', 'time_updated': '2024-05-15 18:07:21.356531 +0000 UTC', 'app_name': 'Llama 2 70B', 'hidden': False, 'jobTypes': ['train'], 'shared_with': [], 'type': '', 'dependencies': [], 'replacement_model_id': '', 'replacement_model_name': '', 'deprecation_date': '', 'linked_coe_models': [], 'model_metadata': None, 'model_parallel_rdus': 0, 'metadata': {'labels': {}, 'creationTimestamp': '', 'updateTimestamp': '', 'resourceVersion': '', 'tenant': '', 'owner': '', 'permissions': ['ModelView', 'ModelExport']}}, {'model_id': 'eaf458c5-25d2-4b16-9b94-75d8801f6d8c', 'model_checkpoint_name': 'Llama-2-7b-sambalingo-thai-base-hf', 'app_id': 'ec012370-6ffa-4a3a-b230-2c62613f1d89', 'user_id': '', 'tenant_id': '', 'checkpoint_type': 'pretrained', 'model_size_gb': 26, 'dataset': '', 'dataset_info': '', 'dataset_url': '', 'path': 'common/checkpoints/eaf458c5-25d2-4b16-9b94-75d8801f6d8c/checkpoint', 'sambanova_provided': True, 'version': 1, 'job_config': '', 'description': 'SambaLingo-Thai-Base is a pretrained Bi-lingual Thai and English model that adapts Llama-2-7b to Thai by training on 38 billion tokens from the Thai split of the Cultura-X dataset. This model reports state of the art evaluation results in perplexity and FLORES-200 translation. \\n\\nThis checkpoint can serve two primary use cases.\\n1. Continuous pretraining on English and Thai corpus.\\n2. IT on Thai tasks\\n\\nThis model is currently only supported with Train Mode on SN40L platform.\\n', 'status': 'AvailableToDownload', 'params': {'invalidates_checkpoint': {'max_seq_length': 4096, 'vocab_size': 57344}, 'modifiable': None}, 'metrics': '', 'steps': 0, 'application_field': 'language', 'hyperparams': {'train': {}, 'batch_predict': {}, 'deploy': {}}, 'architecture': 'LLaMA v2 7B', 'pipeline': None, 'model_io': {'train': {'input': {'description': 'A folder containing .hdf5 files produced by the SambaNova generative_data_prep package (please see documentation for usage instructions).', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'infer': {'input': {'description': '', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'serve': {'input': {'description': '', 'example': ''}, 'output': {'description': '', 'example': ''}}}, 'dataset_metadata': {'info': 'View on Huggingface: https://huggingface.co/sambanovasystems/SambaLingo-Thai-Base\\n', 'url': ''}, 'preset_ids': [], 'system_prompt_ids': [], 'device': 'rdu', 'evaluation': {}, 'time_created': '2024-05-21 19:16:50.237944 +0000 UTC', 'time_updated': '2024-05-21 19:16:50.237944 +0000 UTC', 'app_name': 'Llama 2 7B', 'hidden': False, 'jobTypes': ['train', 'batch_predict', 'deploy'], 'shared_with': [], 'type': '', 'dependencies': [], 'replacement_model_id': '', 'replacement_model_name': '', 'deprecation_date': '', 'linked_coe_models': [], 'model_metadata': None, 'model_parallel_rdus': 0, 'metadata': {'labels': {}, 'creationTimestamp': '', 'updateTimestamp': '', 'resourceVersion': '', 'tenant': '', 'owner': '', 'permissions': ['ModelView', 'ModelExport']}}, {'model_id': '2a12977a-dfd3-41ea-b6b8-e4bb2a9ab04a', 'model_checkpoint_name': 'E5 Large V2', 'app_id': '89fbfbe6-ee77-4f5c-9ff6-56e2ab69f6ee', 'user_id': '', 'tenant_id': '', 'checkpoint_type': 'pretrained', 'model_size_gb': 1.2, 'dataset': '', 'dataset_info': '', 'dataset_url': '', 'path': 'common/checkpoints/2a12977a-dfd3-41ea-b6b8-e4bb2a9ab04a/checkpoint', 'sambanova_provided': True, 'version': 1, 'job_config': '', 'description': 'This checkpoint is a huggingface pretrained checkpoint that has been trained on query-passage text pairs.\\nThis checkpoint can be used in the following ways:\\n1.  As a document embedding model, by prefixing the text with \"passage: \".\\n2.  As a query embedding model, by prefixing the query with \"query: \".\\nSee the paper for more information: \\u200bhttps://arxiv.org/pdf/2212.03533.pdf.\\n', 'status': 'Available', 'params': {'invalidates_checkpoint': {'max_seq_length': 512, 'model_arch_type': 'bert'}, 'modifiable': None}, 'metrics': '', 'steps': 0, 'application_field': 'language', 'hyperparams': {'train': {}, 'batch_predict': {}, 'deploy': {}}, 'architecture': 'BERT', 'pipeline': None, 'model_io': {'train': {'input': {'description': 'The training dataset should include a train.jsonl file and passages.jsonl file. passages.jsonl contains all of the passages / documents that a query may match to. Each json object in passages.jsonl should contain a \"contents\" field with a string value representing the document. Note that the passage ID numbers that train.jsonl refers to will be the line number stored within the passages.jsonl file, so the ordering of passages in passages.jsonl matters for referencing the passages in train.jsonl. Each json object in train.jsonl includes a \"query\" text field, along with \"positives\" and \"negatives\". \"positives\" has a \"doc_id\" key which points to a list of IDs, where each ID is the line number in passages.jsonl of a positive document match for the \"query\" text. \"negatives\" has the same structure as \"positives\", but the IDs should correspond to line numbers in passages.jsonl of negative document matches for the \"query\" text. The number of positive document matches may vary from 1 -> inf, while the number of negative documents may vary from (train_n_passages - 1) -> inf. This is because train_n_passages dictates how many positive + negative passages are associated with a query. Here is an example of a row in train.jsonl from the MSMarco dataset - {\"query\":\")what was the immediate impact of the success of the manhattan project?\",\"positives\":{\"doc_id\":[\"0\"]},\"negatives\":{\"doc_id\":[\"2942565\",\"1668795\",\"3870086\",\"6324845\",\"7249233\",\"3870083\",\"2616684\",\"2395250\",\"2942570\",\"9\"]}}\\n', 'example': ''}, 'output': {'description': '', 'example': ''}}, 'infer': {'input': {'description': 'A folder containing .json files where the json object contains a \"text_list\" key.\\nThis key should contain a list of text. These texts are what will be embedded by the model.\\nMake sure to include a \"query: \" or \"passage: \" prefix depending on the text being embedded.\\n', 'example': ''}, 'output': {'description': 'An output folder with a json file for each inputted json file. \\nThese new json files contain the embeddings along with reference to the text it corresponds to in the original json file.\\n', 'example': ''}}, 'serve': {'input': {'description': 'The raw input text to embed. The text will be automatically truncated to a maximum of 512 tokens.\\nMake sure to include a \"query: \" or \"passage: \" prefix depending on the text being embedded.\\n', 'example': ''}, 'output': {'description': 'The output is the text embedding vector.', 'example': ''}}}, 'dataset_metadata': {'info': 'This checkpoint was trained on CCPairs, a text pair dataset from Reddit, Semantic Scholar, Stack Exchange, Common Crawl,\\nand various news sources.\\nThe model was trained with a batch size of 32,768 for 20k steps.\\n', 'url': ''}, 'preset_ids': [], 'system_prompt_ids': [], 'device': 'rdu', 'evaluation': {'eval_1': {'dataset': {'info': 'MTEB - Massive Text Embedding Benchmark', 'url': ''}, 'metrics': {'Average': 0.6225}}, 'eval_2': {'dataset': {'info': 'BEIR - Benchmarking-Information Retrieval', 'url': ''}, 'metrics': {'Average': 0.506}}}, 'time_created': '2024-05-15 18:11:54.02993 +0000 UTC', 'time_updated': '2024-06-04 10:18:19.143442 +0000 UTC', 'app_name': 'Text Embedding', 'hidden': False, 'jobTypes': ['batch_predict', 'deploy'], 'shared_with': [], 'type': '', 'dependencies': [], 'replacement_model_id': '', 'replacement_model_name': '', 'deprecation_date': '', 'linked_coe_models': [], 'model_metadata': None, 'model_parallel_rdus': 0, 'metadata': {'labels': {}, 'creationTimestamp': '', 'updateTimestamp': '', 'resourceVersion': '', 'tenant': '', 'owner': '', 'permissions': ['ModelView', 'ModelExport']}}], 'status_code': 200}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:49:43,088 [INFO] Dataset with name 'smol_sql_pretraining_mm' found with id d80f6355-af9d-406b-95c5-c31854f36f2c\n",
      "2024-07-22 12:49:43,382 [INFO] Job with name 'None' created: '{'job_id': '385632fa-284c-42a2-892a-b1bfb6161a7e', 'job_name': 'snsdk_test_job', 'job_type': 'train', 'user_id': 'jorge.piedrahita', 'project_id': '51b1fe13-dcdb-41e3-8a78-514da36937c8', 'tenant_id': '41ceaded-9f08-47ae-aa02-15f39c899618', 'rdu_arch': 'sn20', 'result_path': '', 'parallel_instances': 1, 'app_id': 'ec012370-6ffa-4a3a-b230-2c62613f1d89', 'model_checkpoint': '6090d4ac-a7bd-4c46-b417-7f8e42cf7bdb', 'checkpoint_id': '', 'dataset_id': 'd80f6355-af9d-406b-95c5-c31854f36f2c', 'description': 'snsdk test training project', 'status': 'CREATED', 'image_version': '', 'variant_set_version': '', 'variant_name': '', 'project_name': '', 'dataset_name': '', 'input_data_path': '', 'hyperparams': {'batch_size': '256', 'debug_mode': 'false', 'do_eval': 'false', 'dump_inputs': 'false', 'eval_steps': '50', 'evaluation_strategy': 'no', 'fix_rank_rdu_mapping': 'false', 'learning_rate': '0.00001', 'logging_steps': '1', 'lr_schedule': 'fixed_lr', 'max_seq_length': '4096', 'max_sequence_length': '4096', 'num_iterations': '100', 'prompt_loss_weight': '0', 'save_optimizer_state': 'true', 'save_steps': '50', 'skip_checkpoint': 'false', 'sockets': '1', 'subsample_eval': '0.01', 'subsample_eval_seed': '123', 'use_token_type_ids': 'true', 'vocab_size': '32000', 'warmup_steps': '0', 'weight_decay': '0.1'}, 'config': {'batch_size': '256', 'debug_mode': 'false', 'do_eval': 'false', 'dump_inputs': 'false', 'eval_steps': '50', 'evaluation_strategy': 'no', 'fix_rank_rdu_mapping': 'false', 'learning_rate': '0.00001', 'logging_steps': '1', 'lr_schedule': 'fixed_lr', 'max_seq_length': '4096', 'max_sequence_length': '4096', 'num_iterations': '100', 'prompt_loss_weight': '0', 'save_optimizer_state': 'true', 'save_steps': '50', 'skip_checkpoint': 'false', 'sockets': '1', 'subsample_eval': '0.01', 'subsample_eval_seed': '123', 'use_token_type_ids': 'true', 'vocab_size': '32000', 'warmup_steps': '0', 'weight_decay': '0.1'}, 'time_created': '2024-07-22T17:49:43.496856000Z', 'time_updated': '2024-07-22T17:49:43.496856000Z', 'load_state': False, 'app_name': '', 'environment_variables': '', 'status_code': 200}'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'385632fa-284c-42a2-892a-b1bfb6161a7e'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "snsdk.run_job(project_name=\"example project\", dataset_name=\"smol_sql_pretraining_mm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-18 11:40:28,992 [INFO] Job with name 'snsdk_test_job' in project sql_finetuning found with id a876668b-fc48-49b9-af23-6b078c40ef84\n",
      "2024-07-18 11:40:29,379 [ERROR] Failed to check job progress. Details: {'detail': 'view job action not authorized for the user', 'status_code': 403}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'detail': 'view job action not authorized for the user', 'status_code': 403}\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Error message: {'detail': 'view job action not authorized for the user', 'status_code': 403}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0m \u001b[43msnsdk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_job_progress\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msql_finetuning\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/Documents/ask_public_own/ai-starter-kit/utils/fine_tuning/src/sambastudio_util.py:227\u001b[0m, in \u001b[0;36mSnsdkWrapper.check_job_progress\u001b[0;34m(self, project_name, job_name)\u001b[0m\n",
      "\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m    226\u001b[0m     logging\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to check job progress. Details: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheck_job_progress_response\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;32m--> 227\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError message: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheck_job_progress_response\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "\u001b[0;31mException\u001b[0m: Error message: {'detail': 'view job action not authorized for the user', 'status_code': 403}"
     ]
    }
   ],
   "source": [
    "snsdk.check_job_progress(project_name=\"sql_finetuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-18 12:48:21,276 [INFO] Job with name 'snsdk_test_job' in project 'sql_finetuning' found with id 'a876668b-fc48-49b9-af23-6b078c40ef84'\n",
      "2024-07-18 12:48:21,660 [ERROR] Failed to list checkpoints. Details: {'detail': 'view checkpoints action not authorized for the user', 'status_code': 403}\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Error message: {'detail': 'view checkpoints action not authorized for the user', 'status_code': 403}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0m \u001b[43msnsdk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_checkpoints\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msql_finetuning\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/Documents/ask_public_own/ai-starter-kit/utils/fine_tuning/src/sambastudio_util.py:285\u001b[0m, in \u001b[0;36mSnsdkWrapper.list_checkpoints\u001b[0;34m(self, project_name, job_name)\u001b[0m\n",
      "\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m    284\u001b[0m     logging\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to list checkpoints. Details: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlist_checkpoints_response\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError message: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlist_checkpoints_response\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "\u001b[0;31mException\u001b[0m: Error message: {'detail': 'view checkpoints action not authorized for the user', 'status_code': 403}"
     ]
    }
   ],
   "source": [
    "snsdk.list_checkpoints(project_name=\"sql_finetuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-18 15:01:55,077 [INFO] Project with name 'sql_finetuning' found with id 302411f2-a7f7-42b3-a056-13f99330d3f9\n",
      "2024-07-18 15:01:55,332 [INFO] Project with name 'sql_finetuning' found with id 302411f2-a7f7-42b3-a056-13f99330d3f9\n",
      "2024-07-18 15:01:55,605 [INFO] Job with name 'snsdk_test_job' in project 'sql_finetuning' found with id 'a876668b-fc48-49b9-af23-6b078c40ef84'\n",
      "2024-07-18 15:01:55,906 [ERROR] Failed to promote checkpoint 'a876668b-fc48-49b9-af23-6b078c40ef84-50' to model. Details: {'detail': 'checkpoint promote action is not authorized for the user', 'status_code': 403}\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Error message: {'detail': 'checkpoint promote action is not authorized for the user', 'status_code': 403}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0m \u001b[43msnsdk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpromote_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ma876668b-fc48-49b9-af23-6b078c40ef84-50\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproject_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msql_finetuning\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/Documents/ask_public_own/ai-starter-kit/utils/fine_tuning/src/sambastudio_util.py:358\u001b[0m, in \u001b[0;36mSnsdkWrapper.promote_checkpoint\u001b[0;34m(self, checkpoint_id, project_name, job_name, model_name, model_description, model_type)\u001b[0m\n",
      "\u001b[1;32m    355\u001b[0m     \u001b[38;5;66;03m#TODO test blocked because of authorization error \u001b[39;00m\n",
      "\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m    357\u001b[0m     logging\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to promote checkpoint \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to model. Details: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madd_model_response\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;32m--> 358\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError message: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madd_model_response\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "\u001b[0;31mException\u001b[0m: Error message: {'detail': 'checkpoint promote action is not authorized for the user', 'status_code': 403}"
     ]
    }
   ],
   "source": [
    "snsdk.promote_checkpoint(\"a876668b-fc48-49b9-af23-6b078c40ef84-50\", project_name=\"sql_finetuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 09:00:07,470 [INFO] Project with name 'sql_finetuning' found with id 302411f2-a7f7-42b3-a056-13f99330d3f9\n",
      "2024-07-19 09:00:07,767 [INFO] Model with name 'llama2_7b_fine_tuned_nstext2sql' found with id c828d8d6-bbc6-4315-8573-014a183ffa58\n",
      "2024-07-19 09:00:07,953 [INFO] Endpoint with name 'test-endpoint-sql' not created it already exist with id 27c560eb-1857-49f4-b707-a5e9ab106b1b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'27c560eb-1857-49f4-b707-a5e9ab106b1b'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "snsdk.create_endpoint(project_name=\"sql_finetuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 09:20:54,363 [INFO] Project with name 'sql_finetuning' found with id 302411f2-a7f7-42b3-a056-13f99330d3f9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'Live',\n",
       " 'url': '/api/predict/generic/302411f2-a7f7-42b3-a056-13f99330d3f9/27c560eb-1857-49f4-b707-a5e9ab106b1b',\n",
       " 'langchain wrapper env': {'SAMBASTUDIO_BASE_URL': 'https://sjc3-demo1.sambanova.net',\n",
       "  'SAMBASTUDIO_BASE_URI': 'api/predict/generic',\n",
       "  'SAMBASTUDIO_PROJECT_ID': '302411f2-a7f7-42b3-a056-13f99330d3f9',\n",
       "  'SAMBASTUDIO_ENDPOINT_ID': '27c560eb-1857-49f4-b707-a5e9ab106b1b',\n",
       "  'SAMBASTUDIO_API_KEY': '5c005055-de1e-4198-be8e-ae25ba9c6fb7'}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "snsdk.get_endpoint_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 09:24:40,647 [INFO] Project with name 'sql_finetuning' found with id 302411f2-a7f7-42b3-a056-13f99330d3f9\n",
      "2024-07-19 09:24:41,195 [INFO] Endpoint 'test-endpoint-sql' deleted in project 'sql_finetuning'\n"
     ]
    }
   ],
   "source": [
    "snsdk.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## deployed endpoint usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"[INST]<<SYS>>\n",
    "\n",
    "Generate a query using valid SQLite to answer the following questions for the summarized tables schemas provided bellow.\n",
    "Do not assume the values on the database tables before generating the SQL query, always generate a SQL that query what is asked. \n",
    "The query must be in the format: \n",
    "```sql\n",
    "query\n",
    "```\n",
    "    \n",
    "Example:\n",
    "\n",
    "```sql\n",
    "SELECT * FROM mainTable;\n",
    "```\n",
    "<</SYS>>\n",
    "\t\n",
    "        \n",
    "CREATE TABLE \"Album\" (\n",
    "\t\"AlbumId\" INTEGER NOT NULL, \n",
    "\t\"Title\" NVARCHAR(160) NOT NULL, \n",
    "\t\"ArtistId\" INTEGER NOT NULL, \n",
    "\tPRIMARY KEY (\"AlbumId\"), \n",
    "\tFOREIGN KEY(\"ArtistId\") REFERENCES \"Artist\" (\"ArtistId\")\n",
    ")\n",
    "\n",
    "/*\n",
    "3 rows from Album table:\n",
    "AlbumId\tTitle\tArtistId\n",
    "1\tFor Those About To Rock We Salute You\t1\n",
    "2\tBalls to the Wall\t2\n",
    "3\tRestless and Wild\t2\n",
    "*/\n",
    "\n",
    "\n",
    "CREATE TABLE \"Artist\" (\n",
    "\t\"ArtistId\" INTEGER NOT NULL, \n",
    "\t\"Name\" NVARCHAR(120), \n",
    "\tPRIMARY KEY (\"ArtistId\")\n",
    ")\n",
    "\n",
    "/*\n",
    "3 rows from Artist table:\n",
    "ArtistId\tName\n",
    "1\tAC/DC\n",
    "2\tAccept\n",
    "3\tAerosmith\n",
    "*/\n",
    "\n",
    "\n",
    "CREATE TABLE \"Customer\" (\n",
    "\t\"CustomerId\" INTEGER NOT NULL, \n",
    "\t\"FirstName\" NVARCHAR(40) NOT NULL, \n",
    "\t\"LastName\" NVARCHAR(20) NOT NULL, \n",
    "\t\"Company\" NVARCHAR(80), \n",
    "\t\"Address\" NVARCHAR(70), \n",
    "\t\"City\" NVARCHAR(40), \n",
    "\t\"State\" NVARCHAR(40), \n",
    "\t\"Country\" NVARCHAR(40), \n",
    "\t\"PostalCode\" NVARCHAR(10), \n",
    "\t\"Phone\" NVARCHAR(24), \n",
    "\t\"Fax\" NVARCHAR(24), \n",
    "\t\"Email\" NVARCHAR(60) NOT NULL, \n",
    "\t\"SupportRepId\" INTEGER, \n",
    "\tPRIMARY KEY (\"CustomerId\"), \n",
    "\tFOREIGN KEY(\"SupportRepId\") REFERENCES \"Employee\" (\"EmployeeId\")\n",
    ")\n",
    "\n",
    "/*\n",
    "3 rows from Customer table:\n",
    "CustomerId\tFirstName\tLastName\tCompany\tAddress\tCity\tState\tCountry\tPostalCode\tPhone\tFax\tEmail\tSupportRepId\n",
    "1\tLuís\tGonçalves\tEmbraer - Empresa Brasileira de Aeronáutica S.A.\tAv. Brigadeiro Faria Lima, 2170\tSão José dos Campos\tSP\tBrazil\t12227-000\t+55 (12) 3923-5555\t+55 (12) 3923-5566\tluisg@embraer.com.br\t3\n",
    "2\tLeonie\tKöhler\tNone\tTheodor-Heuss-Straße 34\tStuttgart\tNone\tGermany\t70174\t+49 0711 2842222\tNone\tleonekohler@surfeu.de\t5\n",
    "3\tFrançois\tTremblay\tNone\t1498 rue Bélanger\tMontréal\tQC\tCanada\tH2G 1A7\t+1 (514) 721-4711\tNone\tftremblay@gmail.com\t3\n",
    "*/\n",
    "\n",
    "\n",
    "CREATE TABLE \"Employee\" (\n",
    "\t\"EmployeeId\" INTEGER NOT NULL, \n",
    "\t\"LastName\" NVARCHAR(20) NOT NULL, \n",
    "\t\"FirstName\" NVARCHAR(20) NOT NULL, \n",
    "\t\"Title\" NVARCHAR(30), \n",
    "\t\"ReportsTo\" INTEGER, \n",
    "\t\"BirthDate\" DATETIME, \n",
    "\t\"HireDate\" DATETIME, \n",
    "\t\"Address\" NVARCHAR(70), \n",
    "\t\"City\" NVARCHAR(40), \n",
    "\t\"State\" NVARCHAR(40), \n",
    "\t\"Country\" NVARCHAR(40), \n",
    "\t\"PostalCode\" NVARCHAR(10), \n",
    "\t\"Phone\" NVARCHAR(24), \n",
    "\t\"Fax\" NVARCHAR(24), \n",
    "\t\"Email\" NVARCHAR(60), \n",
    "\tPRIMARY KEY (\"EmployeeId\"), \n",
    "\tFOREIGN KEY(\"ReportsTo\") REFERENCES \"Employee\" (\"EmployeeId\")\n",
    ")\n",
    "\n",
    "/*\n",
    "3 rows from Employee table:\n",
    "EmployeeId\tLastName\tFirstName\tTitle\tReportsTo\tBirthDate\tHireDate\tAddress\tCity\tState\tCountry\tPostalCode\tPhone\tFax\tEmail\n",
    "1\tAdams\tAndrew\tGeneral Manager\tNone\t1962-02-18 00:00:00\t2002-08-14 00:00:00\t11120 Jasper Ave NW\tEdmonton\tAB\tCanada\tT5K 2N1\t+1 (780) 428-9482\t+1 (780) 428-3457\tandrew@chinookcorp.com\n",
    "2\tEdwards\tNancy\tSales Manager\t1\t1958-12-08 00:00:00\t2002-05-01 00:00:00\t825 8 Ave SW\tCalgary\tAB\tCanada\tT2P 2T3\t+1 (403) 262-3443\t+1 (403) 262-3322\tnancy@chinookcorp.com\n",
    "3\tPeacock\tJane\tSales Support Agent\t2\t1973-08-29 00:00:00\t2002-04-01 00:00:00\t1111 6 Ave SW\tCalgary\tAB\tCanada\tT2P 5M5\t+1 (403) 262-3443\t+1 (403) 262-6712\tjane@chinookcorp.com\n",
    "*/\n",
    "\n",
    "\n",
    "CREATE TABLE \"Genre\" (\n",
    "\t\"GenreId\" INTEGER NOT NULL, \n",
    "\t\"Name\" NVARCHAR(120), \n",
    "\tPRIMARY KEY (\"GenreId\")\n",
    ")\n",
    "\n",
    "/*\n",
    "3 rows from Genre table:\n",
    "GenreId\tName\n",
    "1\tRock\n",
    "2\tJazz\n",
    "3\tMetal\n",
    "*/\n",
    "\n",
    "\n",
    "CREATE TABLE \"Invoice\" (\n",
    "\t\"InvoiceId\" INTEGER NOT NULL, \n",
    "\t\"CustomerId\" INTEGER NOT NULL, \n",
    "\t\"InvoiceDate\" DATETIME NOT NULL, \n",
    "\t\"BillingAddress\" NVARCHAR(70), \n",
    "\t\"BillingCity\" NVARCHAR(40), \n",
    "\t\"BillingState\" NVARCHAR(40), \n",
    "\t\"BillingCountry\" NVARCHAR(40), \n",
    "\t\"BillingPostalCode\" NVARCHAR(10), \n",
    "\t\"Total\" NUMERIC(10, 2) NOT NULL, \n",
    "\tPRIMARY KEY (\"InvoiceId\"), \n",
    "\tFOREIGN KEY(\"CustomerId\") REFERENCES \"Customer\" (\"CustomerId\")\n",
    ")\n",
    "\n",
    "/*\n",
    "3 rows from Invoice table:\n",
    "InvoiceId\tCustomerId\tInvoiceDate\tBillingAddress\tBillingCity\tBillingState\tBillingCountry\tBillingPostalCode\tTotal\n",
    "1\t2\t2021-01-01 00:00:00\tTheodor-Heuss-Straße 34\tStuttgart\tNone\tGermany\t70174\t1.98\n",
    "2\t4\t2021-01-02 00:00:00\tUllevålsveien 14\tOslo\tNone\tNorway\t0171\t3.96\n",
    "3\t8\t2021-01-03 00:00:00\tGrétrystraat 63\tBrussels\tNone\tBelgium\t1000\t5.94\n",
    "*/\n",
    "\n",
    "\n",
    "CREATE TABLE \"InvoiceLine\" (\n",
    "\t\"InvoiceLineId\" INTEGER NOT NULL, \n",
    "\t\"InvoiceId\" INTEGER NOT NULL, \n",
    "\t\"TrackId\" INTEGER NOT NULL, \n",
    "\t\"UnitPrice\" NUMERIC(10, 2) NOT NULL, \n",
    "\t\"Quantity\" INTEGER NOT NULL, \n",
    "\tPRIMARY KEY (\"InvoiceLineId\"), \n",
    "\tFOREIGN KEY(\"TrackId\") REFERENCES \"Track\" (\"TrackId\"), \n",
    "\tFOREIGN KEY(\"InvoiceId\") REFERENCES \"Invoice\" (\"InvoiceId\")\n",
    ")\n",
    "\n",
    "/*\n",
    "3 rows from InvoiceLine table:\n",
    "InvoiceLineId\tInvoiceId\tTrackId\tUnitPrice\tQuantity\n",
    "1\t1\t2\t0.99\t1\n",
    "2\t1\t4\t0.99\t1\n",
    "3\t2\t6\t0.99\t1\n",
    "*/\n",
    "\n",
    "\n",
    "CREATE TABLE \"MediaType\" (\n",
    "\t\"MediaTypeId\" INTEGER NOT NULL, \n",
    "\t\"Name\" NVARCHAR(120), \n",
    "\tPRIMARY KEY (\"MediaTypeId\")\n",
    ")\n",
    "\n",
    "/*\n",
    "3 rows from MediaType table:\n",
    "MediaTypeId\tName\n",
    "1\tMPEG audio file\n",
    "2\tProtected AAC audio file\n",
    "3\tProtected MPEG-4 video file\n",
    "*/\n",
    "\n",
    "\n",
    "CREATE TABLE \"Playlist\" (\n",
    "\t\"PlaylistId\" INTEGER NOT NULL, \n",
    "\t\"Name\" NVARCHAR(120), \n",
    "\tPRIMARY KEY (\"PlaylistId\")\n",
    ")\n",
    "\n",
    "/*\n",
    "3 rows from Playlist table:\n",
    "PlaylistId\tName\n",
    "1\tMusic\n",
    "2\tMovies\n",
    "3\tTV Shows\n",
    "*/\n",
    "\n",
    "\n",
    "CREATE TABLE \"PlaylistTrack\" (\n",
    "\t\"PlaylistId\" INTEGER NOT NULL, \n",
    "\t\"TrackId\" INTEGER NOT NULL, \n",
    "\tPRIMARY KEY (\"PlaylistId\", \"TrackId\"), \n",
    "\tFOREIGN KEY(\"TrackId\") REFERENCES \"Track\" (\"TrackId\"), \n",
    "\tFOREIGN KEY(\"PlaylistId\") REFERENCES \"Playlist\" (\"PlaylistId\")\n",
    ")\n",
    "\n",
    "/*\n",
    "3 rows from PlaylistTrack table:\n",
    "PlaylistId\tTrackId\n",
    "1\t3402\n",
    "1\t3389\n",
    "1\t3390\n",
    "*/\n",
    "\n",
    "\n",
    "CREATE TABLE \"Track\" (\n",
    "\t\"TrackId\" INTEGER NOT NULL, \n",
    "\t\"Name\" NVARCHAR(200) NOT NULL, \n",
    "\t\"AlbumId\" INTEGER, \n",
    "\t\"MediaTypeId\" INTEGER NOT NULL, \n",
    "\t\"GenreId\" INTEGER, \n",
    "\t\"Composer\" NVARCHAR(220), \n",
    "\t\"Milliseconds\" INTEGER NOT NULL, \n",
    "\t\"Bytes\" INTEGER, \n",
    "\t\"UnitPrice\" NUMERIC(10, 2) NOT NULL, \n",
    "\tPRIMARY KEY (\"TrackId\"), \n",
    "\tFOREIGN KEY(\"MediaTypeId\") REFERENCES \"MediaType\" (\"MediaTypeId\"), \n",
    "\tFOREIGN KEY(\"GenreId\") REFERENCES \"Genre\" (\"GenreId\"), \n",
    "\tFOREIGN KEY(\"AlbumId\") REFERENCES \"Album\" (\"AlbumId\")\n",
    ")\n",
    "\n",
    "/*\n",
    "3 rows from Track table:\n",
    "TrackId\tName\tAlbumId\tMediaTypeId\tGenreId\tComposer\tMilliseconds\tBytes\tUnitPrice\n",
    "1\tFor Those About To Rock (We Salute You)\t1\t1\t1\tAngus Young, Malcolm Young, Brian Johnson\t343719\t11170334\t0.99\n",
    "2\tBalls to the Wall\t2\t2\t1\tU. Dirkschneider, W. Hoffmann, H. Frank, P. Baltes, S. Kaufmann, G. Hoffmann\t342562\t5510424\t0.99\n",
    "3\tFast As a Shark\t3\t2\t1\tF. Baltes, S. Kaufman, U. Dirkscneider & W. Hoffman\t230619\t3990994\t0.99\n",
    "*/\n",
    "        \n",
    "    how many music genres are in the db?\n",
    "    [/INST]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' SELECT COUNT(*) FROM \"Genre\"'"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import SambaStudio\n",
    "\n",
    "llm = SambaStudio(\n",
    "    sambastudio_base_url=endpoint_env.get(\"SAMBASTUDIO_BASE_URL\"),\n",
    "    sambastudio_base_uri=endpoint_env.get(\"SAMBASTUDIO_BASE_URI\"),\n",
    "    sambastudio_project_id=endpoint_env.get(\"SAMBASTUDIO_PROJECT_ID\"),\n",
    "    sambastudio_endpoint_id=endpoint_env.get(\"SAMBASTUDIO_ENDPOINT_ID\"),\n",
    "    sambastudio_api_key=endpoint_env.get(\"SAMBASTUDIO_API_KEY\"),\n",
    "    model_kwargs = {\n",
    "        \"do_sample\": True, \n",
    "        \"temperature\": 0.01,\n",
    "        \"max_tokens_to_generate\": 512\n",
    "    }\n",
    ")\n",
    "llm.invoke(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetuning_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
