embedding_model: 
    type: "cpu"
    batch_size: 1
    coe: True
    select_expert: "e5-mistral-7b-instruct"

sn_llm:
    type: "sncloud"
    model: "Meta-Llama-3.1-70B-Instruct"
    max_tokens: 1200
    temperature: 0.0
    top_p: 0.0
    top_k: 1

ss_llm:
    type: "sambastudio"
    temperature: 0.0
    do_sample: False
    max_tokens_to_generate: 1200
    coe: True
    select_expert: "Meta-Llama-3-70B-Instruct-4096"

sn_chat_model:
    type: "sncloud"
    model: "Meta-Llama-3.1-70B-Instruct"
    streaming: False
    max_tokens: 1024
    temperature: 0.7
    top_k: 1
    top_p: 0.01
    stream_options : 
        include_usage: True

sambastudio_chat_model:
    type: "sambastudio"
    model: "Meta-Llama-3-70B-Instruct-4096"
    streaming: false
    max_tokens: 1200
    temperature: 0.0
    top_p: null
    top_k: null
    do_sample: false
    process_prompt: false
    stream_options:
      include_usage: true
    special_tokens:
      start: "<|begin_of_text|>"
      start_role: "<|begin_of_text|><|start_header_id|>{role}<|end_header_id|>"
      end_role: "<|eot_id|>"
      end: "<|start_header_id|>assistant<|end_header_id|>\n"
    model_kwargs: null

bad_format_embeddings_model:
    type:
      - type: "string"
        batch_size: 1
        coe: True
        select_expert: "e5-mistral-7b-instruct"

    string:
      - type: "cpu"
        batch_size: 1
        coe: True
        select_expert: 1
      
      - type: "sambastudio"
        batch_size: 1
        coe: True
        select_expert: 1

    boolean:
      - type: "cpu"
        batch_size: 1
        coe: "string"
        select_expert: "e5-mistral-7b-instruct"
      
      - type: "sambastudio"
        batch_size: 1
        coe: "string"
        select_expert: "e5-mistral-7b-instruct"

    integer:
      - type: "cpu"
        batch_size: "string"
        coe: True
        select_expert: "e5-mistral-7b-instruct"
      
      - type: "sambastudio"
        batch_size: "string"
        coe: True
        select_expert: "e5-mistral-7b-instruct"

bad_format_llm_model:
    string:
      - type: "sncloud"
        model: 1
        max_tokens: 1024
        temperature: 0.0
        streaming: False

      - type: "sambastudio"
        model: 1
        max_tokens: 1024
        temperature: 0.0
        streaming: False
        coe: True
        select_expert: "Meta-Llama-3-70B-Instruct-4096"
    
    boolean:
      - type: "sncloud"
        model: "foo"
        max_tokens: 1024
        temperature: 0.0
        streaming: "string"

      - type: "sambastudio"
        model: "foo"
        max_tokens: 1024
        temperature: 0.0
        streaming: "string"
        coe: True
        select_expert: "Meta-Llama-3-70B-Instruct-4096"

    integer:
      - type: "sncloud"
        model: "foo"
        max_tokens: "one"
        temperature: 0.0
        streaming: False

      - type: "sambastudio"
        model: "foo"
        max_tokens: "one"
        temperature: 0.0
        streaming: False
        coe: True
        select_expert: "Meta-Llama-3-70B-Instruct-4096"

    number:
      - type: "sncloud"
        model: "foo"
        max_tokens: 1024
        temperature: "string"
        streaming: False

      - type: "sambastudio"
        model: "foo"
        max_tokens: 1024
        temperature: "string"
        streaming: False
        coe: True
        select_expert: "Meta-Llama-3-70B-Instruct-4096"

bad_format_chat_model:
    string:
      - type: "sncloud"
        model: 1
        max_tokens: 1024
        temperature: 0.0
        streaming: False

      - type: "sambastudio"
        model: 1
        max_tokens: 1024
        temperature: 0.0
        streaming: False

    boolean:
      - type: "sncloud"
        model: "foo"
        max_tokens: 1024
        temperature: 0.0
        streaming: "string"

      - type: "sambastudio"
        model: "foo"
        max_tokens: 1024
        temperature: 0.0
        streaming: "string"

    integer:
      - type: "sncloud"
        model: "foo"
        max_tokens: "one"
        temperature: 0.0
        streaming: False

      - type: "sambastudio"
        model: "foo"
        max_tokens: "one"
        temperature: 0.0
        streaming: False

    number:
      - type: "sncloud"
        model: "foo"
        max_tokens: 1024
        temperature: "string"
        streaming: False

      - type: "sambastudio"
        model: "foo"
        max_tokens: 1024
        temperature: "string"
        streaming: False

    dict:
      - type: "sncloud"
        model: "foo"
        max_tokens: 1024
        temperature: 0.0
        streaming: False
        stream_options: "string"

      - type: "sambastudio"
        model: "foo"
        max_tokens: 1024
        temperature: 0.0
        streaming: False
        stream_options: "string"
